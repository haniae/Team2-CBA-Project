# ðŸ“Š FinalyzeOS Chatbot Platform

**Institutional-Grade Finance Copilot with Explainable AI**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey)](https://github.com/haniae/Team2-CBA-Project)
[![Data Coverage](https://img.shields.io/badge/data-1,599%20companies%20%7C%2018%20years-success)](https://github.com/haniae/Team2-CBA-Project)
[![NLU Coverage](https://img.shields.io/badge/NLU-100%25%20patterns%20%7C%2093%20metrics-blue)](https://github.com/haniae/Team2-CBA-Project)
[![ML Models](https://img.shields.io/badge/ML-8%20forecasting%20models-purple)](https://github.com/haniae/Team2-CBA-Project)
[![Intents](https://img.shields.io/badge/intents-40%2B%20types-orange)](https://github.com/haniae/Team2-CBA-Project)
[![Spelling](https://img.shields.io/badge/spelling-90%25%20company%20%7C%20100%25%20metric-green)](https://github.com/haniae/Team2-CBA-Project)

**FinalyzeOS** is an institutional-grade copilot for finance teams. It pairs deterministic market analytics with a conversational interface so analysts can ask natural-language questions, inspect lineage, and keep data pipelines auditable. This repository underpins our Fall 2025 DNSC 6317 practicum at The George Washington University, where we are building and governing an explainable finance copilot that can support regulated teams. Our objectives include stress-testing FinalyzeOS against real analyst workflows, documenting orchestration strategies for enterprise rollouts, and demonstrating responsible AI guardrails around data access, lineage, and scenario planning.

## Contributors

- **Hania A.** - haniaa@gwmail.gwu.edu
- **Van Nhi Vuong** - vannhi.vuong@gwmail.gwu.edu
- **Malcolm Muoriyarwa** - malcolm.munoriyarwa@gwmail.gwu.edu
- **Devarsh Patel** - devarsh.patel@gwmail.gwu.edu

## Acknowledgments

Special thanks to Professor Patrick Hall (The George Washington University) for his outstanding mentorship and tireless support. His guidance and encouragement made this project possible.

---

**Quick Links:** [Setup Guide](#ï¸-complete-setup-guide) â€¢ [Documentation](docs/) â€¢ [Features](#core-capabilities) â€¢ [Contributing](CONTRIBUTING.md)

---

## âš¡ Quick Start

**Get started in 30 seconds:**

```bash
# 1. Clone and setup
git clone https://github.com/haniae/Team2-CBA-Project.git
cd Team2-CBA-Project
python -m venv .venv
.\.venv\Scripts\activate  # Windows: .\.venv\Scripts\Activate.ps1
# macOS/Linux: source .venv/bin/activate
pip install -r requirements.txt && pip install -e .

# 1.5. Add API key to .env for faster results (optional but recommended)
# Create .env file and add: OPENAI_API_KEY=sk-your-key-here
# Windows: Copy-Item .env.example .env  (if .env.example exists)
# macOS/Linux: cp .env.example .env  (if .env.example exists)

# 2. Quick test (100 companies, ~15-30 min)
python scripts/ingestion/ingest_universe.py --universe-file data/tickers/test_100.txt --years 5

# 2.5. Index into Vector DB for RAG (optional but recommended)
# This enables semantic search over SEC filings, earnings transcripts, news, and more
# First, install vector DB dependencies:
pip install chromadb sentence-transformers requests beautifulsoup4 yfinance
# Test with one ticker first (SEC filings):
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --fetch-from-sec --limit 3
# Index additional sources (NEW!):
python scripts/index_documents_for_rag.py --database data/financial.db --type earnings --ticker AAPL
python scripts/index_documents_for_rag.py --database data/financial.db --type news --ticker AAPL
# Check status: python scripts/utility/check_vector_db.py
# Then process all: python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp500 --fetch-from-sec --limit 5
# See full guide: docs/guides/VECTOR_DB_INGESTION_GUIDE.md or README section "Vector Database Indexing"

# 3. Run the chatbot
python run_chatbot.py
# Or web UI: python serve_chatbot.py --port 8000
```

**Try these queries:**
 `"What is Apple's revenue?"`
 `"Compare Microsoft and Google's profit margins"`
 `"Show me Tesla's free cash flow in 2023"`
 `"Why is NVDA's stock price increasing?"`

ðŸ“– **For detailed setup with options (100/250/500/1500 companies), see [Complete Setup Guide](#ï¸-complete-setup-guide)**

---

## ðŸ“š Table of Contents

### Getting Started
- [âš¡ Quick Start](#-quick-start)
- [ðŸ› ï¸ Complete Setup Guide](#ï¸-complete-setup-guide)
- [ðŸ“Š Current Data Coverage](#-current-data-coverage)

### Core Features
- [âš¡ Core Capabilities](#-core-capabilities)
- [ðŸš€ Advanced Analytics](#-advanced-analytics)
- [ðŸ¤– Machine Learning Forecasting](#-machine-learning-forecasting-new)
- [ðŸ“š Retrieval-Augmented Generation](#-retrieval-augmented-generation)
- [ðŸ“Š Portfolio Management](#-portfolio-management)

### Technical Documentation
- [ðŸ—ï¸ Architecture Map](#ï¸-architecture-map)
- [ðŸ§  Retrieval & ML Internals](#-retrieval--ml-internals)
- [ðŸ’¬ Running FinalyzeOS](#-running-finalyzeos)
- [ðŸ“¥ Data Ingestion Guide](#-data-ingestion-guide)
- [ðŸ” Vector Database Guide](docs/guides/VECTOR_DB_INGESTION_GUIDE.md) - Complete guide for vector DB indexing
- [âš™ï¸ Configuration Reference](#ï¸-configuration-reference)
- [ðŸ—„ï¸ Database Schema](#ï¸-database-schema)

### Project Structure
- [ðŸ“ Project Layout](#-project-layout)

### Support
- [âœ… Quality and Testing](#-quality-and-testing)
- [ðŸ”§ Troubleshooting](#-troubleshooting)
- [ðŸ“š Further Reading](#-further-reading)
- [ðŸŽ“ System Overview (Professor Summary)](#-system-overview-professor-summary)
- [ðŸ§­ Full Docs Index](docs/README.md)



### ðŸŽ¯ Project Focus

- ðŸ”§ **Production-Grade Analytics** - Translate classroom techniques into a production-grade analytics assistant that blends deterministic KPI calculations with auditable LLM experiences
- ðŸ›¡ï¸ **Resilient Pipelines** - Stand up KPI coverage pipelines that stay resilient when market data lags or filing assumptions drift
- ðŸ“š **Practitioner-Ready Documentation** - Deliver deployment runbooks and testing strategies so stakeholders can re-create the practicum outcomes after the semester concludes



## ðŸ“– Overview

FinalyzeOS ships as a **batteries-included template** for building finance copilots. Out of the box you gain:

- ðŸ—„ï¸ **Durable Storage** - SQLite by default, PostgreSQL optional for conversations, facts, metrics, audit trails, and scenarios
- ðŸ“Š **Analytics Engines** - Normalise SEC filings, hydrate them with market quotes, and expose tabular as well as scenario-ready metrics
- ðŸ¤– **Flexible LLM Integration** - Deterministic echo model for testing or OpenAI for production deployments
- ðŸ–¥ï¸ **Multi-Channel Experiences** - CLI REPL, FastAPI REST service, single-page web UI so you can prototype quickly and scale later
- ðŸ“š **Rich Documentation** - Complete guides for scaling "any company" requests and replicating workflows in production

### ðŸŽ¯ What Can You Do?

Ask natural language questions and get instant, sourced financial insights:

**Single Company Analysis:**
- `"What is Apple's revenue?"` â†’ Get revenue with YoY growth, CAGR, and business drivers
- `"Show me Tesla's free cash flow"` â†’ Detailed FCF analysis with trends and context
- `"What's Microsoft's P/E ratio?"` â†’ Valuation metrics with historical comparison
- `"What is Appel's revenue?"` â†’ Automatically corrects spelling mistakes (90% success rate)
- `"Show me revenu for Apple"` â†’ Handles metric typos (100% success rate)

**Comparisons:**
- `"Compare Apple vs Microsoft's profit margins"` â†’ Side-by-side analysis with sector benchmarks
- `"How do tech companies stack up on ROE?"` â†’ Multi-company ranking and percentile analysis
- `"Compare Microsft and Googl"` â†’ Spelling mistakes automatically corrected

**Deep Analysis:**
- `"Why is Tesla's margin declining?"` â†’ Multi-factor explanation with quantified impacts
- `"What's driving Amazon's revenue growth?"` â†’ Business segment breakdown and drivers
- `"Is NVDA overvalued?"` â†’ Valuation analysis with peer comparison

**Forecasting & Scenarios:**
- `"Forecast Microsoft's revenue for 2026"` â†’ ML-powered forecasts with confidence intervals
- `"What if Apple's revenue grows 10% faster?"` â†’ Scenario analysis with impact on valuation

**Portfolio Management:**
- `"Show me my portfolio performance"` â†’ Portfolio analytics with risk metrics
- `"What's my portfolio's sector exposure?"` â†’ Diversification analysis

**Query Flexibility:**
- `"Apple revenue"` â†’ Minimal queries work perfectly
- `"Revenue for Apple"` â†’ Reversed word order supported
- `"What was Tesla's profit last quarter?"` â†’ Temporal queries with natural language
- `"Top 5 companies by revenue"` â†’ Ranking queries
- `"How has Microsoft's revenue changed over time?"` â†’ Trend analysis queries

All responses include clickable SEC filing sources, charts, and exportable reports (PowerPoint, PDF, Excel).

---

## ðŸ› ï¸ Complete Setup Guide

### **Step 1: Install Dependencies**

#### Prerequisites
- **Python 3.10+** (Python 3.11 or 3.12 recommended)
- **pip** (Python package manager)
- **Git** (to clone the repository)

#### Installation Steps

**1. Clone the Repository**
```bash
git clone https://github.com/haniae/Team2-CBA-Project.git
cd Team2-CBA-Project
```

**2. Create Virtual Environment**
```bash
# Windows
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# macOS/Linux
python3 -m venv .venv
source .venv/bin/activate
```

**3. Install Python Dependencies**
```bash
# Upgrade pip first
pip install --upgrade pip

# Install all required packages
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

**4. Verify Installation**
```bash
# Check Python version (should be 3.10+)
python --version

# Verify key packages are installed
python -c "import fastapi, openai, pandas, sqlalchemy; print('âœ… Core packages installed')"

# Verify FinalyzeOS imports
python -c "from finanlyzeos_chatbot import load_settings; print('âœ… FinalyzeOS setup complete')"
```

**5. Configure Environment**
```bash
# Copy environment template
# Windows
Copy-Item .env.example .env

# macOS/Linux
cp .env.example .env

# Edit .env file and add your OpenAI API key (optional for testing)
# OPENAI_API_KEY=your_key_here
```

---

### **Step 2: Choose Your Data Ingestion Option**

Select the option that best fits your needs:

#### **Option 1: Quick Test (100 Companies) - âš¡ Fastest**
**Best for**: Quick testing, demos, learning the system  
**Time**: ~15-30 minutes  
**Database Size**: ~20-30 MB

```bash
# Create a custom ticker file with 100 companies
# Create file: data/tickers/test_100.txt with one ticker per line:
# AAPL
# MSFT
# GOOGL
# AMZN
# ... (add 100 tickers)

# Run ingestion
python scripts/ingestion/ingest_universe.py --universe-file data/tickers/test_100.txt --years 5 --chunk-size 10

# Load prices (optional, adds ~10 minutes)
python scripts/ingestion/load_historical_prices_15years.py
```

**Expected Results**:
- ~5,000-8,000 financial facts
- ~10,000-15,000 metric snapshots
- ~100,000 price records (if prices loaded)
- Database size: ~20-30 MB

---

#### **Option 2: Medium Coverage (250 Companies) - âš–ï¸ Balanced**
**Best for**: Testing with good coverage, small teams  
**Time**: ~1-2 hours  
**Database Size**: ~50-80 MB

```bash
# Create a custom ticker file with 250 companies
# Create file: data/tickers/test_250.txt with 250 tickers

# Run ingestion
python scripts/ingestion/ingest_universe.py --universe-file data/tickers/test_250.txt --years 10 --chunk-size 15 --resume

# Load prices (optional, adds ~30 minutes)
python scripts/ingestion/load_historical_prices_15years.py
```

**Expected Results**:
- ~15,000-25,000 financial facts
- ~40,000-60,000 metric snapshots
- ~250,000 price records (if prices loaded)
- Database size: ~50-80 MB

---

#### **Option 3: S&P 500 (500 Companies) - ðŸŽ¯ Recommended**
**Best for**: Production use, comprehensive analysis  
**Time**: ~2-3 hours  
**Database Size**: ~150-200 MB

```bash
# Ingest S&P 500 financial data (15 years)
python scripts/ingestion/ingest_sp500_15years.py

# Load historical prices (1-2 hours)
python scripts/ingestion/load_historical_prices_15years.py

# Verify ingestion
python scripts/utility/check_ingestion_status.py
```

**Expected Results**:
- ~50,000-80,000 financial facts
- ~150,000-250,000 metric snapshots
- ~1.7M+ price records
- Database size: ~150-200 MB
- **Coverage**: 500 companies, 15 years of data

---

#### **Option 4: S&P 1500 (1,500 Companies) - ðŸš€ Full Coverage**
**Best for**: Maximum coverage, institutional use  
**Time**: ~6-9 hours  
**Database Size**: ~850 MB

```bash
# Ingest S&P 1500 (S&P 500 + MidCap + SmallCap)
python scripts/ingestion/ingest_extended_universe.py

# Load historical prices (2-3 hours)
python scripts/ingestion/load_historical_prices_15years.py

# Verify ingestion
python scripts/utility/check_ingestion_status.py
```

**Expected Results**:
- ~240,000+ financial facts
- ~750,000+ metric snapshots
- ~1.7M+ price records
- Database size: ~850 MB
- **Coverage**: 1,500 companies, 18 years of data

---

### **Step 2.5: Complete Ingestion Scripts Reference & ChromaDB Indexing**

This section provides a comprehensive guide to all available ingestion scripts and how to index data into ChromaDB for RAG (Retrieval-Augmented Generation).

#### **ðŸ“Š All Available Ingestion Scripts**

**Primary Ingestion Scripts (Choose One):**

```bash
# Option 1: S&P 500, 15 Years (RECOMMENDED for first run)
# Best for: Quick setup, good coverage, reasonable time
# Time: 15-30 minutes | Coverage: ~500 companies, 15 years
python scripts/ingestion/ingest_sp500_15years.py

# Option 2: S&P 500, 20 Years
# Best for: More historical data
# Time: 20-40 minutes | Coverage: ~500 companies, 20 years
python scripts/ingestion/ingest_20years_sp500.py

# Option 3: Full Coverage Ingestion
# Best for: Maximum coverage across multiple indices
# Time: 1-2 hours | Coverage: Comprehensive (S&P 500, S&P 1500, custom universes)
python scripts/ingestion/full_coverage_ingestion.py

# Option 4: Batch Ingestion
# Best for: Custom ticker lists or specific companies
# Time: Varies | Coverage: Configurable
python scripts/ingestion/batch_ingest.py

# Option 5: Extended Universe
# Best for: Beyond S&P 500 (mid-cap and small-cap companies)
# Time: Varies | Coverage: Extended market
python scripts/ingestion/ingest_extended_universe.py

# Option 6: Company Facts API (Alternative Method)
# Best for: Using SEC Company Facts API directly
# Time: Varies | Coverage: API-based
python scripts/ingestion/ingest_companyfacts.py
# Or batch version:
python scripts/ingestion/ingest_companyfacts_batch.py
```

**Additional Data Ingestion Scripts:**

```bash
# Historical Price Data (run after SEC filing ingestion)
# Yahoo Finance - 15 years of historical prices
python scripts/ingestion/load_historical_prices_15years.py

# Current prices from Yahoo Finance
python scripts/ingestion/load_prices_yfinance.py

# Current prices from Stooq
python scripts/ingestion/load_prices_stooq.py

# Ticker-CIK Mapping (maps stock tickers to SEC CIK numbers)
python scripts/ingestion/load_ticker_cik.py

# Private Companies (if applicable)
python scripts/ingestion/ingest_private_companies.py
```

**Verify Ingestion Completed:**

```bash
# Check database status (shows SQLite and ChromaDB status)
python scripts/check_database_status.py --database data/sqlite/finanlyzeos_chatbot.sqlite3

# Expected output:
# - âœ… company_filings table exists
# - âœ… Filings in database: > 0 (e.g., "10,000" or more)
# - âœ… Sample filings shown
```

---

#### **ðŸ” Vector Database (ChromaDB) Indexing for RAG**

The vector database enables **semantic search** over multiple document types, allowing the chatbot to answer questions using comprehensive context from various financial sources.

**Available Collections:**
1. âœ… **SEC Filing Narratives** - MD&A, Risk Factors, Business Overview sections from 10-K and 10-Q filings
2. âœ… **User-Uploaded Documents** - PDFs, CSVs, and other documents uploaded through the web interface
3. ðŸ†• **Earnings Transcripts** - Earnings call transcripts with management commentary and Q&A
4. ðŸ†• **Financial News** - Recent news articles from Yahoo Finance and NewsAPI
5. ðŸ†• **Analyst Reports** - Professional equity research reports and analysis
6. ðŸ†• **Press Releases** - Company announcements and strategic updates
7. ðŸ†• **Industry Research** - Sector analysis and market trend reports

**Chunking Strategy**: All documents are split into 1500-character chunks with 200-character overlap for optimal retrieval

**How It Works:**
1. Documents are downloaded from SEC or loaded from your database
2. Narrative sections are extracted (MD&A, Risk Factors, Business Overview)
3. Text is chunked into smaller segments
4. Each chunk is embedded using `all-MiniLM-L6-v2` model (384 dimensions)
5. Embeddings are stored in ChromaDB for fast semantic search

**Check Vector Database Status:**

This is the **easiest way** to see how much data you have indexed:

**Windows PowerShell/CMD:**
```cmd
REM Quick status check - shows all collections and document counts
python scripts/utility/check_vector_db.py

REM Check specific database
python scripts/utility/check_vector_db.py --database data/financial.db

REM Check without showing sample documents
python scripts/utility/check_vector_db.py --no-samples
```

**What it shows:**
- âœ… Document counts for all 7 collections (SEC, earnings, news, analyst, press, industry, uploaded)
- âœ… Total document count across all collections
- âœ… Storage size in MB
- âœ… Sample documents from each collection

**Example Output:**
```
================================================================================
VECTOR DATABASE STATUS CHECK
================================================================================
Database: data/financial.db

ðŸ“Š Document Counts by Collection:
--------------------------------------------------------------------------------
  âœ… SEC narratives:             13,800 documents
  âœ… Uploaded documents:              5 documents
  âœ… Earnings transcripts:            0 documents
  âœ… Financial news:                  0 documents
  âœ… Analyst reports:                 0 documents
  âœ… Press releases:                  0 documents
  âœ… Industry research:               0 documents
  âœ… Portfolio spreadsheets:          0 documents
--------------------------------------------------------------------------------
  ðŸ“ˆ TOTAL:                      13,805 documents

  ðŸ’¾ Storage Size:                 45.23 MB
================================================================================
```

**Prerequisites:**

```bash
# Install ChromaDB dependencies (if not already installed)
pip install chromadb sentence-transformers
```

**Step 1: Test Indexing (RECOMMENDED - Start Here!)**

**Windows PowerShell/CMD:**
```cmd
REM Test with a single ticker first
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --fetch-from-sec --limit 3
```

**Step 2: Index All Tickers (S&P 500 or S&P 1500)**

**Windows PowerShell/CMD:**
```cmd
REM Process all S&P 500 tickers (482 companies)
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp500 --fetch-from-sec --limit 5

REM Process all S&P 1500 tickers (1,599 companies) - takes longer!
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp1500 --fetch-from-sec --limit 5

REM Test with first 10 tickers only
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp500 --fetch-from-sec --limit 3 --max-tickers 10

REM List available universes
python scripts/index_documents_for_rag.py --list-universes
```

**Step 3: Index from Existing Database (if you already have filings)**

```bash
# If your database already has company_filings table populated
python scripts/index_documents_for_rag.py --database data/financial.db --type sec
```

**Indexing Options:**

**Windows PowerShell/CMD:**
```cmd
REM Index specific ticker only
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --fetch-from-sec --limit 5

REM Index only 10-K filings (annual reports)
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --filing-type 10-K --fetch-from-sec --limit 5

REM Index only 10-Q filings (quarterly reports)
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --filing-type 10-Q --fetch-from-sec --limit 5

REM Index uploaded documents (user-uploaded PDFs, CSVs, etc.)
python scripts/index_documents_for_rag.py --database data/financial.db --type uploaded

REM Index earnings transcripts (NEW!)
python scripts/index_documents_for_rag.py --database data/financial.db --type earnings --ticker AAPL
REM Or use individual fetcher:
python scripts/fetchers/fetch_earnings_transcripts.py --database data/financial.db --ticker AAPL

REM Index financial news (NEW!)
python scripts/index_documents_for_rag.py --database data/financial.db --type news --ticker AAPL
REM Or use individual fetcher:
python scripts/fetchers/fetch_financial_news.py --database data/financial.db --ticker AAPL --limit 20

REM Index analyst reports (NEW!)
python scripts/index_documents_for_rag.py --database data/financial.db --type analyst --ticker AAPL
REM Or use individual fetcher:
python scripts/fetchers/fetch_analyst_reports.py --database data/financial.db --ticker AAPL --limit 10

REM Index press releases (NEW!)
python scripts/index_documents_for_rag.py --database data/financial.db --type press --ticker AAPL
REM Or use individual fetcher:
python scripts/fetchers/fetch_press_releases.py --database data/financial.db --ticker AAPL --limit 20

REM Index industry research (NEW!)
python scripts/index_documents_for_rag.py --database data/financial.db --type industry --sector Technology
REM Or use individual fetcher:
python scripts/fetchers/fetch_industry_research.py --database data/financial.db --sector Technology --limit 10

REM Index everything (all document types)
python scripts/index_documents_for_rag.py --database data/financial.db --type all --ticker AAPL --fetch-from-sec

REM Resume from a specific ticker (if interrupted)
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp500 --fetch-from-sec --limit 5 --start-from MSFT
```

**Batch Process All Document Types for Ticker Universe:**

**Windows PowerShell/CMD:**
```cmd
REM Process all S&P 1500 tickers with earnings, news, analyst, and press releases
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp1500

REM Test with first 10 tickers
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp1500 --max-tickers 10

REM Use S&P 500 instead
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp500

REM Resume from a specific ticker
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp1500 --start-from MSFT

REM Skip certain sources
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp1500 --skip-earnings

REM Custom limits per source
python scripts/fetchers/batch_fetch_all_sources_sp1500.py --database data/financial.db --universe sp1500 --news-limit 20 --analyst-limit 15
```

**What the batch fetcher does:**
- Processes all tickers in the specified universe (sp500, sp1500, etc.)
- For each ticker, fetches and indexes:
  - Earnings transcripts (Yahoo Finance primary, Seeking Alpha fallback)
  - Financial news (Yahoo Finance - reliable)
  - Analyst reports (Yahoo Finance primary, Seeking Alpha fallback)
  - Press releases (Company IR pages)
- Shows progress, time estimates, and statistics
- Handles errors gracefully and continues processing
- Can be resumed from any ticker if interrupted

**Time Estimates:**
- S&P 500 (482 companies): ~8-16 hours
- S&P 1500 (1,599 companies): ~25-50 hours

**Step 4: Verify Vector Database Status**

**Windows PowerShell/CMD:**
```cmd
REM Quick status check
python scripts/utility/check_vector_db.py

REM Detailed check
python scripts/utility/check_vector_db.py

REM Expected output:
REM - âœ… SEC narratives: X,XXX documents
REM - âœ… Uploaded documents: X documents
REM - âœ… Earnings transcripts: X documents (NEW!)
REM - âœ… Financial news: X documents (NEW!)
REM - âœ… Analyst reports: X documents (NEW!)
REM - âœ… Press releases: X documents (NEW!)
REM - âœ… Industry research: X documents (NEW!)
REM - âœ… Total: X,XXX documents
REM - ðŸ’¾ Storage Size: XX.XX MB
```

**What Gets Indexed:**

- **SEC Filings:**
  - **MD&A** (Management's Discussion and Analysis)
  - **Risk Factors**
  - **Business Overview**
  - Each section is chunked into ~1500 character pieces
  - Embedded using `all-MiniLM-L6-v2` model (384 dimensions)
  - Stored with metadata (ticker, filing type, fiscal year, section, etc.)

- **Uploaded Documents:**
  - Full text content from user-uploaded files
  - Chunked similarly for vectorization
  - Metadata includes filename, file type, conversation ID

- **Earnings Transcripts** (NEW!):
  - Management commentary and Q&A sessions
  - Forward guidance and strategic discussions
  - Earnings history and quarterly data
  - Sources: **Yahoo Finance (primary, reliable)**, Seeking Alpha (fallback, may be blocked), Company IR pages
  - Metadata: ticker, date, quarter, source URL

- **Financial News** (NEW!):
  - Recent news articles affecting stocks
  - Market sentiment and breaking news
  - Sources: **Yahoo Finance (reliable)**, NewsAPI (optional, requires API key)
  - Metadata: ticker, date, publisher, title, source URL

- **Analyst Reports** (NEW!):
  - Professional equity research and analysis
  - Price targets and investment theses
  - Analyst recommendations and upgrades/downgrades
  - Sources: **Yahoo Finance (primary, reliable)**, Seeking Alpha (fallback, may be blocked with 403)
  - Metadata: ticker, date, analyst, rating, target price

- **Press Releases** (NEW!):
  - Company announcements and strategic updates
  - Product launches and M&A news
  - Sources: Company IR pages
  - Metadata: ticker, date, category, title

- **Industry Research** (NEW!):
  - Sector analysis and market trends
  - Competitive landscape reports
  - Sources: SSRN, Government sources
  - Metadata: sector, industry, date, title

**Complete Workflow Example:**

**Windows PowerShell/CMD:**
```cmd
REM 1. Check current vector DB status
python scripts/utility/check_vector_db.py

REM 2. Test indexing with one ticker
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --ticker AAPL --fetch-from-sec --limit 3

REM 3. Verify indexing worked
python scripts/utility/check_vector_db.py

REM 4. Process all S&P 500 tickers (or use sp1500 for all 1,599 tickers)
python scripts/index_documents_for_rag.py --database data/financial.db --type sec --universe sp500 --fetch-from-sec --limit 5

REM 5. Check final status
python scripts/utility/check_vector_db.py
```

**Time Estimates:**
- **S&P 500**: ~500 tickers Ã— 2-5 min/ticker = **16-40 hours**
- **S&P 1500**: ~1,599 tickers Ã— 2-5 min/ticker = **50-125 hours**

**New Document Types (Available Now!):**

The system now supports indexing additional document types for richer financial analysis:

- **Earnings Transcripts**: Management commentary and Q&A from earnings calls
  ```cmd
  python scripts/fetchers/fetch_earnings_transcripts.py --database data/financial.db --ticker AAPL
  ```

- **Financial News**: Recent news articles affecting stocks
  ```cmd
  python scripts/fetchers/fetch_financial_news.py --database data/financial.db --ticker AAPL --limit 20
  ```

- **Analyst Reports**: Professional equity research and analysis
  ```cmd
  python scripts/fetchers/fetch_analyst_reports.py --database data/financial.db --ticker AAPL --limit 10
  ```

- **Press Releases**: Company announcements and strategic updates
  ```cmd
  python scripts/fetchers/fetch_press_releases.py --database data/financial.db --ticker AAPL --limit 20
  ```

- **Industry Research**: Sector analysis and market trends
  ```cmd
  python scripts/fetchers/fetch_industry_research.py --database data/financial.db --sector Technology --limit 10
  ```

**Or use the main indexing script for all types:**
```cmd
python scripts/index_documents_for_rag.py --database data/financial.db --type all --ticker AAPL
```

See [Fetcher Scripts Usage Guide](docs/guides/FETCHER_SCRIPTS_USAGE.md) for detailed documentation.

**Tips:**
- Start with `--max-tickers 10` to test
- Use `--limit 5` to get 5 filings per ticker (enough for recent data)
- Run overnight for full universe processing
- Use `--start-from TICKER` to resume if interrupted
- **New**: Index additional document types for comprehensive analysis

**Troubleshooting Vector Database Indexing:**

**Windows PowerShell/CMD:**
```cmd
REM Issue: "no such table: company_filings"
REM Solution: Script auto-creates tables, but you can manually initialize:
python -c "from finanlyzeos_chatbot.database import initialise; from pathlib import Path; initialise(Path('data/financial.db'))"

REM Issue: ChromaDB not available
REM Solution: Install dependencies
pip install chromadb sentence-transformers requests beautifulsoup4

REM Issue: "Unknown ticker universe: sp1500"
REM Solution: Check available universes
python scripts/index_documents_for_rag.py --list-universes

REM Issue: MemoryError during processing
REM Solution: Already fixed! Script now limits section sizes and chunk counts

REM Issue: No sections extracted
REM Solution: Parser uses fallback extraction. Some filings may not have standard sections.

REM Issue: SEC API returns 0 filings
REM Solution: Try different ticker, check internet connection, wait a few minutes (rate limiting)
```

**Performance Tips:**

- **Start Small**: Test with `--max-tickers 10` before processing full universe
- **Use Limits**: `--limit 5` gets 5 filings per ticker (sufficient for recent data)
- **Run Overnight**: Full S&P 500/1500 processing takes many hours
- **Resume Support**: Use `--start-from TICKER` if interrupted
- **Monitor Progress**: Script shows progress for each ticker
- **Check Status**: Run `python scripts/utility/check_vector_db.py` anytime to see current counts

**Check Vector Database Anytime:**
```cmd
python scripts/utility/check_vector_db.py
```

---

### **Step 3: Verify Your Setup**

After ingestion completes, verify everything works:

```bash
# Check database status
python scripts/utility/check_ingestion_status.py

# Or use the simple checker
python scripts/utility/check_correct_database.py

# Test the chatbot
python run_chatbot.py
```

**Test Queries**:
- "Show me Apple's revenue"
- "Compare Microsoft and Google's profit margins"
- "What's Tesla's free cash flow in 2023?"

---

### **Step 4: Start Using FinalyzeOS**

#### **Option A: Command Line Interface (CLI)**
```bash
python run_chatbot.py
```

#### **Option B: Web Interface**
```bash
python serve_chatbot.py --port 8000
```
Then open: `http://localhost:8000`

---

### ðŸš€ Quick Start Examples

**Try these queries immediately after setup:**

#### In CLI (`python run_chatbot.py`):
```
> What is Apple's revenue?
> Compare Microsoft and Google's profit margins
> Show me Tesla's free cash flow in 2023
> Why is NVDA's stock price increasing?
> Forecast Microsoft's revenue for 2026
```

#### In Web UI (`http://localhost:8000`):
1. Type: `"Show me Apple's dashboard"`
2. Type: `"Compare AAPL vs MSFT vs GOOGL"`
3. Type: `"What's driving Tesla's revenue growth?"`
4. Click **"Export PowerPoint"** to download a presentation

**Expected Response Times:**
- Simple queries: < 2 seconds
- Comparisons: 3-5 seconds
- Dashboards: 5-8 seconds
- Forecasts: 10-15 seconds

---

## ðŸ“Š Ingestion Comparison Table

| Option | Companies | Time | Database Size | Best For |
|--------|-----------|------|---------------|----------|
| **Quick Test** | 100 | 15-30 min | 20-30 MB | Learning, demos |
| **Medium** | 250 | 1-2 hours | 50-80 MB | Testing, small teams |
| **S&P 500** | 500 | 2-3 hours | 150-200 MB | Production use â­ |
| **S&P 1500** | 1,500 | 6-9 hours | 850 MB | Full coverage |

---

## ðŸ”§ Troubleshooting Setup

### **Issue: Package Installation Fails**
```bash
# Upgrade pip first
pip install --upgrade pip

# Try installing in smaller batches
pip install fastapi uvicorn python-dotenv
pip install openai requests httpx
pip install pandas numpy sqlalchemy
pip install -r requirements.txt
```

### **Issue: Database Not Found**
```bash
# The database will be created automatically on first run
# Or manually initialize:
python -c "from finanlyzeos_chatbot.database import initialise; from pathlib import Path; initialise(Path('data/sqlite/finanlyzeos_chatbot.sqlite3'))"
```

### **Issue: Ingestion Stops or Fails**
```bash
# Most scripts support resume capability
# Just run the same command again - it will resume from where it stopped

# Check progress
python scripts/ingestion/monitor_ingestion.py

# Check for errors
python scripts/utility/check_ingestion_status.py
```

### **Issue: Out of Memory During Ingestion**
```bash
# Reduce batch size
python scripts/ingestion/ingest_universe.py --universe-file your_tickers.txt --years 10 --chunk-size 5

# Or process in smaller chunks
# Split your ticker file into multiple smaller files and process separately
```

---


## ðŸ“Š Current Data Coverage

The database currently contains **2,880,138 total rows** of financial data across 1,505 companies:

| Table | Rows | Description |
|-------|------|-------------|
| market_quotes | 1,730,061 | Historical daily price data (15+ years) |
| metric_snapshots | 777,979 | Pre-calculated analytics and KPIs |
| financial_facts | 243,777 | Raw SEC filing data (revenue, expenses, etc.) |
| company_filings | 80,332 | SEC filing metadata (10-K, 10-Q forms) |
| audit_events | 26,338 | Data ingestion and processing logs |
| kpi_values | 16,071 | KPI backfill and override values |
| ticker_aliases | 1,507 | Company ticker mappings |
| conversations | 3,979 | Chat history and user interactions |
| portfolio_holdings | 14 | Portfolio positions |
| scenario_results | 2 | Saved scenario analysis results |

### ðŸ“ˆ Data Characteristics

- ðŸ“… **Year Range:** 2009-2027 (18 years of coverage)
- ðŸ¢ **Companies:** 1,599 unique tickers supported (S&P 1500: S&P 500 + S&P 400 + S&P 600)
- ðŸ“Š **Metrics:** 93 unique financial metrics with 200+ natural language synonyms
- ðŸ”¤ **Natural Language:** 150+ question patterns, 40+ intent types, spelling mistake handling
- ðŸ“¡ **Data Sources:** SEC EDGAR (10-K, 10-Q filings), Yahoo Finance (market quotes), FRED, IMF
- ðŸ”„ **Update Frequency:** On-demand ingestion with smart gap detection
- ðŸ” **Audit Trail:** Full lineage tracking for every data point
- ðŸ’¾ **Database Size:** ~850 MB (SQLite file)

### ðŸ“Š Coverage Status Definitions

The Company Universe view categorizes companies by data completeness:

| Status | Criteria | Description |
|--------|----------|-------------|
| **âœ… Complete** | 5+ years AND 12+ metrics | Good historical coverage with comprehensive metrics |
| **âš ï¸ Partial** | 2-4 years OR 6-11 metrics | Some data available but could use more years or metrics |
| **âŒ Missing** | <2 years OR <6 metrics | Very little data or no data available |

**Note:** The chatbot can access **all 2.88M rows** of data regardless of coverage status. The coverage label is a UI indicator showing data completeness, not access restrictions.

**Current Coverage:**
- âœ… **Complete:** 1,035 companies (68%)
- âš ï¸ **Partial:** 469 companies (31%)
- âŒ **Missing:** 13 companies (1%)

To improve coverage, run: `python scripts/ingestion/full_coverage_ingestion.py --years 20`

## âš¡ Core Capabilities

- ðŸ’¬ **Multi-Channel Chat** â€“ CLI REPL, REST API endpoints, and browser client with live status indicators
- ðŸ“Š **Deterministic Analytics** â€“ Calculate primary/secondary metrics, growth rates, valuation multiples, and derived KPIs from the latest filings and quotes
- ðŸ“¥ **Incremental Ingestion** â€“ Pull SEC EDGAR facts, Yahoo quotes, and optional Bloomberg feeds with retry/backoff
- ðŸ”’ **Audit-Ready Storage** â€“ Complete metric snapshots, raw financial facts, audit events, and full chat history for compliance reviews
- ðŸ¤– **Extensible LLM Layer** â€“ Toggle between local echo model and OpenAI, or extend for other vendors
- ðŸ”„ **Task Orchestration** â€“ Queue abstraction for ingestion and long-running commands
- ðŸŽ¯ **Advanced Natural Language Processing** â€“ 100% query pattern detection, 90% company name spelling correction, 100% metric spelling correction, 40+ intent types, 150+ question patterns, 200+ metric synonyms
- ðŸ¢ **Comprehensive Company Coverage** â€“ Full support for all 1,599 S&P 1500 companies (S&P 500 + S&P 400 + S&P 600) via ticker symbol or company name
- ðŸ¤– **8 ML Forecasting Models** â€“ ARIMA, Prophet, ETS, LSTM, GRU, Transformer, Ensemble, and Auto selection for institutional-grade predictions
- ðŸ“š **Enhanced RAG Integration** â€“ Explicit data dumps, comprehensive context building, response verification, and technical detail enforcement for ML forecasts

## ðŸš€ Advanced Analytics 

Four sophisticated analytics modules deliver institutional-grade capabilities:

### 1ï¸âƒ£ Sector Benchmarking (`sector_analytics.py`)
- ðŸ­ Compare companies within 11 GICS sectors (Technology, Financials, Healthcare, etc.)
- ðŸ“Š Calculate sector-wide averages/medians for all key metrics
- ðŸ† Identify top performers and percentile rankings
- ðŸ’¡ **Example:** "Apple ranks 100th percentile for revenue in Technology with $391B vs sector avg $49B"

### 2ï¸âƒ£ Anomaly Detection (`anomaly_detection.py`)
- ðŸ” Statistical detection using Z-score analysis with configurable thresholds
- âš ï¸ Identifies outliers in revenue growth, margins, cash flow, balance sheet ratios
- ðŸš¨ Severity classification (low/medium/high/critical) with confidence scores
- ðŸ’¡ **Example:** "Revenue growth spike: 51.2% vs historical avg 23.5% (3.2 std devs, high severity)"

### 3ï¸âƒ£ Predictive Analytics (`predictive_analytics.py`)
- ðŸ”® Forecast metrics using linear regression and CAGR projections
- ðŸ“ˆ Confidence intervals and trend classification (increasing/decreasing/stable/volatile)
- ðŸŽ¯ Scenario analysis (optimistic/base/pessimistic)
- ðŸ’¡ **Example:** "MSFT revenue forecast 2026: $280.9B (CAGR: 13.78%, increasing trend, 66% confidence)"

### 4ï¸âƒ£ Advanced KPI Calculator (`advanced_kpis.py`)
- ðŸ’° 30+ sophisticated ratios: ROE, ROA, ROIC, ROCE, debt-to-equity, interest coverage, FCF metrics
- ðŸ“‹ Categorized outputs: profitability, liquidity, leverage, efficiency, cash flow
- ðŸ’¡ **Example:** "Apple: ROE 164.59%, ROIC 49.60%, FCF-to-Revenue 32.66%, Debt-to-Equity 5.41"

**Documentation:** See `docs/PHASE1_ANALYTICS_FEATURES.md` for complete API reference and integration examples.  
**Test Suite:** Run `python test_new_analytics.py` to see live demonstrations with real S&P 500 data.

These modules transform FinalyzeOS into a professional analytics platform comparable to Bloomberg Terminal and FactSet.

## ðŸ¤– Machine Learning Stack

FinalyzeOS blends deterministic analytics with a modular ML layer so finance teams can prototype forecasts without giving up auditability. The ML forecasting system integrates seamlessly with the advanced natural language processing layer, automatically handling spelling mistakes in company names and metrics, and recognizing forecast-related queries through 40+ intent types.

### Architecture Overview

- **Data Foundation:** `analytics_engine.AnalyticsEngine.refresh_metrics()` normalises SEC filings into `metric_snapshots`. Forecast pipelines consume the same curated metrics, keeping model inputs aligned with what the dashboard renders.
- **Model Registry:** Classical (Prophet, ARIMA/ETS) and ML estimators (LSTM, GRU, Transformer) live under `src/finanlyzeos_chatbot/ml_forecasting/`. **8 forecasting models** available: ARIMA, Prophet, ETS, LSTM, GRU, Transformer, Ensemble, and Auto (automatic selection). Shared base classes (`ml_forecasting.ml_forecaster`) expose a consistent interface so new models can be dropped in with minimal wiring.
- **Context Builder:** `context_builder.build_forecast_context()` assembles explicit data dumps (predictions, confidence bands, training diagnostics, model architecture, hyperparameters) that are injected verbatim into the LLM prompt. The bot cannot answer without citing these artefacts. Includes comprehensive technical details for institutional analysts.
- **Natural Language Integration:** The ML forecasting system automatically recognizes forecast queries through advanced intent detection, handles spelling mistakes in company names (90% success rate) and metrics (100% success rate), and supports natural language variations like "forecast", "predict", "project", "outlook", "what if", etc.

### Forecast Workflow

1. **Trigger:** The intent router flags a forecasting query (see `routing/enhanced_router.py`).  
2. **Dataset Assembly:** Historical metrics are pulled from SQLite or Postgres and preprocessed (`predictive_analytics.prepare_training_series`).  
3. **Model Selection:** The ensemble coordinator benchmarks candidates, caching scores so repeated queries stay performant.  
4. **Output Packaging:** Predictions, bull/base/bear scenarios, CAGR deltas, and sector benchmarks are serialised into the forecast context.  
5. **Conversation Delivery:** `FinalyzeOSChatbot.ask()` appends the forecast context to the conversational history before calling the LLM client.

### Guardrails & Verification

- `ml_response_verifier.verify_ml_forecast_response()` checks that every figure from the explicit data dump appears in the generated answer and back-fills omissions.
- `response_verifier.verify_response()` plus the `confidence_scorer` attach a confidence footer and can redact the reply if confidence falls below configurable thresholds.
- Structured fallbacks prevent snapshots or dashboards from leaking into forecast responses; missing numbers yield a polite apology instead of hallucinations.

### Developer Workflow

- **Enable/Disable:** Toggle forecasting via the runtime settings object (`config.get_settings().forecasting_enabled`) or by exporting the matching environment variable (see `config.py` for names).  
- **Refresh Data:** `python scripts/ingestion/fill_data_gaps.py --ticker AAPL --years-back 5` hydrates the metric store before training.  
- **Unit Tests:** `pytest tests/unit/test_analytics_engine.py tests/unit/test_analysis_templates.py` cover metric hydration, forecast assembly, and verification hooks.  
- **Interactive Checks:** In a Python shell run `from finanlyzeos_chatbot.predictive_analytics import build_forecast_payload` to assemble the forecast dictionary for a given ticker/metric before handing it to the chatbot.

- `src/finanlyzeos_chatbot/context_builder.py` â€“ forecast context orchestration.  
- `src/finanlyzeos_chatbot/predictive_analytics.py` â€“ training/evaluation utilities and scenario generation.  
- `src/finanlyzeos_chatbot/ml_forecasting/` â€“ individual model implementations and preprocessing helpers.  
- `src/finanlyzeos_chatbot/ml_response_verifier.py` â€“ forecast-specific guardrails.  
- `src/finanlyzeos_chatbot/response_verifier.py` & `confidence_scorer.py` â€“ cross-cutting verification and confidence scoring.

## ðŸ“š Retrieval-Augmented Generation

Natural-language answers are grounded in auditable data through a **production-grade RAG stack** that combines structured metrics, uploaded documents, semantic search, and advanced retrieval features.


### Document Lifecycle

1. **Upload:** The frontend posts to `/api/documents/upload`; FastAPI persists the binary, metadata, and extracted text alongside the active `conversation_id` (`web.py`, `database.store_uploaded_document`).  
2. **Extraction:** File-type specific parsers normalise text and capture warnings (e.g., OCR failures) that are surfaced back to the user and stored for context generation.  
3. **Indexing:** Documents are **automatically indexed** into ChromaDB for semantic search, with fallback to SQLite for deterministic recall. Every snippet can be reviewed in audits.

### Prompt-Aware Retrieval

- **Semantic Search**: `document_context.build_uploaded_document_context()` uses vector embeddings for semantic search over uploaded documents, with automatic fallback to token overlap matching.
- **Vector Store**: ChromaDB with `all-MiniLM-L6-v2` embeddings (384 dimensions) for fast semantic search.
- **Chunking Strategy**: 1500 characters with 200 overlap to prevent breaking mid-sentence.
- **Reranking**: Cross-encoder reranking improves relevance by ~10-20% over initial retrieval.
- Chunk overlap, snippet length, and stop-word lists are configurable, letting admins tighten or loosen recall depending on compliance needs.
- Matched terms, file metadata, and extraction warnings are embedded directly in the context so the model can cite sources verbatim.

### Context Fusion

- `FinalyzeOSChatbot.ask()` merges multiple layers: portfolio analytics, financial KPI context, SEC filing narratives (semantic search), and uploaded document snippets.  
- **Source Fusion**: Normalizes scores across sources and applies reliability weights (SQL=1.0, SEC=0.9, Uploaded=0.7).
- **Confidence Scoring**: Computes overall retrieval confidence and adjusts LLM tone accordingly.
- A document-follow-up heuristic (`_is_document_followup`) skips ticker summary heuristics when the user says "summarise it" immediately after an upload.  
- When heuristics cannot serve the request, the bot falls back to a plain conversational instruction set ensuring non-financial prompts still receive responses.

### Quality & Monitoring

- **Unit Tests:** `pytest tests/unit/test_document_upload.py tests/unit/test_uploaded_document_context.py` guard conversation linkage and snippet relevance.  
- **Evaluation Harness:** `scripts/evaluate_rag.py` computes retrieval metrics (Recall@K, MRR, nDCG) and QA metrics (exact match, factual consistency).
- **Observability:** Comprehensive logging of retrieval counts, scores, timing, document IDs, and anomalies via `RAGObserver`.
- **Guardrails:** Min relevance score (0.3), max context chars (15000), max documents (10), anomaly detection.
- **Telemetry:** Progress events (e.g., `context_sources_ready`, `upload_complete`) are emitted via Server-Sent Events so the UI can surface status breadcrumbs.  
- **Operational Runbooks:** Refer to `docs/guides/PORTFOLIO_QUESTIONS_GUIDE.md` (upload section) and inline module docstrings for end-to-end walkthroughs when onboarding analysts.

### Key Modules

- `src/finanlyzeos_chatbot/rag_retriever.py` â€“ Unified retrieval interface (SQL + vector search)
- `src/finanlyzeos_chatbot/rag_reranker.py` â€“ Cross-encoder reranking
- `src/finanlyzeos_chatbot/rag_fusion.py` â€“ Source fusion and confidence scoring
- `src/finanlyzeos_chatbot/rag_grounded_decision.py` â€“ Grounded decision layer
- `src/finanlyzeos_chatbot/rag_memory.py` â€“ Memory-augmented RAG
- `src/finanlyzeos_chatbot/rag_controller.py` â€“ Multi-hop query decomposition
- `src/finanlyzeos_chatbot/rag_observability.py` â€“ Observability and guardrails
- `src/finanlyzeos_chatbot/rag_orchestrator.py` â€“ Complete RAG orchestrator
- `src/finanlyzeos_chatbot/rag_prompt_template.py` â€“ RAG prompt template builder
- `src/finanlyzeos_chatbot/document_context.py` â€“ Prompt-aware chunking and snippet assembly
- `src/finanlyzeos_chatbot/chatbot.py` â€“ Document-aware intent routing and context fusion
- `src/finanlyzeos_chatbot/static/app.js` & `webui/app.js` â€“ Frontend upload orchestration with persistent `conversation_id`s
- `src/finanlyzeos_chatbot/web.py` â€“ Backend API endpoint, validation, and database persistence

## ðŸ”§ Troubleshooting

- Virtual environment not activating (Windows PowerShell): run `Set-ExecutionPolicy -Scope CurrentUser RemoteSigned` once, then activate with `.\.venv\Scripts\Activate.ps1`.
- SQLite locked errors: stop running servers/REPLs, wait a few seconds, try again. On Windows, ensure no other process (e.g., file indexers) holds the DB.
- pip install issues: upgrade pip (`python -m pip install --upgrade pip`) and retry `pip install -r requirements.txt`.
- Missing quotes/market data: re-run an ingestion command (e.g., `python scripts/ingestion/fill_data_gaps.py --years-back 3 --batch-size 10`) or narrow to specific tickers using `--ticker`.
- Port already in use: change `--port` (e.g., `python app/serve_chatbot.py --port 8010`) or stop the conflicting process.

## ðŸ“Š Portfolio Management 

FinalyzeOS includes comprehensive **portfolio management capabilities** that enable institutional-grade portfolio analysis, optimization, and risk management. The portfolio system supports multiple portfolios, automatic detection from user queries, and sophisticated analytics.

### ðŸŽ¯ Core Portfolio Features

#### 1. **Portfolio Detection & Management**
- **Automatic Detection**: System automatically detects portfolio-related queries and extracts portfolio identifiers
- **Multiple Portfolios**: Support for managing multiple portfolios simultaneously (e.g., `port_abc123`, `port_xyz789`)
- **Portfolio Upload**: Upload portfolios via CSV files with ticker, weight, or shares+price columns
- **Portfolio Storage**: Persistent storage of portfolio holdings, metadata, and analysis results

#### 2. **Portfolio Holdings Analysis**
- **Holdings Display**: View all tickers, weights, shares, current prices, and market values
- **Fundamental Enrichment**: Automatic enrichment with P/E ratios, dividend yields, ROE, ROIC, and sector classifications
- **Sector Breakdown**: GICS sector classification for all holdings
- **Concentration Metrics**: HHI (Herfindahl-Hirschman Index), top 10 concentration, max weights


#### 3. **Portfolio Exposure Analysis**
- **Sector Exposure**: Weight breakdown across 11 GICS sectors (Technology, Financials, Healthcare, etc.)
- **Factor Exposure**: Beta, momentum, value, size, and quality factor exposures
- **Concentration Analysis**: HHI, top 10 concentration ratio, maximum position weights
- **Geographic Exposure**: Regional allocation (if available in data)


#### 4. **Portfolio Optimization**
- **Mean-Variance Optimization**: Optimize for maximum Sharpe ratio, minimum variance, or target return
- **Constraint Support**: Sector limits, position limits, turnover constraints
- **Rebalancing Recommendations**: Specific buy/sell recommendations with expected impact
- **Performance Projections**: Expected return, variance, and Sharpe ratio for optimized portfolio


#### 5. **Performance Attribution (Brinson-Fachler)**
- **Active Return Decomposition**: Total active return broken down into allocation, selection, and interaction effects
- **Top Contributors**: Best performing positions and their contribution to portfolio returns
- **Top Detractors**: Worst performing positions and their impact
- **Sector-Level Analysis**: Which sectors drove portfolio performance


#### 6. **Risk Metrics & Stress Testing**
- **CVaR (Conditional Value at Risk)**: Expected shortfall at 95% confidence level
- **VaR (Value at Risk)**: Maximum expected loss at specified confidence level
- **Volatility**: Portfolio volatility and individual position contributions
- **Sharpe Ratio**: Risk-adjusted return metric
- **Sortino Ratio**: Downside risk-adjusted return
- **Tracking Error**: Active risk vs. benchmark (S&P 500)
- **Beta**: Portfolio beta vs. market


#### 7. **Scenario Analysis & Stress Testing**
- **Equity Drawdown Scenarios**: Test portfolio performance under market crashes (e.g., -20%, -30%)
- **Sector Rotation Scenarios**: Analyze impact of sector-specific shocks (e.g., tech sector -30%)
- **Custom Scenarios**: Define custom market scenarios with position-specific impacts
- **Monte Carlo Simulation**: Probabilistic scenario analysis with thousands of simulations


#### 8. **ESG & Sustainability Analysis**
- **ESG Scores**: Overall portfolio ESG score and component scores (Environmental, Social, Governance)
- **Holding-Level ESG**: ESG scores for individual positions
- **Sector ESG**: Average ESG scores by sector
- **Controversy Detection**: Portfolio controversy level and flagging of controversial holdings


#### 9. **Tax Analysis**
- **Tax Liability Estimation**: Estimated taxes if positions were sold
- **Tax-Adjusted Returns**: Returns after accounting for tax implications
- **Gain/Loss Breakdown**: Unrealized gains and losses by position
- **Holding Period Classification**: Short-term vs. long-term capital gains
- **Wash Sale Detection**: Identification of potential wash sale issues


#### 10. **Diversification Analysis**
- **Diversification Ratio**: Measure of diversification benefit
- **Effective Number of Holdings**: Equivalent number of equal-weighted positions
- **Risk Contribution Analysis**: Which positions drive portfolio risk
- **Diversification Recommendations**: Specific suggestions to improve diversification


#### 11. **Portfolio Export & Reporting**
- **PowerPoint Export**: 12-slide professional presentation with portfolio summary, holdings, exposure charts, performance attribution, risk metrics, and recommendations
- **PDF Export**: Multi-page PDF report with executive summary, holdings table, charts, and risk analysis
- **Excel Export**: Multi-tab workbook with holdings sheet, exposure breakdowns, performance attribution table, and risk metrics


### ðŸ“‹ Portfolio Data Structure

Portfolios are stored with the following structure:
- **Portfolio ID**: Unique identifier (e.g., `port_abc123`)
- **Holdings**: List of tickers with weights, shares, prices, and market values
- **Metadata**: Creation date, last updated, portfolio name, description
- **Statistics**: Pre-calculated portfolio statistics (P/E, dividend yield, concentration, etc.)
- **Risk Metrics**: Pre-calculated risk metrics (CVaR, VaR, volatility, Sharpe ratio, etc.)
- **Exposure**: Sector and factor exposure breakdowns

### ðŸ”§ Technical Implementation

**Key Files:**
- `src/finanlyzeos_chatbot/portfolio.py` - Main portfolio management module
- `src/finanlyzeos_chatbot/portfolio_optimizer.py` - Portfolio optimization algorithms
- `src/finanlyzeos_chatbot/portfolio_risk_metrics.py` - Risk metric calculations
- `src/finanlyzeos_chatbot/portfolio_attribution.py` - Performance attribution (Brinson-Fachler)
- `src/finanlyzeos_chatbot/portfolio_scenarios.py` - Scenario analysis and stress testing
- `src/finanlyzeos_chatbot/portfolio_export.py` - Export functionality (PowerPoint, PDF, Excel)

**Documentation:** See `docs/guides/PORTFOLIO_QUESTIONS_GUIDE.md` for complete portfolio query examples and response formats.

## ðŸ¤– Machine Learning Forecasting (NEW)

FinalyzeOS includes **sophisticated machine learning forecasting capabilities** that provide institutional-grade financial predictions using multiple ML models. The forecasting system integrates seamlessly with the RAG layer to provide detailed, technically accurate forecasts.

### ðŸŽ¯ ML Forecasting Models

FinalyzeOS supports **7 different ML forecasting models**, each optimized for different use cases:

#### 1. **ARIMA (AutoRegressive Integrated Moving Average)**
- **Best For**: Short-term forecasts, trend-following patterns
- **Method**: Statistical time series model with auto-regression and moving averages
- **Hyperparameters**: Automatically optimized using AIC/BIC criteria
- **Features**: Handles seasonality, trend decomposition, confidence intervals

#### 2. **Prophet (Facebook's Time Series Forecasting)**
- **Best For**: Seasonal patterns, holidays, long-term trends
- **Method**: Additive time series model with seasonality components
- **Hyperparameters**: Automatically tuned for yearly, weekly, daily seasonality
- **Features**: Handles missing data, outliers, changepoints

#### 3. **ETS (Exponential Smoothing State Space Model)**
- **Best For**: Smooth trends, exponential growth/decay patterns
- **Method**: State space model with error, trend, and seasonality components
- **Hyperparameters**: Automatically selected from 30 possible model configurations
- **Features**: Handles additive/multiplicative trends and seasonality

#### 4. **LSTM (Long Short-Term Memory Neural Network)**
- **Best For**: Complex patterns, non-linear relationships, long-term dependencies
- **Method**: Deep learning recurrent neural network
- **Architecture**: Multi-layer LSTM with dropout, batch normalization
- **Training**: Optimized with Adam optimizer, early stopping, learning rate scheduling
- **Features**: Handles complex patterns, learns from historical data

#### 5. **GRU (Gated Recurrent Unit)**
- **Best For**: Similar to LSTM but faster training, similar accuracy
- **Method**: Simplified RNN architecture with gating mechanisms
- **Architecture**: Multi-layer GRU with dropout and batch normalization
- **Training**: Optimized with Adam optimizer, early stopping
- **Features**: Faster than LSTM, good for real-time forecasting

#### 6. **Transformer (Attention-Based Architecture)**
- **Best For**: Long-term dependencies, complex patterns, attention to important periods
- **Method**: Attention-based neural network architecture
- **Architecture**: Multi-head attention, positional encoding, feed-forward layers
- **Training**: Optimized with Adam optimizer, learning rate scheduling
- **Features**: State-of-the-art for time series with long dependencies

#### 7. **Ensemble (Combines Multiple Models)**
- **Best For**: Maximum accuracy, robust predictions
- **Method**: Weighted combination of ARIMA, Prophet, ETS, LSTM, GRU, and Transformer
- **Weighting**: Optimized based on historical performance
- **Features**: Best accuracy, reduces model-specific errors

#### 8. **Auto (Automatic Model Selection)**
- **Best For**: Ease of use, automatic best model selection
- **Method**: Automatically selects best-performing model based on historical data
- **Selection Criteria**: Cross-validation performance, AIC/BIC, forecast accuracy
- **Features**: No need to specify model - system picks the best one

### ðŸ“Š Forecasting Capabilities

#### **Supported Metrics:**
- **Revenue/Sales**: Revenue forecasts with growth rates
- **Net Income/Earnings**: Earnings forecasts with margin analysis
- **Free Cash Flow**: Cash flow forecasts with FCF margin
- **EBITDA**: EBITDA forecasts with margin trends
- **Other Metrics**: Profit, margin, EPS, assets, liabilities, and more

#### **Forecast Horizons:**
- **Short-term**: 1-2 years (recommended for ARIMA, Prophet)
- **Medium-term**: 3-5 years (recommended for LSTM, GRU, Transformer)
- **Long-term**: 5+ years (recommended for Ensemble, Auto)

#### **Forecast Outputs:**
- **Point Forecasts**: Predicted values for each period
- **Confidence Intervals**: 95% confidence intervals (upper and lower bounds)
- **Growth Rates**: Year-over-year growth rates and multi-year CAGR
- **Trend Classification**: Increasing, decreasing, stable, or volatile trends
- **Model Confidence**: Confidence score (0-1) indicating forecast reliability
- **Technical Details**: Complete model architecture, hyperparameters, training details

### ðŸ” Enhanced RAG Integration

The ML forecasting system is **deeply integrated with the RAG layer** to provide comprehensive, technically detailed forecasts. The system automatically handles spelling mistakes in company names and metrics, recognizes forecast-related queries through 40+ intent types, and supports natural language variations.

#### **1. Explicit Data Dump Section**
- **Purpose**: Ensures LLM receives ALL technical details in structured format
- **Content**: Model architecture (layers, units, activation functions), hyperparameters (learning rate, batch size, epochs, optimizer), training details (loss values, validation metrics, early stopping), computational details (training time, memory usage), model-specific parameters (ARIMA orders, Prophet seasonality, LSTM/GRU/Transformer architecture)
- **Format**: Key-value pairs for easy extraction and inclusion in responses
- **Mandate**: LLM is explicitly instructed to include EVERY value from this section without summarization
- **Spelling Handling**: Company names and metrics in forecasts are automatically corrected for spelling mistakes before model execution

#### **2. Enhanced Context Building**
The `context_builder.py` module builds comprehensive ML forecast context including:
- **Forecast Values**: All predicted values with confidence intervals
- **Model Details**: Complete technical specifications (layers, units, epochs, loss, learning rate, batch size, etc.)
- **Training Process**: Training loss, validation loss, early stopping, learning rate schedule
- **Data Preprocessing**: Scaling methods, outlier handling, missing data treatment
- **Feature Engineering**: Features created for the model
- **Model Selection**: Why this model was chosen, alternative models considered
- **Performance Metrics**: Training metrics, validation metrics, forecast accuracy
- **Forecast Analysis**: Year-over-year growth, CAGR, confidence interval uncertainty
- **Sector Comparison**: How forecast compares to sector averages and peers
- **Scenario Analysis**: Bull/base/bear scenarios based on confidence intervals
- **Risk Analysis**: Model confidence, downside risks, upside opportunities

#### **3. Response Verification**
The `ml_response_verifier.py` module ensures responses include all required technical details:
- **Strict Mode**: When "EXPLICIT DATA DUMP" is present, verifies exact value matches
- **Required Checks**: Verifies presence of model architecture, hyperparameters, training details, computational details
- **Enhancement**: Automatically appends missing technical details if LLM response is incomplete
- **Categorization**: Groups missing details by category (architecture, training, hyperparameters, etc.)

#### **4. System Prompt Enhancements**
The chatbot system prompt includes explicit instructions for ML forecasts:
- **Mandatory Inclusion**: Instructions to include EVERY value from "EXPLICIT DATA DUMP"
- **No Summarization**: Explicit prohibition against summarizing technical details
- **Exact Values**: Instructions to use exact numerical values (e.g., "training loss is 0.001234" not "training loss is low")
- **Technical Depth**: Minimum 500-1000 words for forecast responses
- **Professional Formatting**: Markdown formatting guidelines for professional presentation


### ðŸ“š Documentation

- **Complete Prompt Guide**: See `docs/guides/ALL_ML_FORECASTING_PROMPTS.md` for all working forecast prompts
- **Quick Reference**: See `docs/guides/ML_FORECASTING_QUICK_REFERENCE.md` for quick reference guide
- **Technical Details**: See `src/finanlyzeos_chatbot/ml_forecasting/` for implementation details

## ðŸ—ï¸ Architecture Map

See [`docs/architecture.md`](docs/architecture.md) for the complete component diagram. The latest revision includes the structured parsing pipeline (alias_builder.py, parse.py, time_grammar.py) and the retrieval layer that feeds grounded artefacts into the LLM alongside the existing CLI, FastAPI, analytics, and ingestion components.

## ðŸ§  Retrieval & ML Internals

FinalyzeOS combines **deterministic data prep** with **retrieval-augmented generation (RAG)** so every answer traces back to persisted facts. The RAG layer has been significantly enhanced to support portfolio management and machine learning forecasting with comprehensive technical details.

### ðŸ”¤ Natural-Language Parsing (Deterministic)

- **S&P 1500 Coverage**: `src/finanlyzeos_chatbot/parsing/alias_builder.py` loads a generated `aliases.json` covering all **1,599 S&P 1500 companies** (S&P 500 + S&P 400 + S&P 600). It normalises free-text mentions, resolves ticker aliases, applies 85+ manual overrides (common misspellings, share classes), and when needed performs fuzzy fallback with spelling mistake handling.
- **Advanced NLP**: `parse_to_structured` in `parsing/parse.py` orchestrates alias resolution, metric synonym detection (93 metrics with 200+ synonyms), and the flexible time grammar (`time_grammar.py`). It returns a strict JSON intent schema that downstream planners consume.
- **Spelling Mistake Handling**: 
  - **90% Company Name Correction**: Automatically corrects misspellings (e.g., "Appel" â†’ "Apple", "Microsft" â†’ "Microsoft", "Bookng Holdings" â†’ "Booking Holdings") using fuzzy matching with progressive cutoffs (0.85, 0.80, 0.75, 0.70, 0.65) and manual overrides.
  - **100% Metric Correction**: Handles metric typos (e.g., "revenu" â†’ "revenue", "earnngs" â†’ "earnings", "operatng" â†’ "operating") using multi-level fuzzy matching with adaptive thresholds.
- **Intent Recognition**: Recognizes **40+ intent types** including compare, trend, rank, explain, forecast, scenario, relationship, benchmark, when, why, what-if, recommendation, risk, valuation, and more.
- **Query Pattern Detection**: Supports **150+ question patterns** covering what, how, why, when, where, who, which, contractions, commands, and natural language variations with **100% detection rate**.
- **Portfolio Detection**: The parser automatically detects portfolio-related queries and extracts portfolio identifiers (e.g., `port_abc123`) from user queries.
- **ML Forecast Detection**: The parser detects forecast-related keywords (`forecast`, `predict`, `estimate`, `projection`, etc.) and routes queries to the ML forecasting system.

### ðŸ” Retrieval Layer (RAG) - Production-Grade Implementation

FinalyzeOS implements a **production-grade RAG system** that goes far beyond vanilla RAG with advanced retrieval, reranking, and safety features.

#### **Core Retrieval Architecture**

- ðŸ“Š **SQL Deterministic Retrieval**: Structured intents route directly into AnalyticsEngine, reading metric snapshots, KPI overrides, and fact tables from SQLite/Postgres. **Spelling mistakes in company names and metrics are automatically corrected** before retrieval (90% company name success, 100% metric success).
- ðŸ” **Semantic Search**: Vector embeddings for SEC filing narratives and uploaded documents using ChromaDB with `all-MiniLM-L6-v2` embeddings (384 dimensions)
- ðŸ” Retrieved artefacts (tables, benchmark comparisons, audit trails) become RAG "system" messages that condition the LLM, ensuring no fabricated values slip through
- **Natural Language Processing**: The RAG layer leverages advanced NLP capabilities including **150+ question patterns**, **40+ intent types**, and **spelling mistake handling** to accurately interpret user queries before retrieval.
- **S&P 1500 Coverage**: Retrieves data for all **1,599 S&P 1500 companies** (S&P 500 + S&P 400 + S&P 600) with automatic company name and ticker resolution, including common misspellings.
- **Portfolio Context**: When portfolio queries are detected, the system retrieves portfolio holdings, exposure data, risk metrics, and attribution results from the portfolio database
- **ML Forecast Context**: When forecast queries are detected (via intent detection), the system retrieves historical time series data, runs ML forecasting models (**8 models available**), and builds comprehensive technical context including model architecture, hyperparameters, training details, and forecast results
- **Multi-Source Aggregation**: The RAG layer aggregates data from multiple sources (SEC EDGAR, Yahoo Finance, FRED, IMF) to provide comprehensive context for financial queries

#### **Advanced RAG Features** â­ Production-Grade

**1. Cross-Encoder Reranking** (â­ MOST IMPORTANT)
- **Second-pass relevance scoring** using `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **~10-20% relevance improvement** over bi-encoder similarity alone
- **Far fewer hallucinations** through better document ranking
- Shows understanding of retrieval quality bottlenecks and Transformer cross-attention (Lecture 2 concept)
- **Performance**: Adds ~50-100ms per query (cross-encoder inference)

**2. Source Fusion (Score Normalization & Confidence Fusion)**
- **Normalizes similarity scores** across sources (SEC narratives, uploaded docs, SQL metrics)
- **Applies reliability weights**: SQL (1.0), SEC (0.9), Uploaded (0.7), Macro (0.6), Forecasts (0.5)
- **Computes overall retrieval confidence** (0-1) for answer quality assessment
- **Merges sources** into single ranked list with fused scores
- Shows research-level retrieval engineering

**3. Grounded Decision Layer**
- **Safety checks before answering**: Detects low confidence, source contradictions, missing information
- **Prevents hallucinations**: Returns "I don't have enough information" when confidence < 0.25
- **Source contradiction detection**: Flags when SQL contradicts narrative sources
- **Missing information detection**: Warns when tickers parsed but no data found
- Aligns with Lecture 2's emphasis on grounded, observable systems

**4. Retrieval Confidence Scoring**
- **Computes weighted average** of top-K similarity scores
- **Adjusts LLM tone** based on confidence level (high/medium/low)
- **High confidence (â‰¥0.7)**: "Provide a confident, detailed answer"
- **Medium confidence (0.4-0.7)**: "Provide a helpful answer but acknowledge uncertainties"
- **Low confidence (<0.4)**: "Be cautious and explicit about information gaps"
- Aligns answer tone with retrieval uncertainty - exactly what financial institutions need

**5. Memory-Augmented RAG**
- **Per-conversation document tracking**: Isolates documents by conversation_id
- **Per-user document tracking**: Tracks documents across all user conversations
- **Document lifetime tracking**: Marks stale documents (default 90 days)
- **Topic clustering**: Clusters documents by topic (financial_metrics, risk_analysis, forecasting, governance, operations)
- **Automatic registration**: Documents automatically registered in memory on upload
- Unique feature that treats uploaded docs as ephemeral memory

**6. Multi-Hop Retrieval (Agentic Behavior)**
- **Query decomposition**: Breaks complex questions into sub-queries
- **Sequential retrieval**: Performs multiple retrieval steps (metrics â†’ narratives â†’ macro â†’ portfolio)
- **Complexity detection**: Automatically detects simple/moderate/complex queries
- **Example**: "Why did Apple's revenue decline, and how does this compare to the tech sector?"
  - Step 1: Retrieve Apple's revenue metrics
  - Step 2: Retrieve SEC narratives explaining decline
  - Step 3: Retrieve macro/economic context
  - Step 4: Retrieve sector benchmarks
- Shows agentic behavior beyond simple RAG

**7. Evaluation Harness**
- **Retrieval metrics**: Recall@K, MRR (Mean Reciprocal Rank), nDCG (Normalized Discounted Cumulative Gain)
- **QA metrics**: Exact match, factual consistency, source citation accuracy
- **Evaluation script**: `scripts/evaluate_rag.py` for quantitative assessment
- **Test dataset**: JSON format with queries, expected documents, ground truth answers
- Research-grade evaluation system

**8. Observability & Guardrails**
- **Comprehensive logging**: Retrieval counts, scores, timing, document IDs, anomalies
- **Context window control**: Smart truncation (drops low-scoring documents first)
- **Anomaly detection**: Warns if all scores below threshold, warns if empty retrieval
- **Guardrails**: Min relevance score (0.3), max context chars (15000), max documents (10)
- **Audit trail**: Full traceability of which documents were retrieved and why

**9. Complete RAG Orchestrator**
- **Single entry point**: `RAGOrchestrator` orchestrates all features in one pipeline
- **Automatic feature selection**: Enables/disables features based on query complexity
- **Unified interface**: `process_query()` returns prompt, result, and metadata
- **Production-ready**: All features integrated and tested

#### **RAG Components**

- **`RAGRetriever`**: Unified retrieval interface combining SQL + vector search + uploaded docs
- **`Reranker`**: Cross-encoder reranking for better relevance
- **`SourceFusion`**: Score normalization and confidence fusion
- **`GroundedDecisionLayer`**: Safety checks before answering
- **`MemoryAugmentedRAG`**: Per-conversation/user document tracking
- **`RAGController`**: Multi-hop query decomposition
- **`RAGObserver`**: Observability and guardrails
- **`RAGOrchestrator`**: Complete pipeline orchestrator

#### **Documentation**

- **Complete Guide**: See `docs/RAG_COMPLETE_GUIDE.md` for comprehensive RAG documentation
- **Architecture**: Detailed explanation of all components and features
- **Usage Examples**: Code examples for all advanced features
- **Testing**: Test scripts for verifying functionality

### ðŸŽ¯ Generation / Machine Learning

- ðŸ¤– `llm_client.py` abstracts provider selection (local echo vs. OpenAI). The model verbalises retrieved metrics, summarises trends, and surfaces parser warnings
- ðŸ“ˆ Scenario and benchmarking flows blend deterministic calculations (growth rates, spreads) with LLM narration, preserving numeric accuracy while keeping explanations natural
- **Enhanced ML Forecast Responses**: The system prompt includes explicit instructions for ML forecasts, mandating inclusion of ALL technical details from the "EXPLICIT DATA DUMP" section. The `ml_response_verifier.py` module post-processes responses to ensure all required technical details are present.
- **Portfolio Response Enhancement**: The system prompt includes explicit instructions for portfolio analysis, mandating use of actual portfolio data (tickers, weights, metrics) and prohibiting hallucination of portfolio details.
- **Response Verification**: The `ml_response_verifier.py` module verifies ML forecast responses include all required technical details (model architecture, hyperparameters, training details, computational details) and automatically enhances responses if details are missing.

### ðŸ”§ RAG Enhancements for ML Forecasting

The RAG layer has been significantly enhanced to support detailed ML forecasting:

#### **1. Explicit Data Dump Section**
- **Purpose**: Ensures LLM receives ALL technical details in structured format
- **Content**: Model architecture (layers, units, activation functions), hyperparameters (learning rate, batch size, epochs), training details (loss values, validation metrics), computational details (training time, memory usage), model-specific parameters (ARIMA orders, Prophet seasonality, LSTM/GRU/Transformer architecture)
- **Format**: Key-value pairs for easy extraction and inclusion in responses
- **Mandate**: LLM is explicitly instructed to include EVERY value from this section without summarization

#### **2. Enhanced Context Building**
The `context_builder.py` module builds comprehensive ML forecast context including:
- **Forecast Values**: All predicted values with 95% confidence intervals (upper and lower bounds)
- **Model Details**: Complete technical specifications (layers, units, epochs, loss, learning rate, batch size, optimizer, activation functions, dropout rates, etc.)
- **Training Process**: Training loss, validation loss, early stopping criteria, learning rate schedule, convergence metrics
- **Data Preprocessing**: Scaling methods (standardization, normalization), outlier handling, missing data treatment, feature engineering
- **Model Selection**: Why this model was chosen, alternative models considered, model comparison metrics
- **Performance Metrics**: Training metrics (loss, accuracy), validation metrics, forecast accuracy (MAE, RMSE, MAPE), cross-validation scores
- **Forecast Analysis**: Year-over-year growth rates, multi-year CAGR, confidence interval uncertainty analysis
- **Sector Comparison**: How forecast compares to sector averages and peers, percentile rankings
- **Scenario Analysis**: Bull/base/bear scenarios based on confidence intervals, upside potential, downside risk
- **Risk Analysis**: Model confidence scores, downside risks, upside opportunities, data quality warnings

#### **3. Response Verification & Enhancement**
The `ml_response_verifier.py` module ensures responses include all required technical details:
- **Strict Mode**: When "EXPLICIT DATA DUMP" is present, verifies exact value matches (not just keyword mentions)
- **Required Checks**: Verifies presence of model architecture, hyperparameters, training details, computational details, model-specific parameters
- **Enhancement**: Automatically appends missing technical details if LLM response is incomplete, grouped by category (architecture, training, hyperparameters, computational, other)
- **Categorization**: Groups missing details by category for better readability when appending to response

#### **4. System Prompt Enhancements**
The chatbot system prompt includes explicit instructions for ML forecasts:
- **Mandatory Inclusion**: Instructions to include EVERY value from "EXPLICIT DATA DUMP" without summarization
- **No Summarization**: Explicit prohibition against summarizing technical details (e.g., "training loss is low" is prohibited - must say "training loss is 0.001234")
- **Exact Values**: Instructions to use exact numerical values from the context (e.g., "training loss is {X.XXXXXX}" not "training loss is low")
- **Technical Depth**: Minimum 500-1000 words for forecast responses, suitable for institutional analysts
- **Professional Formatting**: Markdown formatting guidelines for professional presentation (headers, bold text, lists, tables, blockquotes)

### ðŸ”§ RAG Enhancements for Portfolio Management

The RAG layer has been enhanced to support comprehensive portfolio analysis:

#### **1. Portfolio Context Building**
The `context_builder.py` module builds comprehensive portfolio context including:
- **Holdings Data**: All tickers, weights, shares, prices, market values, sectors, fundamental metrics
- **Exposure Analysis**: Sector exposure, factor exposure (beta, momentum, value, size, quality), concentration metrics (HHI, top 10 concentration)
- **Portfolio Statistics**: Weighted average P/E, dividend yield, ROE, ROIC, concentration ratios, diversification metrics
- **Risk Metrics**: Pre-calculated CVaR, VaR, volatility, Sharpe ratio, Sortino ratio, tracking error, beta
- **Performance Attribution**: Brinson-Fachler attribution with allocation, selection, and interaction effects
- **Scenario Results**: Stress test results, scenario analysis outcomes, Monte Carlo simulation results

#### **2. Portfolio Response Instructions**
The system prompt includes explicit instructions for portfolio analysis:
- **Use Actual Data**: Mandates use of actual portfolio data (tickers, weights, metrics) from the portfolio context
- **No Hallucination**: Explicit prohibition against making up portfolio data - must use data from context
- **Quote Exact Numbers**: Instructions to reference exact tickers, weights, and metrics (e.g., "AAPL is 15.2% of the portfolio")
- **Specific Recommendations**: Instructions to provide specific rebalancing actions based on actual portfolio composition
- **Risk Metric Usage**: Instructions to use pre-calculated risk metrics from portfolio context, not estimate them

#### **3. Multi-Source Portfolio Context**
The RAG layer aggregates portfolio data from multiple sources:
- **SEC Filings**: 10-K, 10-Q filings for top holdings
- **Yahoo Finance**: Real-time prices, analyst ratings, market data
- **Portfolio Database**: Holdings, weights, historical performance, risk metrics
- **Sector Analytics**: Sector benchmarks, peer comparisons, percentile rankings

### ðŸ› ï¸ Tooling & Coverage

Regenerate the alias universe with:
```bash
export PYTHONPATH=./src
python scripts/generate_aliases.py
```

The script reads data/tickers/universe_sp500.txt, applies the same normalisation rules as runtime, and rewrites aliases.json with coverage stats.

Guardrails live in tests/test_alias_resolution.py, tests/test_time_grammar.py, and tests/test_nl_parser.py, ensuring alias coverage, period parsing, and structured intents stay within spec.

### ðŸ“¦ Dependencies

#### Prerequisites
- **Python 3.10+** (Python 3.11 or 3.12 recommended)
- **pip** (Python package manager, usually comes with Python)
- **Git** (to clone the repository)

#### Package Requirements

All required packages are specified in **[`requirements.txt`](requirements.txt)** with version constraints.

**Key Package Categories:**
- **Core Framework**: FastAPI, Uvicorn, Python-dotenv
- **AI/ML**: OpenAI, Transformers, PyTorch, Sentence-transformers
- **Database**: SQLAlchemy, PostgreSQL adapters
- **Vector Database**: ChromaDB, Sentence-transformers (for RAG)
- **Financial Data**: yfinance, FRED API, pandas-datareader
- **Web Scraping**: requests, beautifulsoup4 (for document fetchers)
- **Data Processing**: Pandas, NumPy, OpenPyXL
- **Visualization**: Plotly, Dash, Matplotlib, Seaborn
- **ML Forecasting**: Prophet, ARIMA, TensorFlow, Scikit-learn
- **Document Generation**: FPDF2, python-pptx
- **Testing & Development**: Pytest, Black, Flake8, MyPy

> **ðŸ“‹ Complete List**: See [`requirements.txt`](requirements.txt) for all 70+ packages with exact version specifications.
> 
> **ðŸ“– Installation**: Follow the [Complete Setup Guide](#ï¸-complete-setup-guide) above for step-by-step installation instructions.

### ðŸ“Š PowerPoint Export & Analyst Documentation

The PowerPoint export generates a comprehensive **12-slide CFI-style presentation** suitable for client presentations, investment committee meetings, and academic deliverables. Each deck is automatically generated from live dashboard data with **zero manual formatting required**.

**Slide Structure (12 pages):**
1. **Cover Page** â€“ Company name, ticker, date, Team 2 branding with diagonal accent
2. **Executive Summary** â€“ 3-5 data-driven analyst bullets + 8-KPI panel (Revenue, EBITDA, FCF, EPS, EV/EBITDA, P/E, Net Debt, ROIC)
3. **Revenue & EBITDA Growth** â€“ Column chart for revenue + commentary with YoY growth and CAGR calculations
4. **Valuation Multiples vs Time** â€“ Line chart for EV/EBITDA and P/E trends vs 5-year average
5. **Share Price Performance** â€“ Price chart with 50/200-DMA and 52-week high/low analysis
6. **Cash Flow & Leverage** â€“ Free cash flow chart + leverage metrics table (Net Debt/EBITDA, Coverage)
7. **Forecast vs Actuals** â€“ Earnings surprise analysis (EPS & Revenue vs consensus estimates)
8. **Segment / Geographic Mix** â€“ Business unit breakdown with revenue contribution analysis
9. **DCF & Scenario Analysis** â€“ Bull/Base/Bear valuation scenarios with WACC and terminal growth assumptions
10. **Peer Comparison** â€“ Scatter plot of EV/EBITDA vs EBITDA Margin with focal company highlighted
11. **Risk Considerations** â€“ 3-5 automated risk bullets derived from leverage, margin trends, and valuation signals
12. **Data Sources & Appendix** â€“ Clickable hyperlinks to SEC EDGAR, Yahoo Finance, and internal database

**Visual Standards (CFI Style):**
- **Color Palette:** Deep navy `#0B214A`, mid blue `#1E5AA8`, slate grey for gridlines and text
- **Typography:** Titles 20-24pt semibold, body 11-14pt, small-caps labels, clean margins
- **Layout:** Navy title bar with company + date; footer with page numbers and "Prepared by Team 2"
- **Charts:** Thin gridlines, transparent backgrounds, compact numeric labels ($2.1B / 13.4%)

**Analytics Auto-Generated:**
- **Growth Metrics:** YoY and CAGR (3y/5y) for Revenue, EBITDA, FCF with momentum tagging
- **Profitability:** EBITDA margin trend with Â±150 bps change flags
- **Valuation:** EV/EBITDA and P/E vs 5-year average with rich/cheap/in-line interpretation
- **Cash Quality:** FCF trend analysis with leverage ratio warnings (Net Debt/EBITDA > 3.5x)
- **Risk Signals:** Automated bullets for margin compression, negative FCF, elevated leverage

**Data Sources (Embedded as Hyperlinks):**
- [SEC EDGAR Company Filings](https://www.sec.gov/edgar/searchedgar/companysearch.html)
- [SEC Financial Statement & Notes Datasets](https://www.sec.gov/dera/data/financial-statement-and-notes-data-sets.html)
- [Yahoo Finance Market Data](https://finance.yahoo.com)
- [FinalyzeOS GitHub Repository](https://github.com/haniae/Team2-CBA-Project)

**Usage Examples:**

*Via API (Direct Download):*
```bash
# Generate PowerPoint for Apple
curl -o AAPL_deck.pptx "http://localhost:8000/api/export/cfi?format=pptx&ticker=AAPL"

# Generate PDF report for Microsoft
curl -o MSFT_report.pdf "http://localhost:8000/api/export/cfi?format=pdf&ticker=MSFT"

# Generate Excel workbook for Tesla
curl -o TSLA_data.xlsx "http://localhost:8000/api/export/cfi?format=xlsx&ticker=TSLA"
```

*Via Dashboard (UI):*
1. Navigate to `http://localhost:8000`
2. Ask: "Show me [Company Name]'s financial performance"
3. Scroll to bottom of dashboard
4. Click **"Export PowerPoint"** button
5. File downloads automatically: `finanlyzeos-{ticker}-{date}.pptx`

*Programmatic (Python SDK):*
```python
from finanlyzeos_chatbot import AnalyticsEngine, load_settings
from finanlyzeos_chatbot.export_pipeline import generate_dashboard_export

# Initialize engine
settings = load_settings()
engine = AnalyticsEngine(settings)

# Generate PowerPoint
result = generate_dashboard_export(engine, "AAPL", "pptx")

# Save to file
with open("AAPL_analysis.pptx", "wb") as f:
    f.write(result.content)
```

**Quality Assurance Checklist:**
- [ ] Company name and ticker are correct on cover slide
- [ ] As-of date reflects latest data refresh
- [ ] Charts render correctly (no placeholders) for Revenue, EBITDA, Valuation
- [ ] KPI values are reasonable (no `NaN`, `Infinity`, negative multiples)
- [ ] Commentary bullets are grammatically correct and data-driven
- [ ] Footer page numbers are sequential (Page 1 of 12, 2 of 12, ...)
- [ ] Color palette matches CFI standard (Navy #0B214A, Blue #1E5AA8)
- [ ] File size < 10 MB for email distribution

**Target Audience:**
- **Financial Analysts** â€“ Equity research, investment banking, corporate finance
- **Investment Committees** â€“ Board presentations, portfolio reviews
- **Academic Use** â€“ MBA case studies, finance courses, professor deliverables
- **Client Presentations** â€“ Pitch decks, quarterly business reviews

---

## ðŸ’¬ Running FinalyzeOS

### ðŸ–¥ï¸ CLI REPL

```bash
python run_chatbot.py
```

Inside the prompt, type help to see available commands. Common examples:

| Command | Example | What it does |
|---------|---------|--------------|
| metrics | metrics AAPL 2022-2024 | Latest and historical KPI block for one ticker. |
| compare | compare MSFT NVDA 2023 | Side-by-side metrics table. |
| table | table TSLA metrics revenue net_income | Renders a low-level ASCII table (useful in tests). |
| fact | fact AMZN 2023 revenue | Inspect a normalised financial fact. |
| scenario | scenario GOOGL bull rev=+8% mult=+0.5 | Run a what-if scenario with metric deltas. |
| ingest | ingest SHOP 5 | Trigger live ingestion (SEC, Yahoo, optional Bloomberg). |

Comparison responses append an "S&P 500 Avg" column highlighting how each ticker stacks up on margins, ROE, and valuation multiples.

### ðŸŒ FastAPI + SPA

```bash
python serve_chatbot.py --port 8000
# or run the ASGI app directly
uvicorn finanlyzeos_chatbot.web:app --reload --port 8000
```

Navigate to `http://localhost:8000`. The SPA exposes:

- â±ï¸ **Real-time Request Timeline** - Intent, cache, context, compose with slow-step hints
- ðŸ“¤ **Export Shortcuts** - CSV, PDF and in-line benchmarks
- âš™ï¸ **Settings Panel** - Toggle data sources, timeline detail, and export defaults

### ðŸ”Œ REST Endpoints

| Method | Route | Purpose |
|--------|-------|---------|
| POST | /chat | Submit a prompt. Returns reply, conversation_id, structured artefacts, and progress events. |
| GET | /metrics | Retrieve numeric metrics for one or more tickers (start_year / end_year filters supported). |
| GET | /facts | Fetch normalised financial facts backing the numbers. |
| GET | /audit | View the latest ingestion/audit events for a ticker. |
| GET | /health | Basic readiness/liveness check for load balancers. |

The /chat response includes structured extras (highlights, trends, comparison_table, citations, exports, conclusion) so downstream integrations can reuse the analytics without re-parsing text.

## ðŸ“¥ Data Ingestion Guide

> **ðŸ“– For first-time setup with options for 100, 250, 500, or 1500 companies, see the [Complete Setup Guide - Step 2](#-step-2-choose-your-data-ingestion-option) above.**

FinalyzeOS provides **multiple ingestion strategies** to fit different use cases. This section explains advanced ingestion techniques and gap-filling strategies for existing databases.

### â­ Recommended: Smart Gap Filling Script

The `fill_data_gaps.py` script is the **easiest and most powerful** way to ingest data. It automatically:
- Detects which companies are missing data for specified years
- Fetches data from SEC EDGAR with intelligent rate limiting
- Handles retries and failures gracefully
- Provides real-time progress tracking
- Generates comprehensive completion reports

#### Basic Usage Examples

```bash
# 1. Quick Start: Get last 3 years of data (recommended for first-time users)
python scripts/ingestion/fill_data_gaps.py --target-years "2022,2023,2024" --years-back 3 --batch-size 10
# Time: ~5-7 minutes | Records: ~5,000-8,000 | Companies: 475

# 2. Recent History: Last 5 years for analysis
python scripts/ingestion/fill_data_gaps.py --target-years "2020,2021,2022,2023,2024" --years-back 5 --batch-size 10
# Time: ~8-12 minutes | Records: ~12,000-15,000 | Companies: 475

# 3. Full Historical Data: 20 years for long-term trends
python scripts/ingestion/fill_data_gaps.py \
  --target-years "2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025" \
  --years-back 20 \
  --batch-size 10
# Time: ~25-35 minutes | Records: ~50,000-80,000 | Companies: 475

# 4. Fill Specific Gap Years Only
python scripts/ingestion/fill_data_gaps.py --target-years "2019,2020" --years-back 7 --batch-size 10
# Time: ~3-5 minutes | Fills only missing 2019 and 2020 data
```

#### Command-Line Options

| Option | Default | Description |
|--------|---------|-------------|
| `--target-years` | "2019,2020,2025" | Comma-separated years to check and fill |
| `--years-back` | 7 | How many years of data to fetch from SEC API |
| `--batch-size` | 10 | Number of companies to process per batch |
| `--max-tickers` | None | Limit ingestion to first N companies (for testing) |
| `--dry-run` | False | Show what would be ingested without actually doing it |

#### Windows PowerShell Shortcut

```powershell
# One-command 20-year ingestion
.\run_data_ingestion.ps1

# This runs the full historical ingestion automatically
```

#### Understanding the Output

While running, you'll see:
```
================================================================================
FILLING DATA GAPS - MAKING ALL YEARS SOLID
================================================================================
Target years: 2022, 2023, 2024
Years to fetch from SEC: 3
Batch size: 10
Started at: 2025-10-23 15:30:00 UTC

ðŸ“Š Loading existing tickers from database...
   Found 475 tickers in database
ðŸ” Checking coverage for years: 2022, 2023, 2024...
   156 companies missing data for these years
ðŸš€ Starting ingestion of 475 tickers...
   This will take approximately 1.2 minutes

[1/48 - 2.1%] Processing: AAPL, ABBV, ABNB, ABT, ACGL, ACN, ADBE, ADI, ADM, ADP
   âœ… Loaded 335 records (Total: 335)
[2/48 - 4.2%] Processing: ADSK, AEP, AES, AFL, AIG, AIZ, AJG, AKAM, ALB, ALGN
   âœ… Loaded 318 records (Total: 653)
...
ðŸ“Š Progress Report:
   Batches processed: 10/48
   Total records loaded: 3,249
   Successes: 100
   Failures: 0
...
ðŸ”„ Refreshing derived metrics...
   âœ… Metrics refreshed
ðŸ“„ Summary saved to: fill_gaps_summary.json
================================================================================
INGESTION COMPLETE
================================================================================
âœ… Successfully ingested: 475 companies
ðŸ“Š Total records loaded: 15,889
ðŸŽ‰ All companies ingested successfully!
```

#### After Ingestion Completes

Check your data:
```bash
# View row counts per table
python -c "import sqlite3; conn = sqlite3.connect('C:/Users/YOUR_PATH/finanlyzeos_chatbot.sqlite3'); cursor = conn.cursor(); tables = ['financial_facts', 'company_filings', 'metric_snapshots', 'kpi_values']; [print(f'{t}: {cursor.execute(f\"SELECT COUNT(*) FROM {t}\").fetchone()[0]:,}') for t in tables]; conn.close()"

# Check year coverage
python -c "import sqlite3; conn = sqlite3.connect('C:/Users/YOUR_PATH/finanlyzeos_chatbot.sqlite3'); cursor = conn.cursor(); cursor.execute('SELECT MIN(fiscal_year), MAX(fiscal_year), COUNT(DISTINCT ticker) FROM financial_facts'); print('Years: %s-%s | Companies: %s' % cursor.fetchone()); conn.close()"
```

### Alternative: Legacy Batch Scripts

These scripts are available for specific use cases but `fill_data_gaps.py` is generally easier:

| Script | When to use it | Example |
|--------|---------------|---------|
| `scripts/ingestion/ingest_universe.py` | Refresh a watch list with resume support and polite rate limiting. | `python scripts/ingestion/ingest_universe.py --universe sp500 --years 10 --chunk-size 25 --sleep 2 --resume` |
| `scripts/ingestion/batch_ingest.py` | Pull the built-in mega-cap list through ingest_live_tickers with retry/backoff. | `python scripts/ingestion/batch_ingest.py` |
| `scripts/ingestion/load_prices_yfinance.py` | Refresh market quotes from Yahoo Finance. | `python scripts/ingestion/load_prices_yfinance.py` |

### On-Demand Ingestion

AnalyticsEngine.get_metrics calls ingest_live_tickers when it detects missing coverage. You can route this through tasks.TaskManager to queue and monitor ingestion jobsâ€”see inline docstrings for patterns.

All scripts honour the configuration from load_settings() and write audit events so the chatbot can justify sourcing decisions.

### Price-refresh workflow

Use this to keep price-driven ratios current without re-ingesting everything:

```bash
pip install yfinance  # one-time
$env:SEC_TICKERS = (Get-Content data/tickers/universe_sp500.txt) -join ','
# Optional Postgres target
$env:PGHOST='127.0.0.1'; $env:PGPORT='5432'
$env:PGDATABASE='secdb'; $env:PGUSER='postgres'; $env:PGPASSWORD='your_password_here'
python scripts/ingestion/load_prices_yfinance.py

$env:PYTHONPATH = (Resolve-Path .\src).Path
python - <<'PY'
from finanlyzeos_chatbot.config import load_settings
from finanlyzeos_chatbot.analytics_engine import AnalyticsEngine
AnalyticsEngine(load_settings()).refresh_metrics(force=True)
PY
```

Restart serve_chatbot.py afterwards so the SPA sees the refreshed metrics.

> **ðŸ“– For complete data ingestion instructions with options for 100, 250, 500, or 1500 companies, see the [Complete Setup Guide - Step 2](#-step-2-choose-your-data-ingestion-option) above.**

## ðŸ“Š Advanced Ingestion Techniques

> **ðŸ“– For basic setup and ingestion options (100, 250, 500, or 1500 companies), see the [Complete Setup Guide](#ï¸-complete-setup-guide) above.**

This section covers advanced ingestion techniques for users who have already completed the basic setup.

### Advanced Ingestion Options

**Module vs Script Path Forms:**
```bash
# Module form (requires pip install -e .)
python -m scripts.ingestion.ingest_universe --universe sp500 --years 10 --chunk-size 25 --sleep 2

# Script path form (works without installation)
python scripts/ingestion/ingest_universe.py --universe sp500 --years 10 --chunk-size 25 --sleep 2
```

**Key Options:**
- `--years 10` - Pulls the most recent 10 fiscal years
- `--chunk-size 25` - Processes 25 companies per batch
- `--sleep 2` - Delay between batches (respects SEC rate limits)
- `--resume` - Resume from last checkpoint (uses `.ingestion_progress.json`)

**Verify Ingestion:**
```bash
python scripts/utility/check_ingestion_status.py
# Or use the simple checker
python scripts/utility/check_correct_database.py
```

**Common Issues:**
- "Nothing to do" with `--resume`: Delete `.ingestion_progress.json` and re-run
- Yahoo 429 errors: Reduce batch size and add delays between requests
- DB path: Override with `DATABASE_PATH` environment variable
- ModuleNotFoundError: Ensure you ran `pip install -e .` or set `PYTHONPATH=./src`

## âš™ï¸ Configuration Reference

`load_settings()` reads environment variables (or `.env`) and provides sensible defaults.

| Variable | Default | Notes |
|----------|---------|-------|
| DATABASE_TYPE | sqlite | Switch to postgresql for shared deployments. |
| DATABASE_PATH | ./data/sqlite/finanlyzeos_chatbot.sqlite3 | SQLite file location; created automatically. |
| POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE, POSTGRES_USER, POSTGRES_PASSWORD | unset | Required when DATABASE_TYPE=postgresql; POSTGRES_SCHEMA overrides the default sec. |
| LLM_PROVIDER | local | local uses the deterministic echo model; set to openai for real completions. |
| OPENAI_MODEL | gpt-4o-mini | Passed verbatim to the OpenAI Chat Completions API. |
| SEC_API_USER_AGENT | FinalyzeOSBot/1.0 (support@finanlyzeos.com) | Mandatory for SEC EDGAR requests. Customize it for your org. |
| EDGAR_BASE_URL | https://data.sec.gov | Override if you proxy or mirror EDGAR. |
| YAHOO_QUOTE_URL | https://query1.finance.yahoo.com/v7/finance/quote | Used to refresh quotes. |
| YAHOO_QUOTE_BATCH_SIZE | 50 | Maximum tickers per Yahoo batch. |
| HTTP_REQUEST_TIMEOUT | 30 | Seconds before HTTP clients give up. |
| INGESTION_MAX_WORKERS | 8 | Thread pool size for ingestion routines. |
| DATA_CACHE_DIR | ./cache | Stores downloaded filings, facts, and progress markers. |
| ENABLE_BLOOMBERG | false | Toggle Bloomberg ingestion; requires host/port/timeout values. |
| BLOOMBERG_HOST, BLOOMBERG_PORT, BLOOMBERG_TIMEOUT | unset | Only used if Bloomberg is enabled. |
| OPENAI_API_KEY | unset | Looked up in env, then keyring, then ~/.config/finanlyzeos-chatbot/openai_api_key. |

Secrets belong in your local .env. Windows developers can rely on keyring so API keys live outside the repo.

## ðŸ—„ï¸ Database Schema

FinalyzeOS intentionally supports **two storage backends**, but your deployment uses only one at a timeâ€”by default it's SQLite:

- **SQLite (default / implied in this repo)** â€“ shipping the database as a file keeps setup frictionless for development, tests, and CI. All conversations, metrics, and audit events live in the path defined by DATABASE_PATH. For this reason, the stock .env (and most tests such as test_ingestion_perf.py) run purely on SQLite. It was chosen because it "just works": no external server to provision, a trivial backup story, and fast enough for single-user workflows. PRAGMAs (WAL, synchronous=NORMAL, temp_store=MEMORY, cache_size=-16000) are applied automatically so sustained writes remain smooth.
- **PostgreSQL (optional)** â€“ the same helper module can target Postgres when you set DATABASE_TYPE=postgresql and supply the POSTGRES_* DSN variables. Teams switch to Postgres when chat sessions are shared across analysts, when concurrency or replication matters, or when governance requires managed backups. If you haven't changed those settings, Postgres is unused.

In other words, you are currently using a single databaseâ€”SQLiteâ€”because it was selected for simplicity and portability. The PostgreSQL path is documented for teams that choose to run FinalyzeOS in a multi-user/shared environment later.

Regardless of backend, both share the same schema:

### Key tables:

| Table | Purpose | Notable columns |
|-------|---------|-----------------|
| conversations | Stores chat turns for resumable threads. | conversation_id, role, content, created_at |
| cached_prompts | Deduplicates prompts so identical requests reuse cached replies. | prompt_hash, payload, created_at, reply |
| metric_snapshots | Persisted analytics snapshot consumed by the chatbot/UI. | ticker, metric, period, value, start_year, end_year, updated_at, source |
| company_filings | Metadata for SEC filings pulled during ingestion. | ticker, accession_number, form_type, filed_at, data |
| financial_facts | Normalised SEC fact rows (revenues, margins, etc.). | ticker, metric, fiscal_year, period, value, unit, source_filing, raw |
| market_quotes | Latest quotes from Yahoo/Bloomberg/Stooq loaders. | ticker, price, currency, timestamp, source |
| kpi_values | KPI backfill overrides that smooth derived metrics. | ticker, fiscal_year, fiscal_quarter, metric_id, value, method, warning |
| audit_events | Traceability for ingestion and scenario runs. | ticker, event_type, entity_id, details, created_at |
| ticker_aliases | Maps tickers to CIK/company names to speed ingestion. | ticker, cik, company_name, updated_at |

On startup database.initialise() applies schema migrations idempotently. When running in SQLite mode the PRAGMAs mentioned above are applied automatically; switching to Postgres only requires setting the DSN variablesâ€”the rest of the code paths remain identical.

## ðŸ“ Project Layout

```
Project/
â”œâ”€â”€ README.md                          # Main project documentation
â”œâ”€â”€ CHANGELOG.md                       # Project changelog
â”œâ”€â”€ LICENSE                            # Project license (MIT)
â”œâ”€â”€ pyproject.toml                     # Project metadata, dependencies, pytest config
â”œâ”€â”€ requirements.txt                   # Python dependencies lockfile
â”œâ”€â”€ finanlyzeos_chatbot.sqlite3        # Main SQLite database (created on demand)
â”œâ”€â”€ finalyzeos_chatbot.sqlite3         # Main SQLite database (backup)
â”œâ”€â”€ test.db                            # Test database
â”‚
â”œâ”€â”€ app/                               # Application entry points
â”‚   â”œâ”€â”€ run_chatbot.py                 # CLI chatbot entry point (REPL)
â”‚   â”œâ”€â”€ run_server.py                  # Web server entry point (FastAPI)
â”‚   â”œâ”€â”€ serve_chatbot.py               # Alternative web server entry point
â”‚   â””â”€â”€ start_server.sh                # Server startup script (Unix/Linux)
â”‚
â”œâ”€â”€ scripts/                           # Scripts and utilities
â”‚   â”œâ”€â”€ check_packages.py              # Package verification utility
â”‚   â”œâ”€â”€ evaluate_rag.py                # RAG evaluation script
â”‚   â”œâ”€â”€ index_documents_for_rag.py     # Document indexing for RAG
â”‚   â”œâ”€â”€ quick_rag_test.py              # Quick RAG testing
â”‚   â”œâ”€â”€ test_complete_rag.py           # Complete RAG testing
â”‚   â”œâ”€â”€ test_rag_advanced.py           # Advanced RAG testing
â”‚   â”œâ”€â”€ test_rag_integration.py        # RAG integration testing
â”‚   â”œâ”€â”€ test_rag_working.py            # RAG working tests
â”‚   â”œâ”€â”€ run_data_ingestion.ps1         # Windows PowerShell ingestion script
â”‚   â”œâ”€â”€ run_data_ingestion.sh          # Unix/Linux ingestion script
â”‚   â”‚
â”‚   â”œâ”€â”€ fetchers/                      # Document fetcher scripts (NEW!)
â”‚   â”‚   â”œâ”€â”€ fetch_earnings_transcripts.py  # Earnings call transcripts
â”‚   â”‚   â”œâ”€â”€ fetch_financial_news.py        # Financial news articles
â”‚   â”‚   â”œâ”€â”€ fetch_analyst_reports.py       # Analyst research reports
â”‚   â”‚   â”œâ”€â”€ fetch_press_releases.py         # Company press releases
â”‚   â”‚   â””â”€â”€ fetch_industry_research.py      # Industry research reports
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                          # Shared utilities (NEW!)
â”‚   â”‚   â””â”€â”€ chunking.py                 # Document chunking utility
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                      # Analysis scripts
â”‚   â”‚   â””â”€â”€ analyze_coverage_gaps.py   # Analyze coverage gaps (complete/partial/missing)
â”‚   â”‚
â”‚   â”œâ”€â”€ demos/                         # Demo scripts
â”‚   â”‚   â””â”€â”€ (demo scripts for presentations)
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                     # Data ingestion scripts
â”‚   â”‚   â”œâ”€â”€ fill_data_gaps.py          # â­ Recommended: Smart gap-filling script
â”‚   â”‚   â”œâ”€â”€ full_coverage_ingestion.py # â­ Full coverage ingestion (20+ years)
â”‚   â”‚   â”œâ”€â”€ ingest_20years_sp500.py    # Full 20-year historical ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_sp500_15years.py    # S&P 500 15-year ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_more_years.py       # Extend historical years for existing tickers
â”‚   â”‚   â”œâ”€â”€ ingest_extended_universe.py # Extended universe ingestion
â”‚   â”‚   â”œâ”€â”€ batch_ingest.py            # Batch ingestion with retry/backoff
â”‚   â”‚   â”œâ”€â”€ ingest_companyfacts.py     # SEC CompanyFacts API ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_companyfacts_batch.py # Batch CompanyFacts ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_frames.py           # SEC data frames ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_from_file.py        # Ingestion from file input
â”‚   â”‚   â”œâ”€â”€ ingest_universe.py         # Universe-based ingestion with resume support
â”‚   â”‚   â”œâ”€â”€ load_prices_stooq.py       # Stooq price loader (fallback)
â”‚   â”‚   â”œâ”€â”€ load_prices_yfinance.py    # Yahoo Finance price loader
â”‚   â”‚   â”œâ”€â”€ load_historical_prices_15years.py # Historical price loader (15 years)
â”‚   â”‚   â”œâ”€â”€ load_ticker_cik.py         # Ticker to CIK mapping loader
â”‚   â”‚   â”œâ”€â”€ refresh_quotes.py          # Refresh market quotes
â”‚   â”‚   â”œâ”€â”€ backfill_metrics.py        # Backfill missing metrics
â”‚   â”‚   â”œâ”€â”€ fetch_imf_sector_kpis.py   # Fetch IMF sector KPI benchmarks
â”‚   â”‚   â”œâ”€â”€ parse_raw_sec_filings.py   # Parse raw SEC filing data
â”‚   â”‚   â””â”€â”€ monitor_ingestion.py       # Monitor ingestion progress
â”‚   â”‚
â”‚   â”œâ”€â”€ sp1500/                        # S&P 1500 setup and verification scripts
â”‚   â”‚   â”œâ”€â”€ complete_sp1500.py         # Build complete S&P 1500 list from Wikipedia
â”‚   â”‚   â”œâ”€â”€ create_sp1500_file.py      # Create S&P 1500 ticker file
â”‚   â”‚   â”œâ”€â”€ extract_tickers_from_db.py # Extract tickers from database
â”‚   â”‚   â”œâ”€â”€ find_and_test_sp1500.py    # Find and test S&P 1500 file
â”‚   â”‚   â”œâ”€â”€ setup_and_test_sp1500.py   # Setup and test S&P 1500
â”‚   â”‚   â”œâ”€â”€ setup_sp1500.py            # Setup S&P 1500 universe
â”‚   â”‚   â”œâ”€â”€ verify_sp1500_file.py      # Verify S&P 1500 file exists
â”‚   â”‚   â””â”€â”€ verify_sp1500_setup.py     # Quick verification script
â”‚   â”‚
â”‚   â””â”€â”€ utility/                       # Utility and helper scripts
â”‚       â”œâ”€â”€ check_database_simple.py   # Database verification utility
â”‚       â”œâ”€â”€ check_correct_database.py  # Verify correct database path
â”‚       â”œâ”€â”€ check_data_coverage.py     # Check data coverage statistics
â”‚       â”œâ”€â”€ check_dashboard_data.py    # Verify dashboard data integrity
â”‚       â”œâ”€â”€ check_ingestion_status.py  # Ingestion status checker
â”‚       â”œâ”€â”€ check_kpi_values.py        # KPI validation utility
â”‚       â”œâ”€â”€ check_test_progress.py     # Test progress tracker
â”‚       â”œâ”€â”€ check_braces.py            # Syntax checking utility
â”‚       â”œâ”€â”€ check_syntax.py            # Code syntax validation
â”‚       â”œâ”€â”€ find_unclosed_brace.py     # Brace matching utility
â”‚       â”œâ”€â”€ combine_portfolio_files.py # Portfolio file combiner
â”‚       â”œâ”€â”€ chat_terminal.py           # Terminal chat interface
â”‚       â”œâ”€â”€ monitor_progress.py        # Progress monitoring utility
â”‚       â”œâ”€â”€ quick_status.py            # Quick status check
â”‚       â”œâ”€â”€ show_complete_attribution.py # Attribution display utility
â”‚       â”œâ”€â”€ show_detailed_results.py   # Show detailed test results
â”‚       â”œâ”€â”€ show_test_results.py       # Display test results
â”‚       â”œâ”€â”€ plotly_demo.py             # Plotly chart examples
â”‚       â”œâ”€â”€ chat_metrics.py            # Chat metrics utility
â”‚       â”œâ”€â”€ data_sources_backup.py     # Data sources backup utility
â”‚       â”œâ”€â”€ refresh_ticker_catalog.py  # Ticker catalog refresh utility
â”‚       â”œâ”€â”€ improve_kpi_coverage.py    # Improve KPI coverage analysis
â”‚       â”œâ”€â”€ fix_remaining_kpis.py      # Fix remaining KPI issues
â”‚       â”œâ”€â”€ kpi_registry_cli.py        # KPI registry CLI tool
â”‚       â”œâ”€â”€ generate_company_universe.py # Generate company universe JSON for UI
â”‚       â”œâ”€â”€ generate_sp1500_names.py   # Generate S&P 1500 company names from SEC
â”‚       â”œâ”€â”€ generate_help_center_verification_tracker.py # Help center tracker generator
â”‚       â”œâ”€â”€ print_failed_prompts.py    # Print failed test prompts
â”‚       â”œâ”€â”€ smoke_chat_api.py          # Smoke test for chat API
â”‚       â”œâ”€â”€ verify_chatbot_connection.py # Verify chatbot connection
â”‚       â””â”€â”€ main.py                    # Main utility CLI wrapper
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                    # Source package initialization
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ kpi_library.json           # KPI library definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ finalyzeos_chatbot/            # Alternative chatbot module
â”‚   â”‚   â””â”€â”€ (benchmark chatbot files)
â”‚   â”‚
â”‚   â””â”€â”€ finanlyzeos_chatbot/           # Main chatbot source code
â”‚       â”œâ”€â”€ __init__.py                # Package initialization
â”‚       â”‚
â”‚       â”œâ”€â”€ Core Components:
â”‚       â”œâ”€â”€ chatbot.py                 # Main chatbot orchestration (RAG, LLM integration)
â”‚       â”œâ”€â”€ config.py                  # Configuration management (settings loader)
â”‚       â”œâ”€â”€ database.py                # Database abstraction layer (SQLite/Postgres)
â”‚       â”œâ”€â”€ llm_client.py              # LLM provider abstraction (OpenAI/local echo)
â”‚       â”œâ”€â”€ web.py                     # FastAPI web server (REST API endpoints)
â”‚       â”‚
â”‚       â”œâ”€â”€ Analytics & Data:
â”‚       â”œâ”€â”€ analytics_engine.py        # Core analytics engine (KPI calculations)
â”‚       â”œâ”€â”€ analytics_workspace.py     # Analytics workspace management
â”‚       â”œâ”€â”€ advanced_kpis.py           # Advanced KPI calculator (30+ ratios)
â”‚       â”œâ”€â”€ anomaly_detection.py       # Anomaly detection (Z-score analysis)
â”‚       â”œâ”€â”€ predictive_analytics.py    # Predictive analytics (regression, CAGR)
â”‚       â”œâ”€â”€ sector_analytics.py        # Sector benchmarking (GICS sectors)
â”‚       â”‚
â”‚       â”œâ”€â”€ Data Sources & Ingestion:
â”‚       â”œâ”€â”€ data_ingestion.py          # Data ingestion pipeline (SEC, Yahoo, Bloomberg)
â”‚       â”œâ”€â”€ data_sources.py            # Data source integrations (SEC EDGAR, Yahoo Finance)
â”‚       â”œâ”€â”€ data_sources_private.py    # Private data source configurations
â”‚       â”œâ”€â”€ data_validator.py          # Data validation utilities
â”‚       â”œâ”€â”€ external_data.py           # External data providers (FRED, IMF)
â”‚       â”œâ”€â”€ macro_data.py              # Macroeconomic data provider
â”‚       â”œâ”€â”€ multi_source_aggregator.py # Multi-source data aggregation
â”‚       â”œâ”€â”€ sec_bulk.py                # SEC bulk data access
â”‚       â”œâ”€â”€ sec_filing_parser.py       # SEC filing parser
â”‚       â”œâ”€â”€ secdb.py                   # SEC database utilities
â”‚       â”‚
â”‚       â”œâ”€â”€ Context & RAG System:
â”‚       â”œâ”€â”€ context_builder.py         # Financial context builder for RAG
â”‚       â”œâ”€â”€ context_validator.py       # Context validation utilities
â”‚       â”œâ”€â”€ document_context.py        # Document context management
â”‚       â”œâ”€â”€ document_processor.py      # Document processing utilities
â”‚       â”œâ”€â”€ followup_context.py        # Follow-up question context management
â”‚       â”œâ”€â”€ intent_carryover.py        # Intent carryover between conversations
â”‚       â”œâ”€â”€ rag_claim_verifier.py      # RAG claim verification
â”‚       â”œâ”€â”€ rag_controller.py          # RAG controller
â”‚       â”œâ”€â”€ rag_evaluation.py          # RAG evaluation utilities
â”‚       â”œâ”€â”€ rag_feedback.py            # RAG feedback system
â”‚       â”œâ”€â”€ rag_fusion.py              # RAG fusion techniques
â”‚       â”œâ”€â”€ rag_grounded_decision.py   # RAG grounded decision making
â”‚       â”œâ”€â”€ rag_hybrid_retriever.py    # Hybrid retrieval system
â”‚       â”œâ”€â”€ rag_intent_policies.py     # RAG intent policies
â”‚       â”œâ”€â”€ rag_knowledge_graph.py     # Knowledge graph integration
â”‚       â”œâ”€â”€ rag_memory.py              # RAG memory management
â”‚       â”œâ”€â”€ rag_observability.py       # RAG observability
â”‚       â”œâ”€â”€ rag_orchestrator.py        # RAG orchestration
â”‚       â”œâ”€â”€ rag_prompt_template.py     # RAG prompt templates
â”‚       â”œâ”€â”€ rag_reranker.py            # RAG reranking
â”‚       â”œâ”€â”€ rag_retriever.py           # RAG retrieval system
â”‚       â”œâ”€â”€ rag_sparse_retriever.py    # Sparse retrieval system
â”‚       â”œâ”€â”€ rag_structure_aware.py     # Structure-aware RAG
â”‚       â”œâ”€â”€ rag_temporal.py            # Temporal RAG
â”‚       â”‚
â”‚       â”œâ”€â”€ Quality & Verification:
â”‚       â”œâ”€â”€ confidence_scorer.py       # Confidence scoring for responses
â”‚       â”œâ”€â”€ response_corrector.py      # Response correction utilities
â”‚       â”œâ”€â”€ response_verifier.py       # Response verification system
â”‚       â”œâ”€â”€ source_tracer.py           # Source tracing utilities
â”‚       â”œâ”€â”€ source_verifier.py         # Source verification system
â”‚       â”œâ”€â”€ hallucination_detector.py  # Hallucination detection
â”‚       â”œâ”€â”€ ml_response_verifier.py    # ML forecast response verification
â”‚       â”‚
â”‚       â”œâ”€â”€ Formatting & Templates:
â”‚       â”œâ”€â”€ finance_forecast_formatter.py # Finance forecast formatting
â”‚       â”œâ”€â”€ rewrite_formatter.py       # Response rewrite formatting
â”‚       â”œâ”€â”€ template_processor.py      # Template processing utilities
â”‚       â”œâ”€â”€ universal_ml_formatter.py  # Universal ML forecast formatter
â”‚       â”œâ”€â”€ framework_processor.py     # Framework processing utilities
â”‚       â”œâ”€â”€ table_renderer.py          # ASCII table rendering
â”‚       â”‚
â”‚       â”œâ”€â”€ Parsing & NLP:
â”‚       â”œâ”€â”€ parsing/
â”‚       â”‚   â”œâ”€â”€ __init__.py            # Parsing package initialization
â”‚       â”‚   â”œâ”€â”€ alias_builder.py       # Ticker alias resolution (S&P 1500, 90% spelling correction)
â”‚       â”‚   â”œâ”€â”€ aliases.json           # Generated ticker aliases (S&P 1500 coverage)
â”‚       â”‚   â”œâ”€â”€ ontology.py            # Metric ontology (93 KPIs, 200+ synonyms)
â”‚       â”‚   â”œâ”€â”€ parse.py               # Natural language parser (40+ intents, 150+ patterns)
â”‚       â”‚   â”œâ”€â”€ time_grammar.py        # Time period parser (FY, quarters, ranges)
â”‚       â”‚   â”œâ”€â”€ abbreviations.py       # Abbreviation expansion
â”‚       â”‚   â”œâ”€â”€ company_groups.py      # Company group detection
â”‚       â”‚   â”œâ”€â”€ comparative.py         # Comparative language parsing
â”‚       â”‚   â”œâ”€â”€ conditionals.py        # Conditional statement parsing
â”‚       â”‚   â”œâ”€â”€ fuzzy_quantities.py    # Fuzzy quantity parsing
â”‚       â”‚   â”œâ”€â”€ metric_inference.py    # Metric inference from context
â”‚       â”‚   â”œâ”€â”€ multi_intent.py        # Multi-intent detection
â”‚       â”‚   â”œâ”€â”€ natural_filters.py     # Natural language filters
â”‚       â”‚   â”œâ”€â”€ negation.py            # Negation handling
â”‚       â”‚   â”œâ”€â”€ question_chaining.py   # Question chaining detection
â”‚       â”‚   â”œâ”€â”€ sentiment.py           # Sentiment analysis
â”‚       â”‚   â”œâ”€â”€ temporal_relationships.py # Temporal relationship parsing
â”‚       â”‚   â””â”€â”€ trends.py              # Trend detection
â”‚       â”‚
â”‚       â”œâ”€â”€ Spelling & Correction:
â”‚       â”œâ”€â”€ spelling/
â”‚       â”‚   â”œâ”€â”€ __init__.py            # Spelling package initialization
â”‚       â”‚   â”œâ”€â”€ company_corrector.py   # Company name spelling correction
â”‚       â”‚   â”œâ”€â”€ correction_engine.py   # Main correction engine
â”‚       â”‚   â”œâ”€â”€ fuzzy_matcher.py       # Fuzzy matching utilities
â”‚       â”‚   â””â”€â”€ metric_corrector.py    # Metric spelling correction
â”‚       â”‚
â”‚       â”œâ”€â”€ Routing:
â”‚       â”œâ”€â”€ routing/
â”‚       â”‚   â”œâ”€â”€ __init__.py            # Routing package initialization
â”‚       â”‚   â””â”€â”€ enhanced_router.py     # Enhanced intent routing (dashboard detection)
â”‚       â”‚
â”‚       â”œâ”€â”€ Portfolio Management:
â”‚       â”œâ”€â”€ portfolio.py               # Main portfolio management module (combined)
â”‚       â”œâ”€â”€ portfolio_attribution.py   # Performance attribution (Brinson-Fachler)
â”‚       â”œâ”€â”€ portfolio_calculations.py  # Portfolio calculation utilities
â”‚       â”œâ”€â”€ portfolio_enhancements.py  # Portfolio enhancement utilities
â”‚       â”œâ”€â”€ portfolio_enrichment.py    # Portfolio enrichment with fundamentals
â”‚       â”œâ”€â”€ portfolio_export.py        # Portfolio export (PowerPoint, PDF, Excel)
â”‚       â”œâ”€â”€ portfolio_exposure.py      # Exposure analysis (sector, factor)
â”‚       â”œâ”€â”€ portfolio_optimizer.py     # Portfolio optimization (mean-variance)
â”‚       â”œâ”€â”€ portfolio_ppt_builder.py   # Portfolio PowerPoint builder
â”‚       â”œâ”€â”€ portfolio_reporting.py     # Portfolio reporting utilities
â”‚       â”œâ”€â”€ portfolio_risk_metrics.py  # Risk metrics (CVaR, VaR, Sharpe, Sortino)
â”‚       â”œâ”€â”€ portfolio_scenarios.py     # Scenario analysis & stress testing
â”‚       â””â”€â”€ portfolio_trades.py        # Trade recommendation utilities
â”‚       â”‚
â”‚       â”œâ”€â”€ ML Forecasting:
â”‚       â”œâ”€â”€ ml_forecasting/
â”‚       â”‚   â”œâ”€â”€ __init__.py            # ML forecasting package initialization
â”‚       â”‚   â”œâ”€â”€ ml_forecaster.py       # Main ML forecaster (model selection)
â”‚       â”‚   â”œâ”€â”€ arima_forecaster.py    # ARIMA model (statistical time series)
â”‚       â”‚   â”œâ”€â”€ prophet_forecaster.py  # Prophet model (seasonal patterns)
â”‚       â”‚   â”œâ”€â”€ ets_forecaster.py      # ETS model (exponential smoothing)
â”‚       â”‚   â”œâ”€â”€ lstm_forecaster.py     # LSTM model (deep learning RNN)
â”‚       â”‚   â”œâ”€â”€ transformer_forecaster.py # Transformer model (attention-based)
â”‚       â”‚   â”œâ”€â”€ multivariate_forecaster.py # Multivariate forecasting
â”‚       â”‚   â”œâ”€â”€ preprocessing.py       # Data preprocessing (scaling, normalization)
â”‚       â”‚   â”œâ”€â”€ feature_engineering.py # Feature engineering utilities
â”‚       â”‚   â”œâ”€â”€ hyperparameter_tuning.py # Hyperparameter optimization (Optuna)
â”‚       â”‚   â”œâ”€â”€ backtesting.py         # Model backtesting utilities
â”‚       â”‚   â”œâ”€â”€ validation.py          # Model validation utilities
â”‚       â”‚   â”œâ”€â”€ explainability.py      # Model explainability (SHAP, attention)
â”‚       â”‚   â”œâ”€â”€ uncertainty.py         # Uncertainty quantification
â”‚       â”‚   â”œâ”€â”€ regime_detection.py    # Regime detection (market states)
â”‚       â”‚   â”œâ”€â”€ technical_indicators.py # Technical indicators for features
â”‚       â”‚   â”œâ”€â”€ external_factors.py    # External factor integration
â”‚       â”‚   â””â”€â”€ user_plugins.py        # User plugin system
â”‚       â”‚
â”‚       â”œâ”€â”€ Export & Presentation:
â”‚       â”œâ”€â”€ export_pipeline.py         # Export pipeline (PDF, PPTX, Excel)
â”‚       â”œâ”€â”€ cfi_ppt_builder.py         # CFI-style PowerPoint builder (12 slides)
â”‚       â”‚
â”‚       â”œâ”€â”€ Utilities:
â”‚       â”œâ”€â”€ tasks.py                   # Task queue management
â”‚       â”œâ”€â”€ help_content.py            # Help content and documentation
â”‚       â”œâ”€â”€ dashboard_utils.py         # Dashboard utility functions
â”‚       â”œâ”€â”€ imf_proxy.py               # IMF data proxy
â”‚       â”œâ”€â”€ kpi_backfill.py            # KPI backfill utilities
â”‚       â”œâ”€â”€ kpi_lookup.py              # KPI lookup utilities
â”‚       â”œâ”€â”€ custom_kpis.py             # Custom KPI definitions
â”‚       â”œâ”€â”€ backfill_policy.py         # Backfill policy management
â”‚       â”œâ”€â”€ ticker_universe.py         # Ticker universe management
â”‚       â”œâ”€â”€ interactive_modeling.py    # Interactive modeling utilities
â”‚       â”‚
â”‚       â””â”€â”€ Static Assets:
â”‚       â””â”€â”€ static/
â”‚           â”œâ”€â”€ app.js                 # Frontend application logic (SPA)
â”‚           â”œâ”€â”€ styles.css             # UI styling (markdown, progress indicator)
â”‚           â”œâ”€â”€ index.html             # Web UI entry point
â”‚           â”œâ”€â”€ favicon.svg            # Favicon
â”‚           â”œâ”€â”€ cfi_dashboard.css      # CFI dashboard styling
â”‚           â”œâ”€â”€ cfi_dashboard.js       # CFI dashboard JavaScript
â”‚           â”œâ”€â”€ portfolio_dashboard.js # Portfolio dashboard JavaScript
â”‚           â””â”€â”€ data/
â”‚               â”œâ”€â”€ company_universe.json # Company universe metadata
â”‚               â””â”€â”€ kpi_library.json   # KPI library definitions
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md                      # Documentation index
â”‚   â”‚
â”‚   â”œâ”€â”€ architecture/                    # Architecture documentation
â”‚   â”‚   â”œâ”€â”€ architecture.md            # System architecture diagram
â”‚   â”‚   â”œâ”€â”€ chatbot_system_overview_en.md # System overview
â”‚   â”‚   â””â”€â”€ product_design_spec.md     # Product design specifications
â”‚   â”‚
â”‚   â”œâ”€â”€ demos/                          # Demo and presentation docs
â”‚   â”‚   â”œâ”€â”€ CBA_POSTER_CONDENSED.md    # CBA poster (condensed)
â”‚   â”‚   â”œâ”€â”€ CBA_POSTER_CONTENT.md      # CBA poster content
â”‚   â”‚   â”œâ”€â”€ CHATBOT_DEMO_GUIDE.md      # Chatbot demo guide
â”‚   â”‚   â””â”€â”€ CLIENT_DEMO_PROMPTS.md     # Client demo prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ guides/                         # User and technical guides
â”‚   â”‚   â”œâ”€â”€ ALL_ML_FORECASTING_PROMPTS.md # All ML forecasting prompts
â”‚   â”‚   â”œâ”€â”€ ML_FORECASTING_QUICK_REFERENCE.md # ML forecasting quick reference
â”‚   â”‚   â”œâ”€â”€ ML_FORECASTING_PROMPTS.md  # ML forecasting prompts guide
â”‚   â”‚   â”œâ”€â”€ PORTFOLIO_QUESTIONS_GUIDE.md # Portfolio questions guide
â”‚   â”‚   â”œâ”€â”€ FINANCIAL_PROMPTS_GUIDE.md # Financial prompts guide
â”‚   â”‚   â”œâ”€â”€ CHATBOT_PROMPT_GUIDE.md    # Chatbot prompt guide
â”‚   â”‚   â”œâ”€â”€ EXPANDED_QUERY_CAPABILITIES.md # Expanded query capabilities guide
â”‚   â”‚   â”œâ”€â”€ COMPREHENSIVE_DATA_SOURCES.md # Data sources guide
â”‚   â”‚   â”œâ”€â”€ DASHBOARD_SOURCES_INSTRUCTIONS.md # Dashboard sources guide
â”‚   â”‚   â”œâ”€â”€ DATA_INGESTION_PLAN.md     # Data ingestion planning
â”‚   â”‚   â”œâ”€â”€ ENABLE_FRED_GUIDE.md       # FRED integration guide
â”‚   â”‚   â”œâ”€â”€ EXPAND_DATA_GUIDE.md       # Data expansion guide
â”‚   â”‚   â”œâ”€â”€ EXTENDED_INGESTION_INFO.md # Extended ingestion info
â”‚   â”‚   â”œâ”€â”€ INSTALLATION_GUIDE.md      # Installation instructions
â”‚   â”‚   â”œâ”€â”€ SETUP_GUIDE.md             # Setup guide
â”‚   â”‚   â”œâ”€â”€ TEAM_SETUP_GUIDE.md        # Team onboarding guide
â”‚   â”‚   â”œâ”€â”€ PLOTLY_INTEGRATION.md      # Plotly integration docs
â”‚   â”‚   â”œâ”€â”€ MULTI_TICKER_DASHBOARD_GUIDE.md # Multi-ticker dashboard guide
â”‚   â”‚   â”œâ”€â”€ MULTI_TICKER_DASHBOARDS.md # Multi-ticker dashboards guide
â”‚   â”‚   â”œâ”€â”€ SOURCES_LOCATION_GUIDE.md  # Sources location guide
â”‚   â”‚   â”œâ”€â”€ SOURCES_TROUBLESHOOTING.md # Sources troubleshooting
â”‚   â”‚   â”œâ”€â”€ SYSTEM_PROMPT_SIMPLIFIED.md # System prompt guide
â”‚   â”‚   â”œâ”€â”€ ENHANCED_ROUTING.md        # Enhanced routing guide
â”‚   â”‚   â”œâ”€â”€ RAW_SEC_PARSER_IMPLEMENTATION_GUIDE.md # SEC parser guide
â”‚   â”‚   â”œâ”€â”€ export_pipeline_scope.md  # Export pipeline scope
â”‚   â”‚   â”œâ”€â”€ orchestration_playbook.md # Deployment orchestration guide
â”‚   â”‚   â”œâ”€â”€ ticker_names.md           # Ticker coverage list (1,599 S&P 1500 companies)
â”‚   â”‚   â””â”€â”€ (additional guides)
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                      # Ingestion documentation
â”‚   â”‚   â”œâ”€â”€ FULL_COVERAGE_GUIDE.md     # Full coverage ingestion guide
â”‚   â”‚   â””â”€â”€ FULL_INGESTION_SCRIPTS.md  # Full ingestion scripts guide
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                       # Database documentation
â”‚   â”‚   â”œâ”€â”€ DATABASE_STRUCTURE_POSTER.md # Database structure poster
â”‚   â”‚   â”œâ”€â”€ EXPECTED_DATA_VOLUMES.md   # Expected data volumes
â”‚   â”‚   â””â”€â”€ full_coverage_summary.json # Full coverage summary data
â”‚   â”‚
â”‚   â”œâ”€â”€ accuracy/                       # Accuracy testing documentation
â”‚   â”‚   â”œâ”€â”€ README_ACCURACY_TESTING.md # Accuracy testing guide
â”‚   â”‚   â”œâ”€â”€ 100_PERCENT_ACCURACY_ACHIEVED.md # 100% accuracy achievement
â”‚   â”‚   â”œâ”€â”€ ACCURACY_100_PERCENT_PROOF.md # Accuracy proof documentation
â”‚   â”‚   â”œâ”€â”€ ACCURACY_EXECUTIVE_SUMMARY.md # Executive summary
â”‚   â”‚   â”œâ”€â”€ ACCURACY_FINAL_PROOF.md    # Final accuracy proof
â”‚   â”‚   â”œâ”€â”€ ACCURACY_IMPROVEMENT_PLAN.md # Improvement plan
â”‚   â”‚   â”œâ”€â”€ ACCURACY_METRICS_DETAILED.md # Detailed metrics
â”‚   â”‚   â”œâ”€â”€ ACCURACY_SLIDE_SUMMARY.md  # Slide summary
â”‚   â”‚   â”œâ”€â”€ ACCURACY_STATS_FOR_SLIDES.md # Stats for slides
â”‚   â”‚   â”œâ”€â”€ ACCURACY_VERIFICATION_SLIDES.md # Verification slides
â”‚   â”‚   â”œâ”€â”€ help_center_confidence_workflow.md # Help center workflow
â”‚   â”‚   â””â”€â”€ help_center_verification_tracker.csv # Verification tracker
â”‚   â”‚
â”‚   â”œâ”€â”€ executive/                      # Executive documentation
â”‚   â”‚   â”œâ”€â”€ BENCHMARKOS_SLIDE.md       # BenchmarkOS slide
â”‚   â”‚   â”œâ”€â”€ COMPREHENSIVE_ACCURACY_FIX_SUMMARY.md # Accuracy fix summary
â”‚   â”‚   â”œâ”€â”€ CRITICAL_ACCURACY_FIX.md   # Critical accuracy fix
â”‚   â”‚   â”œâ”€â”€ FINAL_SP500_ALL_KPIS_REPORT.md # Final S&P 500 KPI report
â”‚   â”‚   â”œâ”€â”€ FIX_CHATBOT_ACCURACY_ISSUE.md # Chatbot accuracy fix
â”‚   â”‚   â””â”€â”€ HOW_TO_MAKE_ALL_ANSWERS_TRUSTED.md # Trusted answers guide
â”‚   â”‚
â”‚   â”œâ”€â”€ plans/                          # Planning documentation
â”‚   â”‚   â””â”€â”€ ML_PROMPT_TESTING_PLAN.md  # ML prompt testing plan
â”‚   â”‚
â”‚   â”œâ”€â”€ organization/                   # Organization documentation
â”‚   â”‚   â”œâ”€â”€ REPOSITORY_ORGANIZATION_2024.md # Repository organization (2024)
â”‚   â”‚   â”œâ”€â”€ REPOSITORY_ORGANIZATION_COMPLETE.md # Repository organization (complete)
â”‚   â”‚   â””â”€â”€ COMPLETE_ORGANIZATION_STATUS.md # Organization status
â”‚   â”‚
â”‚   â”œâ”€â”€ enhancements/                  # Enhancement documentation
â”‚   â”‚   â”œâ”€â”€ FINANCIAL_PROMPTS_ENHANCEMENT_COMPLETE.md # Financial prompts enhancement
â”‚   â”‚   â”œâ”€â”€ MULTI_SOURCE_INTEGRATION.md # Multi-source integration
â”‚   â”‚   â”œâ”€â”€ MARKDOWN_FORMATTING_FIX.md # Markdown formatting fix
â”‚   â”‚   â”œâ”€â”€ MESSAGE_FORMATTING_IMPROVED.md # Message formatting improvements
â”‚   â”‚   â”œâ”€â”€ PDF_ENHANCEMENTS_COMPLETE.md # PDF enhancements
â”‚   â”‚   â”œâ”€â”€ PDF_EXPORT_IMPROVEMENTS.md # PDF export improvements
â”‚   â”‚   â”œâ”€â”€ PDF_LAYOUT_FIXES_COMPLETE.md # PDF layout fixes
â”‚   â”‚   â”œâ”€â”€ PROGRESS_INDICATOR_ENHANCEMENT.md # Progress indicator enhancement
â”‚   â”‚   â”œâ”€â”€ QUESTION_DETECTION_FIX.md  # Question detection fix
â”‚   â”‚   â”œâ”€â”€ SOURCES_AND_DEPTH_FIX.md  # Sources and depth fix
â”‚   â”‚   â””â”€â”€ INVESTMENT_GRADE_PDF_COMPLETE.md # Investment-grade PDF
â”‚   â”‚
â”‚   â”œâ”€â”€ fixes/                          # Fix documentation
â”‚   â”‚   â”œâ”€â”€ FINAL_NAN_FIX_COMPLETE.md  # NaN fix completion
â”‚   â”‚   â”œâ”€â”€ JAVASCRIPT_SYNTAX_ERROR_FIX.md # JavaScript syntax fix
â”‚   â”‚   â”œâ”€â”€ MULTI_TICKER_DASHBOARD_FIX.md # Multi-ticker dashboard fix
â”‚   â”‚   â”œâ”€â”€ MULTI_TICKER_DETECTION_FIX.md # Multi-ticker detection fix
â”‚   â”‚   â”œâ”€â”€ MULTI_TICKER_TOOLBAR_REMOVAL.md # Multi-ticker toolbar removal
â”‚   â”‚   â”œâ”€â”€ PDF_EXPORT_FIX.md          # PDF export fix
â”‚   â”‚   â”œâ”€â”€ PDF_UNICODE_FIX.md        # PDF unicode fix
â”‚   â”‚   â”œâ”€â”€ PLOTLY_NAN_ERRORS_FIX.md   # Plotly NaN errors fix
â”‚   â”‚   â”œâ”€â”€ SOURCES_PANEL_RESTORED.md  # Sources panel restoration
â”‚   â”‚   â””â”€â”€ SOURCES_PANEL_VISIBILITY_FIX.md # Sources panel visibility fix
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                             # UI documentation
â”‚   â”‚   â”œâ”€â”€ USER_GUIDE.md              # User guide
â”‚   â”‚   â”œâ”€â”€ ACCURATE_SOURCE_LINKS_UPDATE.md # Source links update
â”‚   â”‚   â”œâ”€â”€ BUTTON_EVENT_HANDLER_FIX.md # Button event handler fix
â”‚   â”‚   â”œâ”€â”€ COMPANY_SELECTOR_COMPARISON.md # Company selector comparison
â”‚   â”‚   â”œâ”€â”€ COMPANY_SELECTOR_SCALING_FIX.md # Company selector scaling fix
â”‚   â”‚   â”œâ”€â”€ COMPREHENSIVE_IMPROVEMENTS_SUMMARY.md # Comprehensive improvements
â”‚   â”‚   â”œâ”€â”€ DASHBOARD_IMPROVEMENTS.md  # Dashboard improvements
â”‚   â”‚   â”œâ”€â”€ DASHBOARD_LAYOUT_IMPROVEMENTS.md # Dashboard layout improvements
â”‚   â”‚   â”œâ”€â”€ DATA_SOURCES_FORMAT.md     # Data sources format
â”‚   â”‚   â”œâ”€â”€ FINAL_LAYOUT_SUMMARY.md    # Final layout summary
â”‚   â”‚   â”œâ”€â”€ IMPLEMENTATION_COMPLETE.md # Implementation completion
â”‚   â”‚   â”œâ”€â”€ LAYOUT_REORGANIZATION.md   # Layout reorganization
â”‚   â”‚   â”œâ”€â”€ LINKS_FIX_SUMMARY.md       # Links fix summary
â”‚   â”‚   â””â”€â”€ PLOTLY_NAN_FIX.md          # Plotly NaN fix
â”‚   â”‚
â”‚   â”œâ”€â”€ summaries/                      # Summary documentation
â”‚   â”‚   â”œâ”€â”€ PATTERN_EXPANSION_SUMMARY.md # Pattern expansion summary
â”‚   â”‚   â””â”€â”€ (28 summary files documenting various features and improvements)
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                       # Analysis documentation
â”‚   â”‚   â”œâ”€â”€ README.md                   # Analysis documentation index
â”‚   â”‚   â””â”€â”€ (25+ analysis reports and documentation files)
â”‚   â”‚
â”‚   â”œâ”€â”€ sp1500/                         # S&P 1500 documentation
â”‚   â”‚   â”œâ”€â”€ SP1500_SETUP_COMPLETE.md   # S&P 1500 setup completion
â”‚   â”‚   â”œâ”€â”€ SP1500_TESTING_INSTRUCTIONS.md # S&P 1500 testing guide
â”‚   â”‚   â”œâ”€â”€ SP1500_SUPPORT_ANALYSIS.md # S&P 1500 support analysis
â”‚   â”‚   â”œâ”€â”€ SP1500_SUPPORT_STATUS.md   # S&P 1500 support status
â”‚   â”‚   â”œâ”€â”€ SP1500_FIXES_COMPLETE.md   # S&P 1500 fixes completion
â”‚   â”‚   â””â”€â”€ ADD_SP1500_SUPPORT.md      # Adding S&P 1500 support guide
â”‚   â”‚
â”‚   â”œâ”€â”€ improvements/                   # Improvement documentation
â”‚   â”‚   â”œâ”€â”€ COMPREHENSIVE_COVERAGE_REPORT.md # Coverage report
â”‚   â”‚   â”œâ”€â”€ COMPREHENSIVE_IMPROVEMENTS_COMPLETE.md # Improvements completion
â”‚   â”‚   â”œâ”€â”€ FINAL_IMPROVEMENTS_SUMMARY.md # Final improvements summary
â”‚   â”‚   â”œâ”€â”€ IMPROVEMENTS_TO_100_PERCENT.md # Improvements to 100% accuracy
â”‚   â”‚   â””â”€â”€ (additional improvement docs)
â”‚   â”‚
â”‚   â””â”€â”€ reports/                        # Generated reports
â”‚       â””â”€â”€ (various analysis and improvement reports)
â”‚
â”œâ”€â”€ data/                              # Data files and databases
â”‚   â”œâ”€â”€ sample_financials.csv          # Sample financial data
â”‚   â”œâ”€â”€ test_chatbot.db                # Test database
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/                         # Cached data
â”‚   â”‚   â””â”€â”€ edgar_tickers.json         # Cached EDGAR ticker data
â”‚   â”‚
â”‚   â”œâ”€â”€ chroma_db/                     # ChromaDB vector database
â”‚   â”‚   â””â”€â”€ chroma.sqlite3             # ChromaDB SQLite file
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                    # Evaluation datasets
â”‚   â”‚   â””â”€â”€ rag_test_set.json          # RAG evaluation test set
â”‚   â”‚
â”‚   â”œâ”€â”€ external/                      # External data sources
â”‚   â”‚   â””â”€â”€ imf_sector_kpis.json       # IMF sector KPI benchmarks
â”‚   â”‚
â”‚   â”œâ”€â”€ portfolios/                    # Portfolio data files
â”‚   â”‚   â”œâ”€â”€ mizuho_fi_capital_portfolio.csv # Sample portfolio (Mizuho)
â”‚   â”‚   â””â”€â”€ README.md                  # Portfolio data documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ sqlite/                        # SQLite databases
â”‚   â”‚   â”œâ”€â”€ finanlyzeos_chatbot.sqlite3 # Main SQLite database
â”‚   â”‚   â”œâ”€â”€ finanlyzeos_chatbot.sqlite3-shm # SQLite shared memory
â”‚   â”‚   â”œâ”€â”€ finanlyzeos_chatbot.sqlite3-wal # SQLite write-ahead log
â”‚   â”‚   â”œâ”€â”€ finalyzeos_chatbot.sqlite3  # Alternative database
â”‚   â”‚   â”œâ”€â”€ benchmarkos_chatbot.sqlite3-shm # Benchmark shared memory
â”‚   â”‚   â””â”€â”€ benchmarkos_chatbot.sqlite3-wal # Benchmark write-ahead log
â”‚   â”‚
â”‚   â””â”€â”€ tickers/                       # Ticker universe files
â”‚       â”œâ”€â”€ universe_sp500.txt         # S&P 500 ticker list (475 companies)
â”‚       â”œâ”€â”€ universe_sp1500.txt        # S&P 1500 ticker list (1,599 companies)
â”‚       â”œâ”€â”€ sec_top100.txt             # Top 100 SEC companies
â”‚       â”œâ”€â”€ universe_custom.txt        # Custom universe list
â”‚       â””â”€â”€ sample_watchlist.txt       # Sample watchlist
â”‚
â”œâ”€â”€ cache/                              # Generated at runtime (gitignored)
â”‚   â”œâ”€â”€ edgar_tickers.json             # Cached EDGAR ticker data
â”‚   â””â”€â”€ progress/
â”‚       â””â”€â”€ fill_gaps_summary.json     # Ingestion progress tracking
â”‚
â”œâ”€â”€ research/                           # Research and analysis code
â”‚   â””â”€â”€ analysis/                      # Analysis scripts (28 Python files)
â”‚       â”œâ”€â”€ analyze_accuracy_improvements.py # Accuracy improvement analysis
â”‚       â”œâ”€â”€ analyze_chatbot_performance.py # Chatbot performance analysis
â”‚       â”œâ”€â”€ analyze_coverage_gaps.py   # Coverage gap analysis
â”‚       â”œâ”€â”€ analyze_data_quality.py    # Data quality analysis
â”‚       â”œâ”€â”€ analyze_kpi_coverage.py    # KPI coverage analysis
â”‚       â”œâ”€â”€ analyze_metric_coverage.py # Metric coverage analysis
â”‚       â”œâ”€â”€ analyze_portfolio_performance.py # Portfolio performance analysis
â”‚       â”œâ”€â”€ analyze_query_patterns.py  # Query pattern analysis
â”‚       â”œâ”€â”€ analyze_response_quality.py # Response quality analysis
â”‚       â”œâ”€â”€ analyze_source_coverage.py # Source coverage analysis
â”‚       â”œâ”€â”€ benchmark_chatbot.py       # Chatbot benchmarking
â”‚       â”œâ”€â”€ compare_models.py          # Model comparison analysis
â”‚       â”œâ”€â”€ evaluate_accuracy.py       # Accuracy evaluation
â”‚       â”œâ”€â”€ evaluate_completeness.py   # Completeness evaluation
â”‚       â”œâ”€â”€ evaluate_performance.py    # Performance evaluation
â”‚       â”œâ”€â”€ generate_accuracy_report.py # Accuracy report generation
â”‚       â”œâ”€â”€ generate_coverage_report.py # Coverage report generation
â”‚       â”œâ”€â”€ generate_performance_report.py # Performance report generation
â”‚       â”œâ”€â”€ measure_latency.py         # Latency measurement
â”‚       â”œâ”€â”€ profile_memory_usage.py    # Memory usage profiling
â”‚       â”œâ”€â”€ test_data_integrity.py     # Data integrity testing
â”‚       â”œâ”€â”€ test_model_accuracy.py     # Model accuracy testing
â”‚       â”œâ”€â”€ test_query_performance.py  # Query performance testing
â”‚       â”œâ”€â”€ validate_data_sources.py   # Data source validation
â”‚       â”œâ”€â”€ validate_kpi_calculations.py # KPI calculation validation
â”‚       â”œâ”€â”€ validate_metrics.py        # Metrics validation
â”‚       â”œâ”€â”€ validate_portfolio_calculations.py # Portfolio calculation validation
â”‚       â””â”€â”€ validate_responses.py      # Response validation
â”‚
â”œâ”€â”€ temp/                               # Temporary files (gitignored)
â”‚   â”œâ”€â”€ apple-companyfacts.json        # Temporary SEC data
â”‚   â”œâ”€â”€ apple-q4-2024-results.html     # Temporary HTML
â”‚   â”œâ”€â”€ extract_pdf.py                 # PDF extraction utility
â”‚   â”œâ”€â”€ FY24_Q4_Consolidated_Financial_Statements.pdf # Sample PDF
â”‚   â”œâ”€â”€ msft-2024-10k.htm              # Sample SEC filing
â”‚   â””â”€â”€ msft-companyfacts.json         # Sample company facts
â”‚
â”œâ”€â”€ archive/                            # Archived files
â”‚   â””â”€â”€ arxiv_2509_26632.txt           # Archived research paper
â”‚
â”œâ”€â”€ webui/                              # Web UI files
â”‚   â”œâ”€â”€ index.html                      # Web UI entry point
â”‚   â”œâ”€â”€ app.js                          # Frontend application logic
â”‚   â”œâ”€â”€ styles.css                      # UI styling (7432 lines)
â”‚   â”œâ”€â”€ package.json                    # Node.js dependencies
â”‚   â”œâ”€â”€ service-worker.js               # Service worker for PWA
â”‚   â”œâ”€â”€ start_dashboard.js              # Dashboard startup script
â”‚   â”œâ”€â”€ favicon.svg                     # Favicon
â”‚   â”‚
â”‚   â”œâ”€â”€ CFI Dashboards:                 # CFI (Corporate Finance Institute) style dashboards
â”‚   â”œâ”€â”€ cfi_dashboard.html              # Main CFI dashboard HTML
â”‚   â”œâ”€â”€ cfi_dashboard.js                # CFI dashboard JavaScript
â”‚   â”œâ”€â”€ cfi_dashboard.css               # CFI dashboard styling
â”‚   â”œâ”€â”€ cfi_dashboard_v2.html           # CFI dashboard version 2
â”‚   â”œâ”€â”€ cfi_dashboard_improved.html     # Improved CFI dashboard
â”‚   â”œâ”€â”€ cfi_dashboard_backup_original.html # Original backup
â”‚   â”œâ”€â”€ cfi_dashboard_old_backup.html   # Old backup
â”‚   â”‚
â”‚   â”œâ”€â”€ CFI Compare Views:              # CFI comparison interfaces
â”‚   â”œâ”€â”€ cfi_compare.html                # CFI compare view HTML
â”‚   â”œâ”€â”€ cfi_compare.js                  # CFI compare view JavaScript
â”‚   â”œâ”€â”€ cfi_compare.css                 # CFI compare view styling
â”‚   â”œâ”€â”€ cfi_compare_demo.html           # CFI compare demo
â”‚   â”œâ”€â”€ cfi_compare_standalone.html     # CFI compare standalone
â”‚   â”‚
â”‚   â”œâ”€â”€ CFI Dense Views:                # CFI dense layout interfaces
â”‚   â”œâ”€â”€ cfi_dense.html                  # CFI dense view HTML
â”‚   â”œâ”€â”€ cfi_dense.js                    # CFI dense view JavaScript
â”‚   â”œâ”€â”€ cfi_dense.css                   # CFI dense view styling
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                           # Web UI data files
â”‚   â”‚   â”œâ”€â”€ company_universe.json       # Company universe for dropdowns
â”‚   â”‚   â””â”€â”€ kpi_definitions.json        # KPI definitions for UI
â”‚   â”‚
â”‚   â””â”€â”€ static/                         # Static assets
â”‚       â””â”€â”€ portfolio_data.json         # Portfolio data for demos
â”‚
â””â”€â”€ tests/                              # Comprehensive test suite (145 files)
    â”œâ”€â”€ README.md                       # Testing documentation
    â”œâ”€â”€ conftest.py                     # Pytest configuration and fixtures
    â”‚
    â”œâ”€â”€ unit/                           # Unit tests (25+ files)
    â”‚   â”œâ”€â”€ test_analytics.py           # Analytics unit tests
    â”‚   â”œâ”€â”€ test_analytics_engine.py    # Analytics engine unit tests
    â”‚   â”œâ”€â”€ test_analysis_templates.py  # Analysis template unit tests
    â”‚   â”œâ”€â”€ test_cli_tables.py          # CLI table rendering tests
    â”‚   â”œâ”€â”€ test_custom_kpis_workspace.py # Custom KPIs workspace tests
    â”‚   â”œâ”€â”€ test_database.py            # Database unit tests
    â”‚   â”œâ”€â”€ test_data_dictionary.py     # Data dictionary tests
    â”‚   â”œâ”€â”€ test_data_ingestion.py      # Data ingestion unit tests
    â”‚   â”œâ”€â”€ test_document_upload.py     # Document upload tests
    â”‚   â”œâ”€â”€ test_router_kpi_intents.py  # Router KPI intent tests
    â”‚   â”œâ”€â”€ test_uploaded_document_context.py # Uploaded document context tests
    â”‚   â”œâ”€â”€ test_parsing.py             # Parsing unit tests
    â”‚   â”œâ”€â”€ test_portfolio.py           # Portfolio unit tests
    â”‚   â”œâ”€â”€ test_ml_forecasting.py      # ML forecasting unit tests
    â”‚   â”œâ”€â”€ test_rag_components.py      # RAG components unit tests
    â”‚   â”œâ”€â”€ test_verification.py        # Verification unit tests
    â”‚   â”œâ”€â”€ test_export.py              # Export functionality unit tests
    â”‚   â”œâ”€â”€ test_utilities.py           # Utilities unit tests
    â”‚   â””â”€â”€ (additional unit tests)
    â”‚
    â”œâ”€â”€ integration/                    # Integration tests (15+ files)
    â”‚   â”œâ”€â”€ test_chatbot_sec_fix.py     # SEC integration tests
    â”‚   â”œâ”€â”€ test_sec_api_fix.py         # SEC API integration tests
    â”‚   â”œâ”€â”€ test_new_analytics.py       # New analytics integration tests
    â”‚   â”œâ”€â”€ test_dashboard_flow.py      # Dashboard workflow integration tests
    â”‚   â”œâ”€â”€ test_fixes.py               # General fixes integration tests
    â”‚   â”œâ”€â”€ test_enhanced_routing.py    # Enhanced routing integration tests
    â”‚   â”œâ”€â”€ test_data_pipeline.py       # Data pipeline integration tests
    â”‚   â”œâ”€â”€ test_ml_pipeline.py         # ML pipeline integration tests
    â”‚   â”œâ”€â”€ test_portfolio_integration.py # Portfolio integration tests
    â”‚   â”œâ”€â”€ test_rag_integration.py     # RAG integration tests
    â”‚   â””â”€â”€ (additional integration tests)
    â”‚
    â”œâ”€â”€ e2e/                            # End-to-end tests (20+ files)
    â”‚   â”œâ”€â”€ test_all_sp500_dashboards.py # Full S&P 500 dashboard test
    â”‚   â”œâ”€â”€ test_sample_companies.py    # Sample companies test (10 companies)
    â”‚   â”œâ”€â”€ test_single_company.py      # Single company test (Apple)
    â”‚   â”œâ”€â”€ test_chatbot_stress_test.py # FinalyzeOS stress test
    â”‚   â”œâ”€â”€ test_chatgpt_style.py       # ChatGPT-style test
    â”‚   â”œâ”€â”€ test_comprehensive_sources.py # Comprehensive sources test
    â”‚   â”œâ”€â”€ test_ml_detailed_answers.py # ML detailed answers test
    â”‚   â”œâ”€â”€ test_portfolio_workflows.py # Portfolio workflow tests
    â”‚   â”œâ”€â”€ test_export_workflows.py    # Export workflow tests
    â”‚   â”œâ”€â”€ test_user_journeys.py       # User journey tests
    â”‚   â”œâ”€â”€ PORTFOLIO_STRESS_TEST_SUMMARY.md # Portfolio stress test summary
    â”‚   â””â”€â”€ (additional e2e tests)
    â”‚
    â”œâ”€â”€ metric_recognition/             # Metric recognition tests (15+ files)
    â”‚   â”œâ”€â”€ test_metric_variations.py   # Metric variation tests
    â”‚   â”œâ”€â”€ test_metric_edge_cases.py   # Metric edge case tests
    â”‚   â”œâ”€â”€ test_metric_patterns.py     # Metric pattern tests
    â”‚   â”œâ”€â”€ test_metric_recognition.py  # Metric recognition tests
    â”‚   â”œâ”€â”€ test_comprehensive_coverage.py # Comprehensive metric coverage
    â”‚   â”œâ”€â”€ test_comprehensive_spelling.py # Comprehensive spelling tests
    â”‚   â”œâ”€â”€ test_metric_spelling_comprehensive.py # Metric spelling comprehensive
    â”‚   â”œâ”€â”€ test_spelling_mistakes.py   # Spelling mistake tests
    â”‚   â”œâ”€â”€ test_ontology.py            # Ontology tests
    â”‚   â”œâ”€â”€ test_synonyms.py            # Synonym recognition tests
    â”‚   â”œâ”€â”€ test_abbreviations.py       # Abbreviation tests
    â”‚   â””â”€â”€ (additional metric tests)
    â”‚
    â”œâ”€â”€ sp1500/                         # S&P 1500 tests (10+ files)
    â”‚   â”œâ”€â”€ test_all_sp1500_companies.py # All S&P 1500 companies test
    â”‚   â”œâ”€â”€ test_all_sp1500_tickers.py  # All S&P 1500 tickers test
    â”‚   â”œâ”€â”€ test_sp1500_comprehensive.py # Comprehensive S&P 1500 test
    â”‚   â”œâ”€â”€ test_sp1500_support.py      # S&P 1500 support test
    â”‚   â”œâ”€â”€ test_sp1500_coverage.py     # S&P 1500 coverage test
    â”‚   â”œâ”€â”€ test_sp1500_accuracy.py     # S&P 1500 accuracy test
    â”‚   â””â”€â”€ (additional S&P 1500 tests)
    â”‚
    â”œâ”€â”€ debug/                          # Debug and troubleshooting scripts (20+ files)
    â”‚   â”œâ”€â”€ debug_company_names.py      # Debug company name recognition
    â”‚   â”œâ”€â”€ debug_failures.py           # Debug recognition failures
    â”‚   â”œâ”€â”€ debug_remaining_failures.py # Debug remaining failures
    â”‚   â”œâ”€â”€ debug_bookng.py             # Debug specific company (Booking)
    â”‚   â”œâ”€â”€ debug_bookng_detailed.py    # Detailed Booking debug
    â”‚   â”œâ”€â”€ debug_bookng_live.py        # Live Booking debug
    â”‚   â”œâ”€â”€ analyze_company_name_failures.py # Analyze company name failures
    â”‚   â”œâ”€â”€ get_all_failures.py         # Get all failures
    â”‚   â”œâ”€â”€ identify_all_failures.py    # Identify all failures
    â”‚   â”œâ”€â”€ test_all_failures_detailed.py # Detailed failure tests
    â”‚   â”œâ”€â”€ test_specific_failures.py   # Specific failure tests
    â”‚   â”œâ”€â”€ test_specific_spelling_failures.py # Spelling failure tests
    â”‚   â””â”€â”€ (additional debug scripts)
    â”‚
    â”œâ”€â”€ verification/                   # Verification scripts (10+ files)
    â”‚   â”œâ”€â”€ verify_metrics.py           # Metric verification
    â”‚   â”œâ”€â”€ verify_new_data.py          # New data verification
    â”‚   â”œâ”€â”€ verify_100_percent_complete.py # 100% completeness verification
    â”‚   â”œâ”€â”€ check_sources.py            # Source checking utility
    â”‚   â”œâ”€â”€ verify_accuracy.py          # Accuracy verification
    â”‚   â”œâ”€â”€ verify_completeness.py      # Completeness verification
    â”‚   â”œâ”€â”€ verify_performance.py       # Performance verification
    â”‚   â””â”€â”€ (additional verification scripts)
    â”‚
    â”œâ”€â”€ ui/                             # UI test files (5+ files)
    â”‚   â”œâ”€â”€ test_dashboard_sources.html  # Dashboard sources test
    â”‚   â”œâ”€â”€ test_upload_button.html     # Upload button test
    â”‚   â”œâ”€â”€ VERIFY_MARKDOWN_WORKS.html  # Markdown verification test
    â”‚   â”œâ”€â”€ test_responsive_design.html # Responsive design test
    â”‚   â””â”€â”€ test_accessibility.html     # Accessibility test
    â”‚
    â”œâ”€â”€ regression/                     # Regression tests (10+ files)
    â”‚   â”œâ”€â”€ test_ticker_resolution.py   # Ticker resolution regression
    â”‚   â”œâ”€â”€ test_time_fixes.py          # Time parsing fixes regression
    â”‚   â”œâ”€â”€ test_parsing_regression.py  # Parsing regression tests
    â”‚   â”œâ”€â”€ test_accuracy_regression.py # Accuracy regression tests
    â”‚   â”œâ”€â”€ test_performance_regression.py # Performance regression tests
    â”‚   â””â”€â”€ (additional regression tests)
    â”‚
    â”œâ”€â”€ manual/                         # Manual test scripts (25+ files)
    â”‚   â”œâ”€â”€ test_100_percent_accuracy.py # 100% accuracy manual test
    â”‚   â”œâ”€â”€ test_100_percent_confidence.py # 100% confidence manual test
    â”‚   â”œâ”€â”€ test_100_prompts_accuracy.py # 100 prompts accuracy test
    â”‚   â”œâ”€â”€ test_accuracy_100_prompts.py # Accuracy 100 prompts test
    â”‚   â”œâ”€â”€ test_all_sp500_all_kpis.py  # All S&P 500 all KPIs test
    â”‚   â”œâ”€â”€ test_all_sp500_base_metrics.py # All S&P 500 base metrics test
    â”‚   â”œâ”€â”€ test_comprehensive_manual.py # Comprehensive manual tests
    â”‚   â”œâ”€â”€ test_edge_cases_manual.py   # Edge cases manual tests
    â”‚   â”œâ”€â”€ test_stress_manual.py       # Stress testing manual
    â”‚   â””â”€â”€ (additional manual tests)
    â”‚
    â”œâ”€â”€ performance/                    # Performance tests (10+ files)
    â”‚   â”œâ”€â”€ test_query_performance.py   # Query performance tests
    â”‚   â”œâ”€â”€ test_memory_usage.py        # Memory usage tests
    â”‚   â”œâ”€â”€ test_latency.py             # Latency tests
    â”‚   â”œâ”€â”€ test_throughput.py          # Throughput tests
    â”‚   â”œâ”€â”€ test_scalability.py         # Scalability tests
    â”‚   â””â”€â”€ (additional performance tests)
    â”‚
    â”œâ”€â”€ security/                       # Security tests (5+ files)
    â”‚   â”œâ”€â”€ test_input_validation.py    # Input validation tests
    â”‚   â”œâ”€â”€ test_sql_injection.py       # SQL injection tests
    â”‚   â”œâ”€â”€ test_xss_protection.py      # XSS protection tests
    â”‚   â””â”€â”€ (additional security tests)
    â”‚
    â””â”€â”€ fixtures/                       # Test fixtures and data (10+ files)
        â”œâ”€â”€ sample_data.json            # Sample test data
        â”œâ”€â”€ mock_responses.json         # Mock API responses
        â”œâ”€â”€ test_portfolios.csv         # Test portfolio data
        â”œâ”€â”€ test_companies.json         # Test company data
        â””â”€â”€ (additional test fixtures)
```
    â”‚   â”œâ”€â”€ test_fixed_accuracy.py      # Fixed accuracy test
    â”‚   â”œâ”€â”€ test_global_ticker_fix.py   # Global ticker fix test
    â”‚   â”œâ”€â”€ test_real_chatbot_accuracy.py # Real chatbot accuracy test
    â”‚   â”œâ”€â”€ test_show_failed_facts.py   # Show failed facts test
    â”‚   â”œâ”€â”€ test_stress_50_companies.py # Stress test 50 companies
    â”‚   â”œâ”€â”€ test_stress_all_metrics.py  # Stress test all metrics
    â”‚   â”œâ”€â”€ test_stress_edge_cases.py   # Stress test edge cases
    â”‚   â”œâ”€â”€ test_stress_performance.py  # Stress test performance
    â”‚   â”œâ”€â”€ test_verification_coverage.py # Verification coverage test
    â”‚   â””â”€â”€ test_verification_system.py # Verification system test
    â”‚
    â”œâ”€â”€ standalone/                     # Standalone ML forecast tests
    â”‚   â”œâ”€â”€ test_all_ml_forecast_prompts.py # All ML forecast prompts
    â”‚   â”œâ”€â”€ test_all_ml_patterns_comprehensive.py # All ML patterns comprehensive
    â”‚   â”œâ”€â”€ test_all_ml_prompts_comprehensive.py # All ML prompts comprehensive
    â”‚   â”œâ”€â”€ test_ml_batch.py            # ML batch test
    â”‚   â”œâ”€â”€ test_ml_debug.py            # ML debug test
    â”‚   â”œâ”€â”€ test_ml_focused.py          # ML focused test
    â”‚   â”œâ”€â”€ test_ml_forecast_prompts.py # ML forecast prompts test
    â”‚   â”œâ”€â”€ test_ml_forecast_quality.py # ML forecast quality test
    â”‚   â”œâ”€â”€ test_ml_forecast_quick.py   # ML forecast quick test
    â”‚   â””â”€â”€ test_ml_incremental.py      # ML incremental test
    â”‚
    â”œâ”€â”€ outputs/                        # Test output files
    â”‚   â”œâ”€â”€ sp500_dashboard_test_results.txt # S&P 500 dashboard results
    â”‚   â”œâ”€â”€ sp500_test_output.txt       # S&P 500 test output
    â”‚   â”œâ”€â”€ ml_test_output.txt          # ML test output
    â”‚   â””â”€â”€ test_single_company_payload.json # Single company test payload
    â”‚
    â”œâ”€â”€ Parser & NLP Tests:
    â”œâ”€â”€ test_alias_resolution.py         # Alias resolution tests
    â”œâ”€â”€ test_time_grammar.py            # Time grammar tests
    â”œâ”€â”€ test_nl_parser.py               # Natural language parser tests
    â”œâ”€â”€ test_abbreviations.py           # Abbreviation tests
    â”œâ”€â”€ test_advanced_followups.py     # Advanced follow-up tests
    â”œâ”€â”€ test_company_groups.py          # Company group tests
    â”œâ”€â”€ test_comparative_language.py    # Comparative language tests
    â”œâ”€â”€ test_conditionals.py            # Conditional statement tests
    â”œâ”€â”€ test_enhanced_intents.py        # Enhanced intent tests
    â”œâ”€â”€ test_enhanced_metric_synonyms.py # Enhanced metric synonym tests
    â”œâ”€â”€ test_enhanced_question_patterns.py # Enhanced question pattern tests
    â”œâ”€â”€ test_followup_features_unit.py  # Follow-up feature unit tests
    â”œâ”€â”€ test_fuzzy_quantities.py        # Fuzzy quantity tests
    â”œâ”€â”€ test_metric_inference.py        # Metric inference tests
    â”œâ”€â”€ test_multi_intent.py            # Multi-intent tests
    â”œâ”€â”€ test_natural_filters.py         # Natural filter tests
    â”œâ”€â”€ test_negation_handling.py       # Negation handling tests
    â”œâ”€â”€ test_performance_benchmarks.py  # Performance benchmark tests
    â”œâ”€â”€ test_period_normalization.py   # Period normalization tests
    â”œâ”€â”€ test_pronoun_resolution.py      # Pronoun resolution tests
    â”œâ”€â”€ test_question_chaining.py      # Question chaining tests
    â”œâ”€â”€ test_sentiment.py               # Sentiment analysis tests
    â”œâ”€â”€ test_spelling_correction.py     # Spelling correction tests
    â”œâ”€â”€ test_temporal_relationships.py  # Temporal relationship tests
    â”œâ”€â”€ test_time_period_enhancement.py # Time period enhancement tests
    â”œâ”€â”€ test_trend_direction.py        # Trend direction tests
    â”‚
    â”œâ”€â”€ Portfolio Tests:
    â”œâ”€â”€ test_portfolio_detection_working.py # Portfolio detection tests
    â”œâ”€â”€ test_portfolio_patterns.py      # Portfolio pattern tests
    â”œâ”€â”€ test_portfolio_questions.py     # Portfolio question tests
    â”œâ”€â”€ test_portfolio_stress_test.py   # Portfolio stress test
    â”‚
    â”œâ”€â”€ ML Forecasting Tests:
    â”œâ”€â”€ test_all_forecast_prompts.py   # All forecast prompt tests
    â”œâ”€â”€ test_forecast_detection.py     # Forecast detection tests
    â”œâ”€â”€ test_forecast_prompts.py       # Forecast prompt tests
    â”œâ”€â”€ test_ml_context_debug.py       # ML context debug tests
    â”œâ”€â”€ test_ml_detailed_response.py   # ML detailed response tests
    â”‚
    â”œâ”€â”€ Other Tests:
    â”œâ”€â”€ test_terminal_bot.py            # Terminal bot tests
    â”œâ”€â”€ test_working_prompts.py         # Working prompt tests
    â”œâ”€â”€ test_api_direct.sh             # API direct test script
    â”œâ”€â”€ test_dashboard_sources.html     # Dashboard sources HTML test
    â”œâ”€â”€ test_integration_e2e.py        # Integration E2E tests
    â”œâ”€â”€ test_source_completeness.py    # Source completeness tests
    â”œâ”€â”€ test_chatbot_stress_test.py    # FinalyzeOS stress test
    â”œâ”€â”€ test_chatgpt_style.py          # ChatGPT-style test
    â”œâ”€â”€ portfolio_stress_test_results.json # Portfolio stress test results
    â”‚
    â”œâ”€â”€ cache/                          # Test cache (gitignored)
    â”œâ”€â”€ data/                           # Test data fixtures
    â””â”€â”€ outputs/                        # Test outputs (gitignored)
```

## âœ… Quality and Testing

- Run the suite: `pytest`
- Parser & alias focus: `pytest tests/test_alias_resolution.py tests/test_time_grammar.py tests/test_nl_parser.py`
- Target a single test: `pytest tests/test_cli_tables.py::test_table_command_formats_rows`
- Manual sanity: point LLM_PROVIDER=local to avoid burning API credits during smoke tests.
- Database reset: delete finanlyzeos_chatbot.sqlite3 and rerun ingestionâ€”migrations run automatically on startup.

CI isn't configured by default, but pytest -ra (preconfigured in pyproject.toml) surfaces skipped/xfail tests neatly. Consider adding ruff or black once your team standardises formatting.

## ðŸ”§ Troubleshooting

### âš ï¸ General Issues

- **"OpenAI API key not found"** â€“ set OPENAI_API_KEY, store it via keyring, or create ~/.config/finanlyzeos-chatbot/openai_api_key.
- **WinError 10048 when starting the server** â€“ another process is on the port. Run `Get-NetTCPConnection -LocalPort 8000` and terminate it, or start with `--port 8001`.
- **PostgreSQL auth failures** â€“ confirm SSL/network settings, then double-check POSTGRES_* vars; the DSN is logged at debug level when DATABASE_TYPE=postgresql is active.
- **Pytest cannot locate modules** â€“ run from the repo root so the pythonpath = ["src", "."] entry in pyproject.toml kicks in.

### ðŸ“¥ Data Ingestion Issues

#### âŒ "No data showing up in chatbot after ingestion"
**Cause:** Metrics need to be refreshed after data ingestion.
**Solution:**
```bash
python -c "from finanlyzeos_chatbot.config import load_settings; from finanlyzeos_chatbot.analytics_engine import AnalyticsEngine; AnalyticsEngine(load_settings()).refresh_metrics(force=True)"
```
The `fill_data_gaps.py` script does this automatically, but manual ingestion scripts may not.

#### "Yahoo Finance 429 errors during ingestion"
**Cause:** Yahoo Finance rate limits (too many requests too quickly).
**Solution:**
- The script automatically retries with exponential backoff (1s â†’ 2s â†’ 4s â†’ 8s)
- These are warnings, not errors - the process continues
- If persistent, lower `YAHOO_QUOTE_BATCH_SIZE` in your `.env` file:
  ```bash
  YAHOO_QUOTE_BATCH_SIZE=25  # Default is 50
  ```
- Alternative: Use `scripts/ingestion/load_prices_stooq.py` for market data

#### "SEC API returns 403 Forbidden"
**Cause:** Missing or invalid User-Agent header (SEC requires identification).
**Solution:** Set a descriptive User-Agent in your `.env`:
```bash
SEC_API_USER_AGENT="YourCompany/1.0 (your.email@example.com)"
```

#### "Some companies show 'Failed to ingest' messages"
**Cause:** Some tickers may not have SEC filings (delisted, private, or ticker changed).
**Examples from logs:** ALP, BRV, CTL, FIN (these are known issues)
**Solution:** This is expected behavior - the script handles failures gracefully and continues. Check the summary report in `fill_gaps_summary.json` for details.

#### "Ingestion seems slow or stuck"
**Cause:** SEC API rate limiting (10 requests/second limit enforced by script).
**What's normal:**
- 3-year ingestion: 5-7 minutes
- 5-year ingestion: 8-12 minutes
- 20-year ingestion: 25-35 minutes

**Progress indicators to watch:**
```
[10/48 - 20.8%] Processing: CMCSA, CME, CMG, CMI, CMS, CNC, CNP, COF, COO, COP
   âœ… Loaded 331 records (Total: 3,254)
```
If you see new batches completing, the script is working correctly.

#### "Database file not found"
**Cause:** Default database path may differ from your configuration.
**Solution:** Check your `.env` file for `DATABASE_PATH`:
```bash
DATABASE_PATH=./data/sqlite/finanlyzeos_chatbot.sqlite3
```
Or use the full path:
```bash
DATABASE_PATH=C:/Users/YOUR_USERNAME/Documents/GitHub/Project/finanlyzeos_chatbot.sqlite3
```

#### "ModuleNotFoundError: finanlyzeos_chatbot"
**Cause:** Package not installed in editable mode.
**Solution:**
```bash
pip install -e .
# Or set PYTHONPATH manually:
$env:PYTHONPATH = (Resolve-Path .\src).Path  # PowerShell
export PYTHONPATH=./src  # Bash
```

### Verifying Ingestion Success

After ingestion completes, verify your data:

```bash
# 1. Check total row counts
python -c "import sqlite3; conn = sqlite3.connect('C:/Users/YOUR_PATH/finanlyzeos_chatbot.sqlite3'); cursor = conn.cursor(); print(f'financial_facts: {cursor.execute(\"SELECT COUNT(*) FROM financial_facts\").fetchone()[0]:,}'); print(f'metric_snapshots: {cursor.execute(\"SELECT COUNT(*) FROM metric_snapshots\").fetchone()[0]:,}'); conn.close()"

# 2. Check year coverage
python -c "import sqlite3; conn = sqlite3.connect('C:/Users/YOUR_PATH/finanlyzeos_chatbot.sqlite3'); cursor = conn.cursor(); cursor.execute('SELECT MIN(fiscal_year), MAX(fiscal_year), COUNT(DISTINCT ticker) FROM financial_facts'); print('Years: %s-%s | Companies: %s' % cursor.fetchone()); conn.close()"

# 3. Test a specific company
python run_chatbot.py
# Then type: metrics AAPL
```

**Expected results after successful 3-year ingestion:**
- financial_facts: ~30,000-35,000 rows
- metric_snapshots: ~250,000-350,000 rows
- Companies: 475 tickers
- Years: 2022-2024

**Expected results after successful 20-year ingestion:**
- financial_facts: ~80,000-120,000 rows
- metric_snapshots: ~500,000-700,000 rows
- Companies: 475 tickers
- Years: 2005-2025 (varies by company IPO date)

## ðŸ“š Further Reading

- ðŸ“– [`docs/orchestration_playbook.md`](docs/orchestration_playbook.md) â€“ Three ingestion/orchestration patterns (local queue, serverless fetchers, batch jobs) and how to wire them into FinalyzeOSChatbot
- ðŸ’» **Inline Module Documentation** - Comprehensive docs across `src/finanlyzeos_chatbot/` describe invariants, data contracts, and extension hooks
- ðŸ”§ **Versioning Best Practices** - Consider versioning your `.env` templates and deployment runbooks alongside these docs as the project evolves

<div align="center">

## ðŸŽ‰ Happy Building!

**FinalyzeOS** - Institutional-grade analytics tooling for finance teams

*Conversational interface â€¢ Reproducible metrics â€¢ Transparent data lineage*

</div>
