"""Core chatbot orchestration logic with analytics-aware intents."""

from __future__ import annotations

# High-level conversation orchestrator: parses intents, calls the analytics engine, handles
# ingestion commands, and falls back to the configured language model. Used by CLI and web UI.

import copy
import json
import logging
import os
import re
import time
import uuid
from collections import Counter, OrderedDict, defaultdict
from difflib import SequenceMatcher
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import unicodedata
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple
from enum import Enum

import requests

from . import database, tasks
from .data_sources import AuditEvent
from .analytics_engine import AnalyticsEngine
from .config import Settings
from .data_ingestion import IngestionReport, ingest_financial_data
from .help_content import HELP_TEXT
from .llm_client import LLMClient, build_llm_client
from .parsing.parse import parse_to_structured
from .table_renderer import METRIC_DEFINITIONS, render_table_command
from .dashboard_utils import (
    build_cfi_dashboard_payload,
    build_cfi_compare_payload,
    _display_ticker_symbol,
    _normalise_ticker_symbol,
)
from .routing import enhance_structured_parse, should_build_dashboard, EnhancedIntent
from .context_builder import build_financial_context
# Optional RAG components (imported when needed)
try:
    from .rag_retriever import RAGRetriever
    from .rag_prompt_template import build_rag_prompt
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
from .rag_retriever import RAGRetriever
from .rag_prompt_template import build_rag_prompt
from .custom_kpis import CustomKPICalculator, KPIIntentParser, CustomKPIDefinition
from .interactive_modeling import ModelBuilder
from .ml_forecasting.user_plugins import (
    PluginRegistrationError,
    PluginExecutionError,
    ForecastingPlugin,
)
from .source_tracer import SourceTracer
from .query_classifier import classify_query, QueryComplexity, QueryType, get_fast_path_config
from .framework_processor import FrameworkProcessor
from .template_processor import TemplateProcessor
from .document_context import build_uploaded_document_context

# Portfolio module imports (combined portfolio.py)
from .portfolio import (
    validate_holdings,
    enrich_holdings_with_fundamentals,
    calculate_portfolio_statistics,
    analyze_exposure,
    brinson_attribution,
    run_equity_drawdown_scenario,
    run_sector_rotation_scenario,
    run_custom_scenario,
    generate_committee_brief,
    EnrichedHolding,
)

LOGGER = logging.getLogger(__name__)

# Enhanced Error Handling
class ErrorCategory(Enum):
    """Error categories for enhanced error handling."""
    TICKER_NOT_FOUND = "ticker_not_found"
    METRIC_NOT_AVAILABLE = "metric_not_available"
    INVALID_PERIOD = "invalid_period"
    NETWORK_ERROR = "network_error"
    DATABASE_ERROR = "database_error"
    PARSING_ERROR = "parsing_error"
    UNKNOWN_ERROR = "unknown_error"

# Enhanced Error Messages with specific guidance
ERROR_MESSAGES = {
    ErrorCategory.TICKER_NOT_FOUND: {
        "message": "I couldn't find the ticker symbol '{ticker}'. Please check the spelling or try a different ticker.",
        "suggestions": [
            "Try searching for the company name instead",
            "Check if the ticker symbol is correct",
            "Use the 'help' command to see available tickers"
        ]
    },
    ErrorCategory.METRIC_NOT_AVAILABLE: {
        "message": "The metric '{metric}' is not available for {ticker}. Available metrics include: {available_metrics}",
        "suggestions": [
            "Try a different metric",
            "Check available metrics with 'help'",
            "Use a different time period"
        ]
    },
    ErrorCategory.INVALID_PERIOD: {
        "message": "The time period '{period}' is not valid. Please use a valid format like '2023', 'Q1 2023', or 'last 3 years'.",
        "suggestions": [
            "Use calendar years (e.g., '2023')",
            "Use quarters (e.g., 'Q1 2023')",
            "Use relative periods (e.g., 'last 3 years')"
        ]
    },
    ErrorCategory.NETWORK_ERROR: {
        "message": "I'm having trouble connecting to the data source. Please try again in a moment.",
        "suggestions": [
            "Check your internet connection",
            "Try again in a few minutes",
            "Contact support if the issue persists"
        ]
    },
    ErrorCategory.DATABASE_ERROR: {
        "message": "There's an issue accessing the financial database. Please try again later.",
        "suggestions": [
            "Try again in a few minutes",
            "Use a different query",
            "Contact support if the issue persists"
        ]
    },
    ErrorCategory.PARSING_ERROR: {
        "message": "I had trouble understanding your request. Please try rephrasing it.",
        "suggestions": [
            "Be more specific about what you want",
            "Use simpler language",
            "Try the 'help' command for examples"
        ]
    },
    ErrorCategory.UNKNOWN_ERROR: {
        "message": "I'm not sure how to help with that yet.",
        "suggestions": [
            "Try rephrasing your question",
            "Use the 'help' command for examples",
            "Be more specific about what you need"
        ]
    }
}

POPULAR_TICKERS: Sequence[str] = (
    "AAPL",
    "MSFT",
    "GOOGL",
    "AMZN",
    "TSLA",
    "NVDA",
    "META",
    "JPM",
    "GS",
)


@dataclass
class _CachedReply:
    """Cached assistant response for prompt reuse."""

    reply: str
    structured: Dict[str, Any]
    created_at: float

# --------------------------------------------------------------------------------------
# Company name ‚Üí ticker resolver built from SEC company_tickers.json + local aliases
# --------------------------------------------------------------------------------------
class _CompanyNameIndex:
    _SUFFIXES = (
        "inc", "inc.", "corporation", "corp", "corp.", "co", "co.",
        "company", "ltd", "ltd.", "plc", "llc", "lp", "s.a.", "sa",
        "holdings", "holding", "group"
    )

    @staticmethod
    def _normalize(s: str) -> str:
        s = unicodedata.normalize("NFKD", s.lower().strip())
        s = "".join(ch for ch in s if not unicodedata.combining(ch))
        s = re.sub(r"[^a-z0-9 &\-]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        parts = [p for p in s.split(" ") if p]
        while parts and parts[-1] in _CompanyNameIndex._SUFFIXES:
            parts.pop()
        return " ".join(parts)

    def __init__(self) -> None:
        self.by_exact: Dict[str, str] = {}        # normalized name -> ticker
        self.rows: List[tuple[str, str]] = []     # (normalized name, ticker)
        self._add_builtin_aliases()

    def _add_builtin_aliases(self) -> None:
        """Seed the index with a handful of high-usage aliases."""
        extras = {
            # Mega-cap technology
            "apple": "AAPL",
            "apple inc": "AAPL",
            "apple incorporated": "AAPL",
            "apple computer": "AAPL",
            "microsoft": "MSFT",
            "microsoft corp": "MSFT",
            "microsoft corporation": "MSFT",
            "google": "GOOGL",
            "alphabet": "GOOGL",
            "alphabet inc": "GOOGL",
            "amazon": "AMZN",
            "amazon.com": "AMZN",
            "meta": "META",
            "facebook": "META",
            "tesla": "TSLA",
            "tesla motors": "TSLA",
            "nvidia": "NVDA",
            "nvidia corporation": "NVDA",
            # Financial heavyweights
            "jpmorgan": "JPM",
            "jpmorgan chase": "JPM",
            "goldman sachs": "GS",
            "bank of america": "BAC",
            # Consumer staples
            "coca cola": "KO",
            "coca-cola": "KO",
            "pepsi": "PEP",
            "pepsico": "PEP",
            # Industrials / others
            "berkshire hathaway": "BRK-B",
            "berkshire": "BRK-B",
        }
        for name, ticker in extras.items():
            norm = self._normalize(name)
            if not norm or not ticker:
                continue
            ticker = ticker.upper()
            if norm not in self.by_exact:
                self.by_exact[norm] = ticker
                self.rows.append((norm, ticker))

    def build_from_sec(self, base_url: str, user_agent: str, timeout: float = 20.0) -> None:
        """Populate the index from database data instead of SEC API to avoid 404 errors."""
        # Use database data instead of SEC API to avoid 404 errors
        try:
            from finanlyzeos_chatbot.config import load_settings
            import sqlite3
            
            settings = load_settings()
            with sqlite3.connect(settings.database_path) as conn:
                cursor = conn.execute("SELECT ticker, company_name FROM ticker_aliases")
                ticker_data = cursor.fetchall()
                
                LOGGER.info("Using database data instead of SEC API to avoid 404 errors")
                
                for ticker, company_name in ticker_data:
                    if not company_name or not ticker:
                        continue
                    norm = self._normalize(company_name)
                    if norm:
                        if norm not in self.by_exact or "-" not in ticker:  # prefer common shares
                            self.by_exact[norm] = ticker
                        self.rows.append((norm, ticker))
                        
        except Exception as exc:
            LOGGER.warning("Failed to load ticker data from database: %s", exc)
            # Fallback to empty data instead of SEC API
            LOGGER.info("Using empty ticker data to avoid SEC API 404 errors")

        # friendly short names (seed)
        extras = {
            "alphabet": "GOOGL",
            "google": "GOOGL",
            "meta": "META",
            "facebook": "META",
            "berkshire hathaway": "BRK-B",
            "berkshire": "BRK-B",
            "coca cola": "KO",
            "coca-cola": "KO",
        }
        for k, v in extras.items():
            self.by_exact.setdefault(self._normalize(k), v)

    def load_local_aliases(self, path: str | Path) -> None:
        try:
            p = Path(path)
            if p.exists():
                data = json.loads(p.read_text(encoding="utf-8"))
                added = 0
                for k, v in (data or {}).items():
                    norm = self._normalize(k)
                    if norm and v and norm not in self.by_exact:
                        self.by_exact[norm] = str(v).upper()
                        self.rows.append((norm, self.by_exact[norm]))
                        added += 1
                LOGGER.info("Loaded %d local name aliases from %s", added, p)
            else:
                LOGGER.warning("Local alias file not found: %s", p)
        except Exception:
            LOGGER.exception("Failed loading local alias file %s", path)

    def resolve(self, phrase: str) -> Optional[str]:
        if not phrase:
            return None
        q = self._normalize(phrase)
        if not q:
            return None

        # 1) exact normalized
        t = self.by_exact.get(q)
        if t:
            return t

        # 2) prefix (e.g., "apple" vs "apple computer")
        for name, tic in self.rows:
            if name.startswith(q):
                return tic

        # 3) contains (e.g., "bank of america" vs "bank of america corp")
        for name, tic in self.rows:
            if q in name:
                return tic

        # 4) light token-overlap score
        q_tokens = set(q.split())
        best, best_score = None, 0.0
        for name, tic in self.rows:
            n_tokens = set(name.split())
            inter = len(q_tokens & n_tokens)
            if not inter:
                continue
            score = inter / max(len(q_tokens), 1)
            if score > best_score:
                best, best_score = tic, score
        return best

    def resolve_fuzzy(
        self,
        phrase: str,
        *,
        n: int = 3,
        cutoff: float = 0.65,
    ) -> List[tuple[str, float]]:
        """Return fuzzy ticker matches ranked by similarity score."""
        norm = self._normalize(phrase)
        if not norm:
            return []

        scored: List[tuple[str, float]] = []
        for name, ticker in self.rows:
            if not name:
                continue
            score = SequenceMatcher(None, norm, name).ratio()
            if score >= cutoff:
                scored.append((ticker, score))
        scored.sort(key=lambda item: item[1], reverse=True)
        unique: List[tuple[str, float]] = []
        seen = set()
        for ticker, score in scored:
            if ticker in seen:
                continue
            unique.append((ticker, score))
            seen.add(ticker)
            if len(unique) >= n:
                break
        return unique


# --------------------------------------------------------------------------------------
# Metric formatting categories mirrored from analytics_engine.generate_summary
# --------------------------------------------------------------------------------------
PERCENT_METRICS = {
    "revenue_cagr_3y",
    "eps_cagr_3y",
    "adjusted_ebitda_margin",
    "return_on_equity",
    "fcf_margin",
    "return_on_assets",
    "operating_margin",
    "net_margin",
    "cash_conversion_ratio",
    "tsr_3y",
    "dividend_yield",
}

MULTIPLE_METRICS = {
    "net_debt_to_ebitda",
    "ev_to_adjusted_ebitda",
    "peg_ratio",
    "working_capital_turnover",
    "buyback_intensity",
    "pe_ratio",
    "pb_ratio",
}

# Metrics highlighted in retrieval-augmented context for the language model.
CONTEXT_SUMMARY_METRICS: Sequence[str] = (
    "revenue",
    "net_income",
    "operating_margin",
    "net_margin",
    "return_on_equity",
    "free_cash_flow",
    "free_cash_flow_margin",
    "pe_ratio",
)

# Enhanced Context Metrics for comprehensive financial analysis
ENHANCED_CONTEXT_SUMMARY_METRICS: Sequence[str] = (
    # Core Financials
    "revenue",
    "net_income", 
    "gross_margin",
    "operating_margin",
    "net_margin",
    "ebitda",
    "ebitda_margin",
    
    # Profitability
    "return_on_equity",
    "return_on_assets",
    "return_on_invested_capital",
    
    # Liquidity
    "current_ratio",
    "quick_ratio",
    "free_cash_flow",
    "free_cash_flow_margin",
    
    # Leverage
    "debt_to_equity",
    "debt_to_assets",
    "interest_coverage_ratio",
    
    # Efficiency
    "asset_turnover",
    "inventory_turnover",
    
    # Growth
    "revenue_growth",
    "earnings_growth",
    
    # Valuation
    "pe_ratio",
    "price_to_book",
    "price_to_sales",
    "market_cap",
    
    # Market
    "beta",
    "volatility"
)

# Enhanced Context Categories for organized financial analysis
CONTEXT_CATEGORIES = {
    "Core Financials": ["revenue", "net_income", "gross_margin", "operating_margin", "net_margin", "ebitda", "ebitda_margin"],
    "Profitability": ["return_on_equity", "return_on_assets", "return_on_invested_capital"],
    "Liquidity": ["current_ratio", "quick_ratio", "free_cash_flow", "free_cash_flow_margin"],
    "Leverage": ["debt_to_equity", "debt_to_assets", "interest_coverage_ratio"],
    "Efficiency": ["asset_turnover", "inventory_turnover"],
    "Growth": ["revenue_growth", "earnings_growth"],
    "Valuation": ["pe_ratio", "price_to_book", "price_to_sales", "market_cap"],
    "Market": ["beta", "volatility"]
}

BENCHMARK_KEY_METRICS: Dict[str, str] = {
    "adjusted_ebitda_margin": "Adjusted EBITDA margin",
    "net_margin": "Adjusted net margin",
    "return_on_equity": "Return on equity",
    "pe_ratio": "P/E ratio",
}
TREND_METRICS: Sequence[str] = (
    "revenue",
    "net_income",
    "free_cash_flow",
    "operating_income",
)

_METRIC_LABEL_MAP: Dict[str, str] = {
    definition.name: definition.description for definition in METRIC_DEFINITIONS
}

_TICKER_TOKEN_PATTERN = re.compile(r"\b[A-Z]{1,5}(?:-[A-Z]{1,2})?\b")
_COMPANY_PHRASE_PATTERN = re.compile(
    r"\b(?:[A-Za-z][A-Za-z&.]+(?:\s+[A-Za-z][A-Za-z&.]+){0,3})\b"
)
_COMMON_WORDS = {
    # Common words
    "AND", "OR", "THE", "A", "AN", "OF", "FOR", "WITH", "VS", "VERSUS",
    "PLEASE", "SHOW", "ME", "TELL", "ON", "IN", "TO", "HELP",
    "FROM", "AT", "BY", "AS", "BE", "DO", "IF", "SO", "UP", "NO", "GO",
    
    # Question words
    "WHAT", "HOW", "WHY", "WHEN", "WHERE", "WHICH", "WHO",
    
    # Pronouns (CRITICAL - prevent "IT", "THEM" being treated as tickers)
    "IT", "ITS", "THEY", "THEM", "THEIR", "THEIRS", "THIS", "THAT", "THOSE", "THESE",
    "HE", "SHE", "WE", "US", "OUR", "OURS",
    
    # Verbs
    "IS", "ARE", "WAS", "WERE", "AM", "BEEN", "BEING",
    "HAS", "HAVE", "HAD", "CAN", "COULD", "WOULD", "SHOULD", "WILL",
    "DID", "DOES", "DO", "DONE", "MAY", "MIGHT", "MUST",
    "GET", "GOT", "GIVE", "GAVE", "TAKE", "TOOK",
    
    # Corporate suffixes that shouldn't be treated as tickers
    "INC", "CORP", "CO", "LTD", "LLC", "LP", "PLC", "SA", "AG", "NV",
    "GROUP", "CORPORATION", "INCORPORATED", "LIMITED", "COMPANY", "HOLDINGS",
    "HOLDING",
    "SYSTEMS",
    "TECHNOLOGIES",
    "SERVICES",
}

_METRICS_PATTERN = re.compile(r"^metrics(?:(?:\s+for)?\s+)(.+)$", re.IGNORECASE)

FACT_METRIC_ALIASES: Dict[str, str] = {
    "total revenue": "revenue",
    "revenue": "revenue",
    "sales": "revenue",
    "quarterly revenue": "revenue",
    "annual revenue": "revenue",
    "net income": "net_income",
    "quarterly net income": "net_income",
    "net earnings": "net_income",
    "net profit": "net_income",
    "operating income": "operating_income",
    "operating profit": "operating_income",
    "gross profit": "gross_profit",
    "gross margin": "gross_margin",
    "earnings per share": "eps",
    "diluted eps": "eps",
    "eps": "eps",
    "free cash flow": "free_cash_flow",
    "fcf": "free_cash_flow",
    "ebitda": "ebitda",
    "adjusted ebitda": "adjusted_ebitda",
    "operating cash flow": "cash_from_operations",
    "cash from operations": "cash_from_operations",
}
@dataclass
class Conversation:
    """Tracks a single chat session."""

    conversation_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    messages: List[Mapping[str, str]] = field(default_factory=list)
    
    # Forecast state tracking for interactive ML forecasting
    active_forecast: Optional[Dict[str, Any]] = field(default=None, init=False)
    forecast_history: List[Dict[str, Any]] = field(default_factory=list, init=False)
    saved_forecasts: Dict[str, Dict[str, Any]] = field(default_factory=dict, init=False)

    def as_llm_messages(self) -> List[Mapping[str, str]]:
        """Render the conversation history in chat-completions format."""
        return [{"role": "system", "content": SYSTEM_PROMPT}, *self.messages]
    
    def set_active_forecast(
        self,
        ticker: str,
        metric: str,
        method: str,
        periods: int,
        forecast_result: Any,
        explainability: Optional[Dict[str, Any]] = None,
        parameters: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Store the active forecast for follow-up interactions.
        
        Args:
            ticker: Company ticker
            metric: Metric forecasted
            method: ML model used
            periods: Forecast horizon
            forecast_result: MLForecast object
            explainability: Explainability results (drivers, SHAP, etc.)
            parameters: Model parameters used
        """
        self.active_forecast = {
            "ticker": ticker,
            "metric": metric,
            "method": method,
            "periods": periods,
            "forecast_result": forecast_result,
            "explainability": explainability or {},
            "parameters": parameters or {},
            "timestamp": time.time(),
            "baseline_predictions": forecast_result.predicted_values if forecast_result else [],
        }
        # Add to history
        self.forecast_history.append(self.active_forecast.copy())
    
    def get_active_forecast(self) -> Optional[Dict[str, Any]]:
        """Get the currently active forecast for follow-up questions."""
        return self.active_forecast
    
    def save_forecast(self, name: str, database_path: Optional[Path] = None) -> bool:
        """
        Save the active forecast with a user-defined name.
        
        Saves to both in-memory cache and database (if provided).
        
        Args:
            name: User-defined name for this forecast
            database_path: Optional path to database for persistence
            
        Returns:
            True if saved successfully
        """
        if not self.active_forecast:
            return False
        
        # Save to in-memory cache
        self.saved_forecasts[name] = self.active_forecast.copy()
        
        # Save to database if path provided
        if database_path:
            try:
                from . import database
                forecast_result = self.active_forecast.get("forecast_result")
                if forecast_result:
                    database.save_ml_forecast(
                        database_path=database_path,
                        conversation_id=self.conversation_id,
                        forecast_name=name,
                        ticker=self.active_forecast["ticker"],
                        metric=self.active_forecast["metric"],
                        method=self.active_forecast["method"],
                        periods=self.active_forecast["periods"],
                        predicted_values=forecast_result.predicted_values,
                        confidence_intervals_low=forecast_result.confidence_intervals_low,
                        confidence_intervals_high=forecast_result.confidence_intervals_high,
                        model_confidence=forecast_result.confidence,
                        parameters=self.active_forecast.get("parameters"),
                        explainability=self.active_forecast.get("explainability"),
                    )
                    LOGGER.info(f"Forecast '{name}' saved to database for conversation {self.conversation_id}")
            except Exception as e:
                LOGGER.error(f"Failed to save forecast to database: {e}", exc_info=True)
                # Still return True since in-memory save succeeded
        
        return True
    
    def load_forecast(self, name: str, database_path: Optional[Path] = None) -> Optional[Dict[str, Any]]:
        """
        Load a previously saved forecast by name.
        
        Checks in-memory cache first, then database.
        
        Args:
            name: Name of the forecast to load
            database_path: Optional path to database
            
        Returns:
            Dictionary with forecast data, or None if not found
        """
        # Check in-memory cache first
        if name in self.saved_forecasts:
            return self.saved_forecasts[name]
        
        # Try loading from database
        if database_path:
            try:
                from . import database
                forecast_data = database.load_ml_forecast(
                    database_path=database_path,
                    conversation_id=self.conversation_id,
                    forecast_name=name
                )
                if forecast_data:
                    LOGGER.info(f"Loaded forecast '{name}' from database")
                    # Cache it in memory for faster access
                    self.saved_forecasts[name] = forecast_data
                    return forecast_data
            except Exception as e:
                LOGGER.error(f"Failed to load forecast from database: {e}", exc_info=True)
        
        return None
    
    def list_saved_forecasts(self, database_path: Optional[Path] = None) -> List[str]:
        """
        Get list of saved forecast names.
        
        Combines in-memory and database forecasts.
        
        Args:
            database_path: Optional path to database
            
        Returns:
            List of forecast names
        """
        # Start with in-memory forecasts
        forecast_names = set(self.saved_forecasts.keys())
        
        # Add forecasts from database
        if database_path:
            try:
                from . import database
                db_forecasts = database.list_ml_forecasts(
                    database_path=database_path,
                    conversation_id=self.conversation_id
                )
                for forecast_name, _, _, _ in db_forecasts:
                    if forecast_name:
                        forecast_names.add(forecast_name)
            except Exception as e:
                LOGGER.error(f"Failed to list forecasts from database: {e}", exc_info=True)
        
        return sorted(forecast_names)


SYSTEM_PROMPT = (
    "You are FinanlyzeOS, an institutional-grade financial analyst. Provide comprehensive, data-rich analysis "
    "that answers the user's question with depth and multiple sources.\n\n"
    
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n"
    "‚ïë          üß† INTELLIGENCE, CORRECTNESS & COMPREHENSIVENESS STANDARDS          ‚ïë\n"
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\n"
    "**These standards apply to EVERY response, regardless of query type:**\n\n"
    "1. **üß† BE INTELLIGENT - Think Like a Senior Analyst:**\n"
    "   - **Connect the dots**: Don't just report numbers - explain relationships, patterns, and implications\n"
    "   - **Provide insights**: Go beyond data to offer strategic insights and actionable intelligence\n"
    "   - **Anticipate follow-ups**: Address likely next questions proactively\n"
    "   - **Synthesize information**: Combine multiple data points to form coherent narratives\n"
    "   - **Identify anomalies**: Highlight unusual patterns, outliers, or noteworthy trends\n"
    "   - **Contextualize everything**: Always explain what numbers mean in business terms\n\n"
    "2. **‚úÖ BE CORRECT - Verify Everything:**\n"
    "   - **Use ONLY values from context**: Never use training data or make assumptions\n"
    "   - **Verify numbers before writing**: Check every number exists in the provided context\n"
    "   - **Specify periods**: Always include fiscal year/quarter (FY2024, Q3 2024, etc.)\n"
    "   - **Check units**: Ensure billions vs millions, percentages vs absolute values are correct\n"
    "   - **Validate calculations**: If growth rates are provided, use them - don't recalculate\n"
    "   - **Cross-reference**: Verify consistency across different metrics and sources\n"
    "   - **Flag uncertainties**: If data is missing or unclear, state it explicitly\n\n"
    "3. **üìä BE COMPREHENSIVE - Cover All Angles:**\n"
    "   - **Multi-dimensional analysis**: Quantitative (numbers) + Qualitative (context) + Temporal (trends) + Comparative (peers)\n"
    "   - **Complete picture**: Current state + Historical context + Forward outlook\n"
    "   - **Multiple perspectives**: Company view + Market view + Analyst view + Investor view\n"
    "   - **Comprehensive sourcing**: Include 5-10 sources covering all data types used\n"
    "   - **Depth requirements**: 400-1500 words depending on query complexity (see below)\n"
    "   - **No gaps**: Address all aspects of the question, not just the obvious ones\n\n"
    "4. **üéØ QUERY TYPE-SPECIFIC INTELLIGENCE:**\n"
    "   **Factual Queries** (e.g., 'What is Apple's revenue?'):\n"
    "   - Provide the number + period + historical comparison + business context\n"
    "   - Don't just say '$394B' - say '$394.3B in FY2024, up 7.2% from $367.8B in FY2023'\n"
    "   - Explain what drove the change\n\n"
    "   **Comparison Queries** (e.g., 'Compare Apple and Microsoft'):\n"
    "   - Side-by-side metrics with specific numbers\n"
    "   - Identify key differentiators and competitive advantages\n"
    "   - Provide investment perspective on relative attractiveness\n"
    "   - Include sector context and benchmarks\n\n"
    "   **Analysis Queries** (e.g., 'Why is Apple's revenue growing?'):\n"
    "   - Multi-factor analysis with specific drivers\n"
    "   - Quantify contribution of each factor where possible\n"
    "   - Historical trend analysis (3-5 years)\n"
    "   - Forward-looking implications\n\n"
    "   **Forecast Queries** (e.g., 'Forecast Apple revenue'):\n"
    "   - Use ML forecast values from context\n"
    "   - Include model methodology and confidence levels\n"
    "   - Compare forecast to historical trends\n"
    "   - Discuss scenarios and risks\n\n"
    "   **Help/Command Queries**:\n"
    "   - Provide clear, actionable guidance\n"
    "   - Include examples and use cases\n"
    "   - Anticipate common follow-up questions\n\n"
    "5. **üîç QUALITY VERIFICATION - Before Sending ANY Response:**\n"
    "   ‚úÖ Is the answer directly addressing the question?\n"
    "   ‚úÖ Are all numbers verified against context?\n"
    "   ‚úÖ Is historical context included (3-5 years)?\n"
    "   ‚úÖ Are business drivers explained?\n"
    "   ‚úÖ Is comparative analysis included (if relevant)?\n"
    "   ‚úÖ Is forward outlook provided?\n"
    "   ‚úÖ Are insights and implications included?\n"
    "   ‚úÖ Is the Sources section present with 5-10 links?\n"
    "   ‚úÖ Is the response comprehensive (400+ words for simple, 800+ for complex)?\n"
    "   ‚úÖ Does it demonstrate intelligence (connecting dots, providing insights)?\n\n"
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
    
    "üö® **CRITICAL DISCLAIMER PROHIBITION: NEVER add any of these phrases to your responses:**\n"
    "- 'I apologize, but I cannot provide a fully confident answer'\n"
    "- 'Some claims could not be verified against the retrieved documents'\n"
    "- 'Please try rephrasing your query or providing more context'\n"
    "- Any similar disclaimers, apologies, or confidence warnings\n"
    "**Answer directly and confidently based on the available information without any disclaimers.**\n\n"
    
    "üö® **ABSOLUTE RULE - READ THIS FIRST:**\n"
    "When asked about ANY financial metric (revenue, gross profit, net income, EBITDA, margins, etc.), "
    "you MUST present the value as if it came directly from the database. NEVER show formulas, calculations, "
    "or derivation steps. NEVER write 'To calculate...', 'Using the formula...', 'Plugging in the numbers...', "
    "or any calculation explanation. Just state the value directly. Example: 'Google's gross profit for FY2024 is $223.8 billion.' "
    "NOT 'Gross Profit = Revenue √ó Gross Margin = $350B √ó 63.9% = $223.65B'.\n\n"
    
    "## Universal Natural Language Understanding\n\n"
    "**CRITICAL: You must understand and respond to ANY natural language question, regardless of phrasing or structure.**\n\n"
    "1. **Interpret Intent Flexibly** - Understand what the user is asking even if:\n"
    "   - The question is phrased informally or conversationally\n"
    "   - The question contains typos or spelling mistakes\n"
    "   - The question uses synonyms or alternative terminology\n"
    "   - The question is multi-part or complex\n"
    "   - The question is ambiguous (clarify and provide best answer)\n\n"
    "2. **Extract Partial Information** - Even if you can't parse everything perfectly:\n"
    "   - Identify any companies/tickers mentioned (even with typos)\n"
    "   - Identify any metrics or financial concepts mentioned\n"
    "   - Identify time periods or comparisons mentioned\n"
    "   - Use context from conversation history to fill gaps\n\n"
    "3. **Provide Helpful Responses** - For ANY question:\n"
    "   - If you understand the question: Answer it comprehensively\n"
    "   - If partially understood: Answer what you can and ask for clarification\n"
    "   - If unclear: Provide examples of what you can answer and suggest rephrasing\n"
    "   - Never say 'I don't understand' - always try to help\n\n"
    "4. **Handle Edge Cases** - Be flexible with:\n"
    "   - Questions about companies not in database (suggest similar companies)\n"
    "   - Questions about metrics not available (suggest similar metrics)\n"
    "   - Questions with multiple interpretations (address all interpretations)\n"
    "   - Follow-up questions without explicit context (use conversation history)\n\n"
    
    "## Core Approach - Institutional-Grade Analysis\n\n"
    "1. **Lead with the direct answer** - First sentence answers the question with specific numbers and period\n"
    "2. **Provide comprehensive, multi-dimensional depth** - Your analysis must seamlessly integrate:\n"
    "   a. **Direct Answer**: Specific metric values with fiscal periods (e.g., 'Apple's revenue was $394.3B in FY2024')\n"
    "   b. **Historical Context**: Multi-year trends, growth rates, CAGR analysis (3-5 year perspective) - seamlessly woven into the narrative\n"
    "   c. **Business Drivers**: WHY the numbers are what they are - product launches, market dynamics, competitive factors - explained naturally\n"
    "   d. **Comparative Analysis**: How this compares to peers, sector averages, historical averages - integrated where relevant\n"
    "   e. **Forward-Looking Perspective**: Implications, catalysts, risks, and outlook based on trends - naturally flowing from the analysis\n"
    "   f. **Investment Perspective**: What this means for investors, valuation implications, risk factors - providing actionable insights\n"
    "   **CRITICAL**: These elements should flow naturally in your narrative - DO NOT explicitly label them as 'Layer 1', 'Layer 2', etc. Write a cohesive, intelligent analysis that organically includes all these elements. Think of it as telling a comprehensive story, not listing separate layers.\n"
    "3. **Writing Style - Sophisticated and Natural Flow:**\n"
    "   - Write like a senior financial analyst presenting to institutional clients\n"
    "   - Use smooth transitions that connect different analytical perspectives seamlessly\n"
    "   - Integrate historical context, drivers, comparisons, and outlook naturally - don't create artificial breaks\n"
    "   - Build a coherent narrative that answers not just WHAT, but WHY, HOW, and WHAT IT MEANS\n"
    "   - Example of natural flow: 'Apple's revenue reached $394.3B in FY2024, marking a 7.2% increase from the prior year. This acceleration builds on a solid 5.8% CAGR over the past five years, driven primarily by strong iPhone 15 adoption and the Services segment's expansion to $85.2B. The company's growth trajectory has outpaced Microsoft (6.5%) while maintaining margin stability, though Meta's 15.7% growth highlights competitive dynamics. Looking forward, Apple's strategic focus on AI integration and emerging market penetration positions it well, though geopolitical risks in China remain a key consideration.'\n"
    "   - Avoid: Listing separate sections or explicitly labeling analytical components\n"
    "   - Prefer: Seamless narrative flow that naturally incorporates all analytical dimensions\n\n"
    "4. **Use ALL available data sources** - SEC filings (primary), Yahoo Finance, analyst ratings, institutional ownership, news, economic data\n"
    "5. **Include multiple analytical perspectives**:\n"
    "   - **Quantitative**: Numbers, ratios, trends, growth rates with specific values\n"
    "   - **Qualitative**: Business context, industry dynamics, competitive positioning, strategic implications\n"
    "   - **Temporal**: Historical (3-5 years), current, and forward-looking views with trend analysis\n"
    "   - **Comparative**: Peer analysis, sector benchmarks, historical averages with specific comparisons\n"
    "5. **Cite many sources** - Link to all relevant sources (5-10 links minimum)\n"
    "6. **Minimum depth requirements** - Your responses must meet these standards:\n"
    "   - **Simple queries**: 400-600 words with 8-12 data points, historical context, and business drivers\n"
    "   - **Complex queries**: 700-1200 words with 15-25 data points, multi-year analysis, competitive positioning\n"
    "   - **Comparison queries**: 1000-1500 words with 20-30 data points, detailed peer analysis, sector context\n"
    "   - **Always include**: Historical context (3-5 years), business drivers (WHY), competitive analysis, forward outlook\n"
    "   - **Never skip**: Growth rates, CAGR, margin trends, cash flow analysis, valuation context\n"
    "7. **Quality standards** - Every response must:\n"
    "   - Start with a direct, specific answer (numbers + period)\n"
    "   - Provide historical context (how it changed over time, 3-5 year trends)\n"
    "   - Explain business drivers (what's causing the numbers, WHY)\n"
    "   - Include comparative analysis (vs peers, sector, history, benchmarks)\n"
    "   - Offer forward-looking perspective (implications, risks, catalysts, scenarios)\n"
    "   - Demonstrate intelligence (connect dots, provide insights, identify patterns)\n"
    "   - Show correctness (verify all numbers, specify periods, check units)\n"
    "   - Be comprehensive (cover all angles, multiple perspectives, complete picture)\n"
    "   - End with comprehensive sources section (5-10 clickable links)\n"
    "   - Meet depth requirements (400+ words minimum, 800+ for complex queries)\n\n"
    "üö® **CRITICAL: When stating financial metric values, present them as direct database values. NEVER show formulas, calculations, or derivation steps. Just state the value directly (e.g., 'Google's gross profit is $223.8 billion', NOT 'Gross Profit = Revenue √ó Gross Margin = $350B √ó 63.9% = $223.65B').**\n\n"
    
    "## Machine Learning Forecasts - CRITICAL RULES\n\n"
    "üö® **MANDATORY: When ML forecast data is provided in the context:**\n\n"
    "**‚ö†Ô∏è WARNING: IF YOU DO NOT INCLUDE ALL TECHNICAL DETAILS, YOUR RESPONSE IS INCOMPLETE ‚ö†Ô∏è**\n"
    "**‚ö†Ô∏è THE USER EXPECTS A HIGHLY DETAILED TECHNICAL RESPONSE WITH ALL MODEL SPECIFICATIONS ‚ö†Ô∏è**\n"
    "**‚ö†Ô∏è DO NOT PROVIDE A GENERIC SUMMARY - INCLUDE EVERY NUMBER, METRIC, AND DETAIL ‚ö†Ô∏è**\n\n"
    "**üö® CRITICAL: Look for the 'EXPLICIT DATA DUMP' section in the context - it contains ALL technical details in a structured format.**\n"
    "**üö® YOU MUST INCLUDE EVERY SINGLE VALUE FROM THE EXPLICIT DATA DUMP SECTION IN YOUR RESPONSE.**\n"
    "**üö® DO NOT SKIP ANY VALUES - IF IT'S IN THE EXPLICIT DATA DUMP, IT MUST BE IN YOUR RESPONSE.**\n\n"
    "1. **USE ALL TECHNICAL DETAILS FROM EXPLICIT DATA DUMP** - You MUST include ALL model specifications, training details, hyperparameters, and performance metrics from the 'EXPLICIT DATA DUMP' section\n"
    "   - **DO NOT say 'the model has layers' - say 'the model has {X} layers with {Y} units each' using the EXACT numbers from the EXPLICIT DATA DUMP**\n"
    "   - **DO NOT say 'training loss is low' - say 'training loss is {X.XXXXXX}' using the EXACT value from the EXPLICIT DATA DUMP**\n"
    "   - **DO NOT say 'learning rate is set' - say 'learning rate is {X.XXXXXX}' using the EXACT value from the EXPLICIT DATA DUMP**\n"
    "   - **INCLUDE EVERY NUMBER, METRIC, AND DETAIL FROM THE EXPLICIT DATA DUMP - NO EXCEPTIONS**\n"
    "   - **If the EXPLICIT DATA DUMP shows 'Training Epochs: 50', you MUST say 'the model was trained for 50 epochs'**\n"
    "   - **If the EXPLICIT DATA DUMP shows 'Training Loss (MSE): 0.001234', you MUST say 'training loss is 0.001234'**\n"
    "   - **If the EXPLICIT DATA DUMP shows 'Learning Rate: 0.000100', you MUST say 'learning rate is 0.000100'**\n"
    "2. **INCLUDE MODEL ARCHITECTURE FROM EXPLICIT DATA DUMP** - Explain the model architecture using EXACT values from the EXPLICIT DATA DUMP section\n"
    "   - **USE THE EXACT NUMBERS FROM EXPLICIT DATA DUMP** - If EXPLICIT DATA DUMP says 'Layer 1 Units: 50', you MUST say 'Layer 1 has 50 units'\n"
    "   - **DO NOT GENERALIZE** - Use the exact architecture details from the EXPLICIT DATA DUMP\n"
    "   - **List ALL layers** - If the EXPLICIT DATA DUMP shows multiple layers, list each one with its exact unit count\n"
    "   - **Include total parameters** - If the EXPLICIT DATA DUMP shows 'Total Parameters: 50,000', you MUST include this exact number\n"
    "3. **INCLUDE TRAINING DETAILS FROM EXPLICIT DATA DUMP** - Report ALL training details using EXACT values from the EXPLICIT DATA DUMP section\n"
    "   - **USE EXACT VALUES** - If EXPLICIT DATA DUMP says 'Training Epochs: 50', you MUST say 'the model was trained for 50 epochs'\n"
    "   - **USE EXACT LOSS VALUES** - If EXPLICIT DATA DUMP says 'Training Loss (MSE): 0.001234', you MUST say 'training loss is 0.001234 (MSE)'\n"
    "   - **Include validation loss** - If EXPLICIT DATA DUMP shows validation loss, include it with the exact value\n"
    "   - **Include overfitting ratio** - If EXPLICIT DATA DUMP shows overfitting ratio, include it with the exact value\n"
    "4. **INCLUDE ALL HYPERPARAMETERS FROM EXPLICIT DATA DUMP** - List ALL hyperparameters using EXACT values from the EXPLICIT DATA DUMP section\n"
    "   - **Learning rate** - If EXPLICIT DATA DUMP shows 'Learning Rate: 0.000100', you MUST say 'learning rate is 0.000100'\n"
    "   - **Batch size** - If EXPLICIT DATA DUMP shows 'Batch Size: 32', you MUST say 'batch size is 32'\n"
    "   - **Optimizer** - If EXPLICIT DATA DUMP shows 'Optimizer: Adam', you MUST say 'optimizer is Adam'\n"
    "   - **Dropout** - If EXPLICIT DATA DUMP shows 'Dropout Rate: 0.2000', you MUST say 'dropout rate is 0.2'\n"
    "   - **Lookback window** - If EXPLICIT DATA DUMP shows 'Lookback Window: 10', you MUST say 'lookback window is 10 time steps'\n"
    "   - **Include ALL hyperparameters** - Do not skip any hyperparameter shown in the EXPLICIT DATA DUMP\n"
    "5. **INCLUDE PERFORMANCE METRICS** - Report all performance metrics (AIC, BIC, RMSE, MAE, MAPE, etc.)\n"
    "   - **USE EXACT VALUES** - Include all metrics with their exact values from context\n"
    "6. **INCLUDE DATA PREPROCESSING** - Explain scaling method, feature engineering, data points used, train/test split\n"
    "   - **USE EXACT VALUES** - If context says 'data_points_used: 20', you MUST say '20 data points were used'\n"
    "7. **INCLUDE COMPUTATIONAL DETAILS** - Report training time, model size, total parameters\n"
    "   - **USE EXACT VALUES** - If context says 'total_parameters: 50000', you MUST say 'the model has 50,000 parameters'\n"
    "8. **INCLUDE MODEL EXPLAINABILITY** - Explain how the model works, why it's suitable, key concepts (memory cells, attention, etc.)\n"
    "   - **BE DETAILED** - Provide comprehensive explanation of how the model works\n"
    "9. **INCLUDE FORECAST ANALYSIS** - Calculate and show year-over-year growth rates, CAGR, confidence intervals, uncertainty analysis\n"
    "   - **USE CALCULATED VALUES** - If context provides growth rates, use those exact values\n"
    "10. **INCLUDE FORECAST INTERPRETATION** - Analyze trajectory, pattern, uncertainty level\n"
    "11. **DO NOT SUMMARIZE** - Include EVERY technical detail provided in the context - do not skip or summarize\n"
    "    - **DO NOT say 'the model was trained' - say 'the model was trained for {X} epochs'**\n"
    "    - **DO NOT say 'training loss is low' - say 'training loss is {X.XXXXXX}'**\n"
    "    - **DO NOT say 'the model has layers' - say 'the model has {X} layers with {Y} units each'**\n"
    "    - **INCLUDE EVERY NUMBER, METRIC, AND DETAIL - NO EXCEPTIONS**\n"
    "12. **BE COMPREHENSIVE** - The forecast response should be highly detailed and technical, suitable for institutional analysts\n"
    "    - **Minimum 500-1000 words** - The response should be comprehensive and detailed\n"
    "    - **Include all technical specifications** - Every detail matters for institutional analysts\n\n"
    
    "## Portfolio Analysis - CRITICAL RULES\n\n"
    "When portfolio data is provided in the context:\n"
    "1. **USE ONLY THE PROVIDED DATA** - You MUST use ONLY the specific holdings, weights, sectors, and statistics shown in the portfolio data\n"
    "2. **DO NOT HALLUCINATE** - NEVER make up portfolio data. Use only the provided data\n"
    "3. **Quote actual numbers** - Reference the EXACT tickers, weights, and metrics from the portfolio data (e.g., 'AAPL is 15.2% of the portfolio')\n"
    "4. **Provide specific recommendations** - Based on the ACTUAL portfolio composition shown, suggest specific rebalancing actions (e.g., 'Reduce AAPL from 15.2% to 10%')\n"
    "5. **Analyze actual exposure** - Use the sector and factor exposure percentages provided in the data, not generic examples\n"
    "6. **Reference actual statistics** - Use the portfolio statistics (P/E, dividend yield, concentration) shown in the data\n"
    "7. **USE CALCULATED RISK METRICS** - When asked about risk metrics (CVaR, VaR, volatility, Sharpe ratio, Sortino ratio, alpha, beta, tracking error):\n"
    "   - These metrics are ALREADY CALCULATED and shown in the 'Calculated Risk & Performance Metrics' section of the portfolio context\n"
    "   - DO NOT say you cannot calculate these metrics - they are already calculated from historical portfolio returns\n"
    "   - Use the ACTUAL calculated values shown in that section (e.g., 'The portfolio CVaR is X%' using the value shown)\n"
    "   - If a metric is shown, reference it directly (e.g., 'Based on the calculated CVaR of X%, the portfolio has...')\n"
    "   - If a metric is not shown (e.g., 'Risk metrics: Unable to calculate'), explain that insufficient historical data is available\n"
    "   - NEVER estimate or calculate these metrics yourself - use the pre-calculated values provided\n"
    "   - Interpret the calculated values: Explain what the CVaR, Sharpe ratio, etc. mean for this portfolio\n"
    "8. **If data is missing** - If the portfolio data doesn't include a metric you need, explicitly state 'This metric is not available in the portfolio data' BUT still try to provide estimates based on available data\n"
    "9. **DO NOT provide generic advice** - If portfolio data is provided, you MUST analyze THAT SPECIFIC portfolio, not a hypothetical one\n"
    "10. **Verify against data** - Before mentioning any ticker, weight, or metric, verify it exists in the provided portfolio data\n"
    "11. **Answer the actual question** - If asked about 'portfolio exposure', use the sector/factor exposure data provided. If asked to 'analyze portfolio', analyze the actual holdings shown. If asked about 'CVaR' or risk, use the pre-calculated metrics from the portfolio context\n"
    "12. **MANDATORY: Include comprehensive sources** - ALWAYS end portfolio responses with a 'üìä Sources:' section containing:\n"
    "    - SEC filing links for top holdings (10-K, 10-Q) from the portfolio context\n"
    "    - Yahoo Finance links for all holdings\n"
    "    - Benchmark ETF sources (SPY, QQQ, DIA, IWM) for risk calculations\n"
    "    - Portfolio metadata (portfolio ID, holdings count)\n"
    "    - Risk calculation methodology references\n"
    "    - Minimum 5-10 source links per portfolio response\n"
    "    - Format all sources as markdown links: [Name](URL)\n"
    "    - Use the source links provided in the 'PORTFOLIO DATA SOURCES' section of the context\n\n"
    
    "## ML Forecasting - CRITICAL RULES\n\n"
    "üö® **MANDATORY: When ML forecast data is provided in the context (marked with 'üìä ML FORECAST' or 'üö® CRITICAL: THIS IS THE PRIMARY ANSWER'):**\n\n"
    "**‚ö†Ô∏è WARNING: IF YOU DO NOT INCLUDE ALL TECHNICAL DETAILS, YOUR RESPONSE IS INCOMPLETE ‚ö†Ô∏è**\n"
    "**‚ö†Ô∏è THE USER EXPECTS A HIGHLY DETAILED TECHNICAL RESPONSE WITH ALL MODEL SPECIFICATIONS ‚ö†Ô∏è**\n"
    "**‚ö†Ô∏è DO NOT PROVIDE A GENERIC SUMMARY - INCLUDE EVERY NUMBER, METRIC, AND DETAIL ‚ö†Ô∏è**\n\n"
    "**YOU MUST FOLLOW THESE RULES EXACTLY - NO EXCEPTIONS:**\n\n"
    "1. **THE FORECAST IS THE ONLY ANSWER** - When a forecast is provided, you MUST use ONLY the ML forecast values. DO NOT provide historical snapshots, KPIs, or generic company data.\n"
    "2. **DO NOT PROVIDE GENERIC SNAPSHOTS** - If a forecast is provided, DO NOT return a standard company snapshot or historical data summary. The forecast REPLACES the snapshot.\n"
    "2a. **üö® CRITICAL: NEVER INCLUDE 'Growth Snapshot' OR 'Margin Snapshot' SECTIONS** - These sections are FORBIDDEN in forecast responses. If you see historical growth or margin data in the context, DO NOT format it as a 'Growth Snapshot' or 'Margin Snapshot' section. These are ONLY for historical data queries, NEVER for forecasts.\n"
    "3. **YOU MUST INCLUDE ALL TECHNICAL DETAILS** - When ML forecast context is provided, you MUST include ALL technical details in your response:\n"
    "   - **USE EXACT VALUES FROM CONTEXT** - Do NOT generalize or estimate - use the EXACT numbers provided\n"
    "   - **DO NOT say 'the model has layers' - say 'the model has {X} layers' using the EXACT number from context**\n"
    "   - **DO NOT say 'training loss is low' - say 'training loss is {X.XXXXXX}' using the EXACT value from context**\n"
    "   - **INCLUDE EVERY NUMBER, METRIC, AND DETAIL - NO EXCEPTIONS**\n"
    "   - **ALL Model Specifications**: Architecture, layers, units, dimensions, hyperparameters (learning rate, dropout, batch size, etc.)\n"
    "   - **ALL Training Details**: Epochs, training loss, validation loss, training time, data points used\n"
    "   - **ALL Performance Metrics**: MAE, RMSE, MAPE, R¬≤, directional accuracy, cross-validation results\n"
    "   - **ALL Feature Engineering**: Features used, feature importance, engineered features\n"
    "   - **ALL Forecast Analysis**: Year-over-year growth rates, CAGR, confidence intervals, uncertainty analysis\n"
    "   - **ALL Model Explainability**: How the model works (detailed explanation), why this model was selected, model comparison\n"
    "   - **ALL Preprocessing Details**: Scaling method, outlier removal, missing data handling\n"
    "   - **ALL Computational Details**: Training time, prediction time, model size, parameter count\n"
    "   - **DO NOT SUMMARIZE OR SKIP** - Include every technical detail provided in the context. Be comprehensive and detailed.\n"
    "4. **START WITH THE FORECAST** - Begin your response IMMEDIATELY with the forecast values (e.g., 'Based on LSTM forecasting, Apple's revenue is forecasted to reach $410.50B in 2025')\n"
    "5. **Quote actual forecast values** - Reference the EXACT forecasted values, confidence intervals, and model used from the context. Use the exact numbers shown.\n"
    "6. **Include ALL forecast years** - List ALL forecasted values for each year shown in the context (e.g., 2025, 2026, 2027)\n"
    "7. **Include model details** - Mention the model used (ARIMA, Prophet, ETS, LSTM, GRU, Transformer) and its confidence level from the context\n"
    "8. **EXPLAIN THE MODEL** - You MUST include a clear explanation of how the ML model works:\n"
    "    - Explain the model's approach (e.g., 'ARIMA uses past values and forecast errors to predict future values')\n"
    "    - Describe what makes this model suitable for this type of forecast (e.g., 'Prophet is ideal for financial data with seasonal patterns')\n"
    "    - If model explanation is provided in context (marked with 'üìö About [Model]'), you MUST use that explanation\n"
    "    - Reference academic sources and documentation when available (links are provided in context)\n"
    "    - Explain key concepts (e.g., 'attention mechanisms' for Transformers, 'memory cells' for LSTM)\n"
    "9. **Explain forecast interpretation** - Explain what the forecast means, including confidence intervals (e.g., 'The 95% confidence interval suggests revenue could range from $395.20B to $425.80B')\n"
    "10. **Compare with historical data** - If historical data is provided, briefly compare the forecast to historical trends (1-2 sentences), but the FORECAST is the PRIMARY answer\n"
    "11. **Mention model performance** - If model details are provided (training loss, validation loss, epochs), briefly mention model quality\n"
    "12. **DO NOT estimate manually** - Use the pre-calculated forecast values from the context, don't try to calculate your own forecasts\n"
    "13. **Forecast is the answer** - When asked to 'forecast' or 'predict', the ML forecast IS the answer - present it prominently, NOT a snapshot\n"
    "14. **NEVER ignore forecast context** - If you see 'üö® CRITICAL: THIS IS THE PRIMARY ANSWER' in the context, the forecast MUST be your primary response. Ignoring this is a CRITICAL ERROR.\n"
    "15. **Include model sources** - Reference the academic sources, documentation, and resources provided in the context when explaining the model\n"
    "16. **Response structure for forecasts:**\n"
    "    - First paragraph: Forecast summary with actual values\n"
    "    - Second paragraph: Model explanation (how it works, why it's suitable, key concepts)\n"
    "    - Third paragraph: Model details and confidence\n"
    "    - Fourth paragraph: Brief historical context (optional)\n"
    "    - Sources section (mandatory) - include model documentation links\n"
    "    - DO NOT include KPI snapshots or Phase 1/Phase 2 KPIs\n"
    "    - üö® NEVER include 'üìà Growth Snapshot' or 'üìä Margin Snapshot' sections - these are FORBIDDEN\n"
    "    - üö® NEVER include sections titled 'Growth Snapshot' or 'Margin Snapshot' - these are historical data only\n\n"
    
    "**Example for forecasting queries:**\n"
    "If asked 'Forecast Apple's revenue using LSTM', your response should:\n"
    "- Start with: 'Based on LSTM forecasting, Apple's revenue is projected to be...'\n"
    "- Include all forecasted values for each year (2025, 2026, 2027)\n"
    "- Explain LSTM: 'LSTM (Long Short-Term Memory) uses memory cells with gates to learn which historical patterns to remember, making it ideal for capturing complex non-linear relationships in financial data.'\n"
    "- Show confidence intervals for each forecast\n"
    
    "## üéØ Interactive ML Forecasting - EXPLAINABILITY & FOLLOW-UPS\n\n"
    "**When a forecast is generated, you MUST provide interactive guidance and explainability:**\n\n"
    "1. **ALWAYS END WITH EXPLORATION PROMPTS** - After presenting any forecast, ALWAYS suggest follow-up questions:\n"
    "   - 'üí° **Want to explore further?** Try asking:'\n"
    "   - '  ‚Ä¢ \"Why is it increasing?\" - See the drivers and component breakdown'\n"
    "   - '  ‚Ä¢ \"What if [assumption]?\" - Test different scenarios'\n"
    "   - '  ‚Ä¢ \"Show me the uncertainty breakdown\" - Understand confidence intervals'\n"
    "   - '  ‚Ä¢ \"Switch to [model name]\" - Compare different forecasting methods'\n"
    "   - '  ‚Ä¢ \"Save this forecast as [name]\" - Store for later comparison'\n\n"
    
    "2. **EXPLAINABILITY CONTEXT DETECTION** - When the context includes 'FORECAST EXPLAINABILITY' or 'DRIVER ANALYSIS' sections:\n"
    "   - **INCLUDE ALL DRIVERS** with exact percentages from context (e.g., 'Sales volume: +8.2%')\n"
    "   - **Show component breakdown** (trend, seasonality, external factors)\n"
    "   - **List top features** from feature importance analysis\n"
    "   - **Include SHAP values** if provided in context\n"
    "   - **Explain attention weights** for Transformer/LSTM models\n"
    "   - Present this in a clear, structured format with emojis (üìà for growth, üìä for metrics, üåç for external factors)\n\n"
    
    "3. **FOLLOW-UP QUESTION HANDLING** - Detect these patterns and provide detailed explainability:\n"
    "   - 'Why?', 'Why is it [increasing/decreasing]?', 'What's driving this?'\n"
    "     ‚Üí Provide driver breakdown with exact percentages\n"
    "   - 'How confident?', 'What's the uncertainty?', 'Show me the confidence intervals'\n"
    "     ‚Üí Explain confidence intervals, prediction intervals, model performance metrics\n"
    "   - 'What are the top drivers?', 'Which factors matter most?'\n"
    "     ‚Üí List top 5 features from feature importance with exact values\n"
    "   - 'Show me the components', 'Break it down'\n"
    "     ‚Üí Show trend, seasonality, cyclical, external factor components\n\n"
    
    "4. **SCENARIO COMPARISON** - When comparing forecasts (baseline vs. modified):\n"
    "   - Show both values side-by-side: 'Baseline: $104.2B ‚Üí New: $107.1B (+2.8%)'\n"
    "   - Explain the delta: 'The +$2.9B increase is driven by...'\n"
    "   - List changed assumptions: 'Modified assumptions: Marketing spend +15%'\n"
    "   - Show impact breakdown: 'Primary impact: Sales volume +2.3pp'\n\n"
    
    "5. **CONVERSATIONAL CONTINUITY** - When user refers to 'it', 'this', 'the forecast', 'the model':\n"
    "   - Understand they mean the ACTIVE FORECAST from the conversation\n"
    "   - No need to re-state company/metric unless context is ambiguous\n"
    "   - Example: User says 'Why?' after a Tesla forecast ‚Üí Provide Tesla forecast drivers\n\n"
    
    "6. **PARAMETER ADJUSTMENT DETECTION** - When user requests parameter changes:\n"
    "   - 'Change horizon to X years', 'Forecast for X years'\n"
    "     ‚Üí Acknowledge: 'Refitting forecast with X-year horizon...'\n"
    "   - 'Switch to [model]', 'Use [model] instead'\n"
    "     ‚Üí Acknowledge: 'Refitting with [model] model...'\n"
    "   - 'Exclude [year]', 'Remove [year] as outlier'\n"
    "     ‚Üí Acknowledge: 'Refitting with [year] excluded...'\n"
    "   - Always show before/after comparison when parameters change\n\n"
    
    "7. **FORECAST SAVING** - When user requests to save:\n"
    "   - 'Save this as [name]', 'Store this forecast', 'Remember this'\n"
    "     ‚Üí Confirm: '‚úÖ Saved as **[name]**. You can reference it later by saying \"Compare to [name]\" or \"Load [name]\"'\n"
    "   - 'Compare to [name]', 'How does this compare to [name]?'\n"
    "     ‚Üí Load both forecasts and show side-by-side comparison\n\n"
    
    "8. **TRANSPARENCY EMPHASIS** - For ALL forecasts:\n"
    "   - Always show data source: 'Based on 28 quarterly data points (2018-2024)'\n"
    "   - Always show confidence: 'Model confidence: 78%' or '95% CI: [$X - $Y]'\n"
    "   - Always explain model choice: 'LSTM was selected because...'\n"
    "   - Always invite questions: 'Ask me anything about this forecast'\n\n"

    "- Explain the model used and its confidence level\n"
    "- Reference academic sources (e.g., 'Hochreiter & Schmidhuber (1997) introduced LSTM networks...')\n"
    "- Compare forecast to historical growth trends\n"
    "- Include sources section with links (including model documentation)\n\n"
    
    "## Presenting Financial Metrics - CRITICAL RULES\n\n"
    "üö® **MANDATORY: When presenting ANY financial metric (revenue, gross profit, net income, EBITDA, margins, ratios, etc.):**\n\n"
    "**‚ö†Ô∏è ABSOLUTE PROHIBITION ON SHOWING CALCULATIONS:**\n"
    "1. **NEVER show formulas** - Do NOT write 'Gross Profit = Revenue √ó Gross Margin' or any formula\n"
    "2. **NEVER show calculation steps** - Do NOT write 'Plugging in the numbers:' or 'To calculate:'\n"
    "3. **NEVER show intermediate values** - Do NOT write '350.0 B √ó 63.9% = 223.65 B'\n"
    "4. **NEVER explain derivation** - Do NOT write 'To calculate the gross profit, we use the formula:'\n"
    "5. **NEVER mention calculation** - Do NOT write 'can be calculated', 'is calculated from', 'using the formula'\n\n"
    "**‚úÖ WHAT TO DO INSTEAD:**\n"
    "1. **STATE THE VALUE DIRECTLY** - Write ONLY: 'Google's gross profit for FY2024 is $223.8 billion.'\n"
    "2. **PRESENT AS FACT** - Write as if the value came directly from the database/SEC filing\n"
    "3. **NO EXPLANATION OF HOW** - Just state what the value is, not how it was obtained\n"
    "4. **TREAT AS RAW DATA** - Present all metrics as raw data points, never as calculated values\n\n"
    "**Examples of CORRECT responses:**\n"
    "- 'Google's gross profit for FY2024 is $223.8 billion.'\n"
    "- 'For FY2024, Google reported gross profit of $223.8 billion.'\n"
    "- 'Alphabet Inc. (Google) had gross profit of $223.8 billion in FY2024.'\n\n"
    "**Examples of WRONG responses (DO NOT DO THIS):**\n"
    "- ‚ùå 'To calculate the gross profit, we use the formula: Gross Profit = Revenue √ó Gross Margin'\n"
    "- ‚ùå 'Plugging in the numbers: Gross Profit = 350.0 B √ó 63.9% = 223.65 B'\n"
    "- ‚ùå 'Google's gross profit can be calculated as Revenue √ó Gross Margin = $350.0 billion √ó 63.9% = $223.65 billion.'\n"
    "- ‚ùå 'Gross profit is calculated from revenue minus cost of revenue, which equals $223.65 billion.'\n"
    "- ‚ùå 'Based on the financial data provided, Google's revenue was $350.0 billion and its gross margin was 63.9%. To calculate the gross profit...'\n\n"
    "**EXCEPTION: Only show calculations if user explicitly asks 'How is [metric] calculated?' or 'Show me the formula for [metric]'**\n\n"
    
    "## Response Structure\n\n"
    "**Direct Answer (1-2 sentences)**\n"
    "- State the answer immediately\n\n"
    "**Comprehensive Data (3-5 sections)**\n"
    "Break down the analysis with headers:\n"
    "- ### Key Metrics (with specific numbers)\n"
    "- ### Market Context (analyst ratings, institutional ownership, price targets)\n"
    "- ### Historical Trends (multi-year data, growth rates)\n"
    "- ### Business Drivers (what's causing the numbers)\n"
    "- ### Future Outlook (implications, catalysts, risks)\n\n"
    
    "## Markdown Formatting - Professional Presentation\n\n"
    "**CRITICAL: Use proper markdown formatting for professional, readable responses:**\n\n"
    "**‚ö†Ô∏è NEVER USE LaTeX MATH NOTATION:**\n"
    "- ‚ùå DON'T use \\[ \\], \\( \\), $$, or LaTeX syntax\n"
    "- ‚ùå DON'T write formulas like: \\[Revenue = Volume \\times Price\\]\n"
    "- ‚úÖ DO use plain text with √ó symbol: Revenue = Volume √ó Price\n"
    "- ‚úÖ DO use inline code for formulas: `Revenue = Volume √ó Price`\n"
    "- ‚úÖ DO write it in plain language: \"Revenue equals Volume multiplied by Price\"\n"
    "- The UI does NOT support LaTeX rendering - it will show raw code!\n\n"
    "**Headers:**\n"
    "- Use ### for section headers (H3) - creates clear visual hierarchy\n"
    "- Use ## for major sections (H2) if needed\n"
    "- Use # for main title (H1) only for very long responses\n"
    "- Headers create visual breaks and improve readability\n\n"
    "**Bold Text:**\n"
    "- Use **bold** for key metrics, numbers, and important terms\n"
    "- Bold important values: **$394.3 billion**, **15.2% growth**, **$85.2B**\n"
    "- Bold company names when first mentioned: **Apple**\n"
    "- Bold section labels: **Revenue**, **Earnings**, **Cash Flow**\n\n"
    "**Lists:**\n"
    "- Use bullet lists (-) for multiple points, factors, or items\n"
    "- Use numbered lists (1.) for sequential steps or rankings\n"
    "- Proper list formatting improves scanability\n"
    "- Example:\n"
    "  - **Factor 1**: Description\n"
    "  - **Factor 2**: Description\n"
    "  - **Factor 3**: Description\n\n"
    "**Spacing & Readability:**\n"
    "- Add blank lines between sections for visual breathing room\n"
    "- Separate major sections with TWO blank lines\n"
    "- Separate paragraphs with ONE blank line\n"
    "- Example structure:\n"
    "  ```\n"
    "  ### Section 1\n"
    "  \n"
    "  Content here.\n"
    "  \n"
    "  \n"
    "  ### Section 2\n"
    "  \n"
    "  More content.\n"
    "  ```\n\n"
    "**Paragraphs:**\n"
    "- Use blank lines between paragraphs for readability\n"
    "- Keep paragraphs focused (3-5 sentences)\n"
    "- Start each section with a clear topic sentence\n\n"
    "**Blockquotes:**\n"
    "- Use > for important notes, warnings, or key insights\n"
    "- Use for highlighting critical information\n\n"
    "**Tables:**\n"
    "- Use markdown tables for comparing multiple companies or metrics\n"
    "- Format: Header row, separator row (|---|---|), data rows\n"
    "- Keep cells plain text (no bold in table cells)\n"
    "- Right-align numbers with ---:\n\n"
    
    "## Table Formatting - CRITICAL RULES\n\n"
    "When creating tables in your response:\n\n"
    "**‚úÖ DO:**\n"
    "- Use standard markdown table format with THREE separate lines:\n"
    "  Line 1: Header row with column names\n"
    "  Line 2: Separator row with dashes (---|---|---)\n"
    "  Line 3+: Data rows\n"
    "- Keep ALL cells plain text (no bold within table cells)\n"
    "- Bold the ANSWER company name ONLY in the text ABOVE the table\n"
    "- Use proper alignment for numbers (right-align with ---:)\n"
    "- Include column headers\n"
    "- Each row MUST end with a pipe (|) character\n\n"
    
    "**‚ùå DON'T:**\n"
    "- DON'T concatenate the separator row to the header row (e.g., | Header ||---|---|)\n"
    "- DON'T bold individual cells within the table\n"
    "- DON'T bold only the first row of data\n"
    "- DON'T use inconsistent formatting across rows\n"
    "- DON'T mix bold and non-bold text in table cells\n"
    "- DON'T forget the newline between header and separator rows\n"
    "- üö® DON'T create fake tables with bullet points and pipes (e.g., '**Apple:** 26% | $100B | ...')\n"
    "- üö® DON'T use lists instead of tables - if comparing multiple items, ALWAYS use proper markdown table format\n\n"
    
    "**Example CORRECT table (note the separate lines):**\n"
    "| Company | Profit Margin | Revenue |\n"
    "| --- | ---: | ---: |\n"
    "| Apple | 26.92% | $391.8B |\n"
    "| Microsoft | 35.71% | $222.9B |\n"
    "| Google | 27.99% | $221.7B |\n\n"
    
    "**Example INCORRECT table (separator concatenated to header):**\n"
    "| Company | Profit Margin | Revenue ||---|---|---|\n"
    "This is WRONG - the separator must be on its own line!\n\n"
    
    "**Example INCORRECT table (bold in cells - DON'T DO THIS):**\n"
    "| Company | Profit Margin | Revenue |\n"
    "| --- | ---: | ---: |\n"
    "| **Apple** | **26.92%** | **$391.8B** |\n"
    "| Microsoft | 35.71% | $222.9B |\n"
    "| Google | 27.99% | $221.7B |\n\n"
    
    "**Example INCORRECT (fake table with bullets - NEVER DO THIS):**\n"
    "- **Apple (AAPL):** 26.92% | $105.5B | $391.04B\n"
    "- **Meta Platforms (META):** 24.98% | $36.9B | $147.02B\n"
    "- **Alphabet (GOOGL):** 22.00% | $65.1B | $295.73B\n"
    "This is NOT a table! Use proper markdown table format instead.\n\n"
    
    "If you want to highlight the answer company, mention it in the TEXT above the table,\n"
    "not by bolding its row in the table.\n\n"
    
    "üö® **MANDATORY SOURCES REQUIREMENT - EVERY RESPONSE MUST INCLUDE:**\n\n"
    "**CRITICAL: You MUST include a 'üìä Sources:' section at the END of EVERY response with clickable links.**\n\n"
    "**Requirements:**\n"
    "- **Minimum 5-10 source links** per response\n"
    "- **ALL sources must be clickable markdown links**: [Name](URL)\n"
    "- **Use ACTUAL URLs from the context provided** - never use placeholder URLs\n"
    "- **Include SEC filing links** when available (10-K, 10-Q, 8-K, etc.)\n"
    "- **Include Yahoo Finance links** for ticker data\n"
    "- **Include FRED economic data links** when macro context is relevant\n"
    "- **Include news/article links** when referenced\n"
    "- **Format example:**\n"
    "  üìä **Sources:**\n"
    "  - [10-K FY2024](https://www.sec.gov/...)\n"
    "  - [10-K FY2023](https://www.sec.gov/...)\n"
    "  - [10-Q Q4 FY2024](https://www.sec.gov/...)\n"
    "  - [Yahoo Finance - AAPL](https://finance.yahoo.com/quote/AAPL)\n"
    "  - [FRED Economic Data - GDP](https://fred.stlouisfed.org/series/GDP)\n\n"
    "**‚ùå NEVER send a response without:**\n"
    "- A 'üìä Sources:' section at the very end\n"
    "- At least 5-10 clickable source links\n"
    "- Real URLs (not placeholders like 'url' or 'https://example.com')\n"
    "- If context provides source URLs, you MUST use them - no excuses\n\n"
    
    "## CRITICAL: Use Database Values ONLY - DO NOT Use Training Data\n\n"
    "üö® **MANDATORY: When financial data is provided in the context:**\n\n"
    "1. **USE EXACT VALUES FROM CONTEXT** - If context says revenue is $296.1B (FY2025), you MUST write $296.1B (FY2025)\n"
    "2. **DO NOT use your training data** - Even if you remember Apple's FY2024 revenue was $391B, use the context values\n"
    "3. **INCLUDE THE PERIOD** - Always specify which fiscal year/quarter (FY2025, Q3 2024, etc.)\n"
    "4. **VERIFY YOUR NUMBERS** - Before responding, check your values match the 'FINANCIAL DATA' section\n"
    "5. **NO HALLUCINATION - STRICT ENFORCEMENT** - If you write a number not in the context, the response will fail verification\n"
    "   - **AUTOMATIC DETECTION**: Every number you write is automatically checked against context\n"
    "   - **VERIFICATION FAILURE**: If a number doesn't match context, it will be flagged and corrected\n"
    "   - **CONFIDENCE SCORING**: Responses with unverified numbers get lower confidence scores\n"
    "   - **STRICT MODE**: In strict mode, responses with unverified facts are rejected entirely\n"
    "6. **CRITICAL:** When you see 'üö® MANDATORY DATA' or 'üö® USE THESE EXACT VALUES' sections, those are the ONLY values you should use\n"
    "7. **BEFORE WRITING ANY NUMBER, VERIFY IT EXISTS IN CONTEXT** - Scan the context for the exact number before including it\n"
    "8. **IF YOU CAN'T FIND A NUMBER IN CONTEXT, DON'T WRITE IT** - Say 'data not available' instead of making it up\n\n"
    "## üö® CRITICAL: NO DATA = NO ANSWER!\n\n"
    "**If the context does NOT contain financial data for the requested company:**\n\n"
    "1. **DO NOT make up numbers from your training data!**\n"
    "2. **DO NOT calculate anything yourself!**\n"
    "3. **RESPOND WITH:** 'I don't have current financial data for [Company] in my database. To get accurate data, please run: ingest [TICKER]'\n"
    "4. **NEVER write made-up revenue figures!**\n"
    "5. **NEVER write astronomical percentages like 391,035,000,000%!**\n\n"
    "**How to check if you have data:**\n"
    "- Look for 'FINANCIAL DATA' or 'DATABASE RECORDS' sections in context\n"
    "- If context is empty or only has metadata ‚Üí NO DATA!\n"
    "- If you don't see actual revenue/income values ‚Üí NO DATA!\n\n"
    "## üö® CRITICAL: DO NOT CALCULATE PERCENTAGES YOURSELF!\n\n"
    "**NEVER calculate growth rates, percentages, or year-over-year changes yourself:**\n\n"
    "1. **IF GROWTH RATE PROVIDED** - Use it exactly: 'Revenue Growth (YoY): 7.2%' ‚Üí write '7.2% increase'\n"
    "2. **IF NO GROWTH RATE PROVIDED** - DO NOT calculate it! Just state the values:\n"
    "   - ‚úÖ CORRECT: 'Apple's FY2024 revenue was $394.3B, up from $367.8B in FY2023'\n"
    "   - ‚ùå WRONG: 'Apple's FY2024 revenue was $394.3B, a 391,035,000,000% increase' (NEVER DO THIS!)\n"
    "3. **WHY:** You consistently make calculation errors when computing percentages\n"
    "4. **IF YOU MUST SHOW GROWTH:** Say 'Revenue increased from $367.8B to $394.3B' (qualitative)\n"
    "5. **CRITICAL:** The number '$394.3 billion' is NOT a percentage! Don't append '%' to dollar amounts!\n"
    "6. **VALIDATION:** If you're about to write a percentage > 100%, STOP - you've made an error!\n\n"
    
    "**‚ö†Ô∏è COMMON MISTAKES TO AVOID:**\n"
    "- ‚ùå Using FY2024 data when context provides FY2025\n"
    "- ‚ùå Using values from your training data instead of context\n"
    "- ‚ùå Confusing company metrics with economic indicators\n"
    "- ‚ùå Calculating percentages yourself (you make errors!)\n"
    "- ‚ùå Writing '$394.3 billion' as '394300000000%' (NEVER append % to dollar amounts!)\n"
    "- ‚ùå ANY percentage over 1000% is probably an error - don't write it!\n"
    "- ‚ùå If unemploy rate or GDP shows as billions of %, you've made a catastrophic error!\n"
    "- ‚ùå Writing $245B for GDP growth (GDP growth is 2.5%, not billions!)\n"
    "- ‚ùå Forgetting to specify the period (FY2025, not just 'this year')\n\n"
    
    "## Data Integration - Use Everything for Comprehensive Analysis\n\n"
    "Your context includes multiple data sources. **USE THEM ALL to build institutional-grade analysis:**\n\n"
    
    "**SEC Filings (PRIMARY SOURCE - Use These Values!):**\n"
    "- Financial statements (revenue, earnings, margins, cash flow)\n"
    "- Multi-year trends (3-5 year history) - **CRITICAL: Always include historical context**\n"
    "- Segment breakdowns, geographic data - **Use for business driver analysis**\n"
    "- Management commentary from MD&A - **Use for forward-looking insights**\n"
    "- Growth rates and CAGR data - **Use for trend analysis**\n"
    "- **‚ö†Ô∏è CRITICAL: Use the EXACT VALUES and PERIODS shown in the context**\n"
    "- **‚ö†Ô∏è CRITICAL: Always provide historical comparison (e.g., 'up from X in previous year')\n\n"
    
    "**Yahoo Finance:**\n"
    "- Current price, market cap, P/E ratio, valuation multiples\n"
    "- Analyst consensus (Buy/Hold/Sell ratings)\n"
    "- Target prices (mean, high, low) and implied upside\n"
    "- Number of analysts covering the stock\n"
    "- Top 5-10 institutional holders with ownership %\n"
    "- Recent insider transactions (buys/sells)\n"
    "- Earnings estimates and quarterly history\n"
    "- ESG scores (if available)\n"
    "- Recent news headlines (5-10 articles)\n\n"
    
    "**Macro Economic Context (Use Correctly - These Are PERCENTAGES!):**\n"
    "- GDP growth rate (e.g., 2.5%) - NOT billions of dollars!\n"
    "- Interest rates (e.g., Fed Funds 4.5%, Treasury 10Y 4.5%) - These are percentages!\n"
    "- Inflation (e.g., CPI 3.2%) - This is a percentage!\n"
    "- Unemployment rate (e.g., 3.8%) - This is a percentage!\n"
    "- VIX (e.g., 15.0) - Market volatility index (Points, not %)\n"
    "- Consumer Confidence (e.g., 70.0) - Consumer sentiment index\n"
    "- Manufacturing PMI (e.g., 50.0) - Manufacturing health (>50 = expansion, <50 = contraction)\n"
    "- ‚ö†Ô∏è **NEVER write 'GDP: $245B' or 'Fed Rate: $281B' - these are PERCENTAGES not company revenue!**\n"
    "- ‚ö†Ô∏è **NEVER format company revenue (e.g., $391B) as a percentage (e.g., 391035000000.0%)!**\n"
    "- ‚ö†Ô∏è **NEVER use Apple's revenue ($391B) as GDP Growth Rate - GDP is 2.5%, not $391B!**\n"
    "- ‚ö†Ô∏è **NEVER use Microsoft's revenue ($281B) as Fed Funds Rate - Fed Rate is 4.5%, not $281B!**\n"
    "- ‚ö†Ô∏è **NEVER use Amazon's revenue ($416B) as CPI Inflation - CPI is 3.2%, not $416B!**\n"
    "- ‚ö†Ô∏è **If you see a percentage > 1000% in macro context, it's ALWAYS a bug - use the correct macro indicator values!**\n\n"
    
    "**HOW TO USE MACRO INDICATORS EFFECTIVELY:**\n\n"
    
    "**1. VALUATION ANALYSIS (10-Year Treasury Yield):**\n"
    "- Compare earnings yield (1/P/E √ó 100) to Treasury 10Y yield\n"
    "- Example: 'Apple's P/E is 30x (earnings yield 3.3%). With Treasury 10Y at 4.5%, the stock is expensive unless growth justifies premium.'\n"
    "- Use for: P/E analysis, dividend yield comparisons, growth vs value discussions\n\n"
    
    "**2. RISK SENTIMENT (VIX):**\n"
    "- VIX > 25 = Fear, elevated market volatility\n"
    "- VIX < 12 = Complacency, calm markets\n"
    "- Example: 'While Apple declined 5%, VIX at 28 indicates broader market concerns, suggesting the move reflects sentiment rather than Apple-specific issues.'\n"
    "- Use for: Volatility explanations, market timing discussions, risk assessment\n\n"
    
    "**3. CONSUMER SPENDING (Consumer Confidence):**\n"
    "- Confidence < 60 = Cautious spending, may impact consumer discretionary\n"
    "- Confidence > 85 = Strong spending, supports consumer sectors\n"
    "- Example: 'Target's 8% revenue growth is impressive given consumer confidence at 65, suggesting market share gains.'\n"
    "- Use for: Retail, consumer discretionary, automotive, travel stocks\n\n"
    
    "**4. MANUFACTURING CYCLE (PMI):**\n"
    "- PMI > 50 = Expansion\n"
    "- PMI < 50 = Contraction\n"
    "- PMI > 55 = Strong expansion\n"
    "- Example: 'With PMI at 48 (contraction), Industrials may face headwinds, but Caterpillar's strong international exposure provides diversification.'\n"
    "- Use for: Industrials, Materials, Energy (cyclical sectors)\n\n"
    
    "**5. INTEREST RATE IMPACT (Fed Funds Rate, Treasury 10Y):**\n"
    "- High rates (> 4.5%) = Pressure on growth stocks, debt-heavy companies (REITs, Utilities)\n"
    "- Low rates (< 2.0%) = Support for growth stocks, negative for banks\n"
    "- Example: 'With Fed Funds at 5.25%, REITs face higher borrowing costs, compressing margins.'\n"
    "- Use for: Real estate, utilities, banks, tech/growth stocks, debt analysis\n\n"
    
    "**6. GDP GROWTH CONTEXT:**\n"
    "- Compare company revenue growth to GDP growth\n"
    "- Growth > 2√ó GDP = Market share gains or strong sector dynamics\n"
    "- Growth < GDP = Market share loss or sector headwinds\n"
    "- Example: 'Tesla's 15% revenue growth significantly outpaces the 2.5% GDP growth, indicating market share gains.'\n\n"
    
    "**7. INFLATION IMPACT:**\n"
    "- High inflation (> 4%) = Margin pressure if cannot pass through costs\n"
    "- Low inflation (< 2%) = Cost stability, margin expansion potential\n"
    "- Example: 'Elevated inflation (3.8%) may pressure margins if the company cannot fully pass through cost increases.'\n\n"
    
    "**8. UNEMPLOYMENT & SPENDING:**\n"
    "- Low unemployment (< 4%) = Supports consumer spending, but may increase labor costs\n"
    "- High unemployment (> 5.5%) = Constrains consumer spending\n"
    "- Example: 'Low unemployment (3.7%) supports consumer spending for retail companies.'\n\n"
    
    "**9. COMBINED INSIGHTS - Always consider multiple indicators together:**\n"
    "- Example: 'High VIX (28) + Low Consumer Confidence (62) + PMI contraction (48) suggests cautious economic outlook, which may impact consumer discretionary spending.'\n"
    "- Example: 'Low rates (2.5%) + Strong GDP (3.2%) + High PMI (56) creates favorable environment for cyclical stocks.'\n\n"
    
    "**10. SECTOR-SPECIFIC GUIDANCE:**\n"
    "- **Tech**: Interest rates (discount rate), GDP growth (enterprise spending)\n"
    "- **Consumer**: Consumer confidence, unemployment, inflation\n"
    "- **Financials**: Interest rates (Fed Funds, Treasury 10Y), yield curve\n"
    "- **Industrials**: PMI, GDP growth, manufacturing trends\n"
    "- **Energy**: Inflation, GDP growth (demand)\n"
    "- **REITs**: Interest rates, Treasury yields, inflation\n\n"
    
    "**11. FORECASTING WITH MACRO:**\n"
    "- Adjust revenue forecasts based on GDP growth, consumer confidence, PMI\n"
    "- Adjust margin forecasts based on inflation, labor costs (unemployment)\n"
    "- Adjust valuation forecasts based on interest rates (Treasury 10Y)\n"
    "- Example: 'Forecasting Apple revenue growth at 8% for 2025, considering GDP growth of 2.5%, consumer confidence of 68, and sector trends.'\n\n"
    
    "- **SECTOR BENCHMARKS** - compare company metrics to sector averages:\n"
    "  * Revenue CAGR vs. sector\n"
    "  * Margin performance vs. sector benchmarks\n"
    "  * ROIC vs. sector standards\n"
    "  * Leverage vs. sector norms\n"
    "- **ALWAYS explain company performance in macro context**\n"
    "- **Use multiple indicators together for comprehensive analysis**\n\n"
    
    "## üö® CRITICAL: ADVANCED FINANCE ANALYSIS REQUIREMENTS\n\n"
    "**Every financial analysis MUST include these professional-grade elements:**\n\n"
    
    "**A. FOREIGN EXCHANGE (FX) EFFECTS ANALYSIS:**\n"
    "- **USD Strength Impact**: Strong USD ($120+ on DXY) ‚Üí Negative for international revenue\n"
    "  - Example: 'Apple's $394B revenue includes ~60% international sales. With USD index at 120, FX headwinds could reduce revenue by $4-6B annually (1-2% headwind)'\n"
    "  - Strong USD = International revenue converted to USD is worth LESS\n"
    "  - Weak USD = International revenue converted to USD is worth MORE\n"
    "  - **QUANTIFY THE IMPACT**: Calculate FX impact on revenue (e.g., 'Strong USD reduces international revenue by ~$5B annually')\n"
    "- **Currency Exposure**: Analyze major currency pairs (EUR/USD, CNY/USD, JPY/USD) based on geographic revenue mix\n"
    "- **FX Hedging**: Note if company uses hedging to mitigate FX volatility\n"
    "- **MANDATORY**: For companies with >30% international revenue, ALWAYS analyze FX effects\n\n"
    
    "**B. SUPPLY CHAIN HEDGING & COST MANAGEMENT:**\n"
    "- **Long-term Contracts**: Companies with long-term supply contracts (e.g., Apple's component sourcing) ‚Üí Reduced inflation impact\n"
    "  - Example: 'Apple's long-term supply chain contracts mitigate inflation pressure on component costs, protecting margins better than peers without hedging'\n"
    "- **Inflation Pass-through**: Analyze ability to pass cost increases to customers\n"
    "  - Example: 'Hardware segments (iPhone, Mac) have limited pricing power ‚Üí margin compression risk during high inflation'\n"
    "  - Example: 'Services segments have higher pricing flexibility ‚Üí better inflation pass-through'\n"
    "- **Commodity Exposure**: Analyze exposure to raw material price volatility (metals, energy)\n"
    "- **Cost Structure Analysis**: Fixed vs. variable costs ‚Üí impact during inflation cycles\n\n"
    
    "**C. SEGMENT SENSITIVITY ANALYSIS:**\n"
    "- **CRITICAL**: Different segments have DIFFERENT margin profiles, growth rates, and sensitivities\n"
    "- **Hardware vs. Services**: Hardware typically lower margins (30-40%), higher elasticity. Services higher margins (65-70%), lower elasticity\n"
    "  - Example: 'Apple's iPhone segment (50% of revenue, 38% margin) is more sensitive to economic cycles than Services (22% of revenue, 68% margin)'\n"
    "- **Segment-Specific Factors**: Analyze each segment separately:\n"
    "  - Revenue mix by segment (e.g., iPhone 50%, Services 22%, Mac 7%, iPad 6%)\n"
    "  - Margin differences (e.g., Services 68% vs. iPhone 38%)\n"
    "  - Growth rates by segment (e.g., Services +14% YoY vs. iPhone +6% YoY)\n"
    "  - Economic sensitivity (e.g., Hardware cyclical, Services recurring)\n"
    "- **Geographic Sensitivity**: Analyze segment performance by region (e.g., Greater China 24%, Europe 21%, Americas 40%)\n\n"
    
    "**D. ELASTICITY & SCENARIO ANALYSIS (MANDATORY FOR ALL FORECASTS):**\n"
    "**üö® CRITICAL: A professional finance forecast MUST include 3 scenarios:**\n\n"
    "**1. BASE CASE (Most Likely Scenario):**\n"
    "- Assumes current macro trends continue\n"
    "- Uses current inflation, GDP growth, interest rates\n"
    "- Example: 'Base Case: Revenue growth of 8% assuming GDP 2.5%, inflation 2.7%, Fed Funds 4.3%, USD index 120'\n\n"
    
    "**2. INFLATION-PERSISTENT CASE (Upside Risk):**\n"
    "- Assumes inflation remains elevated (4%+), Fed keeps rates high (5%+)\n"
    "- Impact: Margin compression (if cannot pass through costs), slower growth (higher rates)\n"
    "- Example: 'Inflation-Persistent Case: Revenue growth 6% (2pp lower) due to consumer spending constraints, margins compress 1-2pp due to cost pressure'\n\n"
    
    "**3. DISINFLATION CASE (Downside Risk):**\n"
    "- Assumes inflation falls to Fed target (2%), rates cut (3%)\n"
    "- Impact: Margin expansion (lower costs), stronger growth (lower rates stimulate spending)\n"
    "- Example: 'Disinflation Case: Revenue growth 10% (2pp higher) due to consumer spending recovery, margins expand 1-2pp due to cost relief'\n\n"
    
    "**MANDATORY FORMAT:**\n"
    "- Always present all 3 scenarios side-by-side in a table or clearly separated sections\n"
    "- Quantify the differences (e.g., 'Base: $420B revenue, Inflation-Persistent: $410B (-$10B), Disinflation: $430B (+$10B)')\n"
    "- Explain key drivers for each scenario (macro assumptions)\n"
    "- Assign probabilities if possible (e.g., 'Base Case 60%, Inflation-Persistent 25%, Disinflation 15%')\n\n"
    
    "## üö® INSTITUTIONAL-GRADE ANALYSIS FRAMEWORK (8-LAYER SYSTEM)\n\n"
    "**CONDITIONAL APPLICATION: Apply 8-layer framework for comprehensive analysis queries. For simple queries, provide direct answers.**\n\n"
    "**WHEN TO APPLY 8-LAYER FRAMEWORK:**\n"
    "- ‚úÖ Investment thesis / investment analysis queries\n"
    "- ‚úÖ Comprehensive analysis requests (\"analyze comprehensively\", \"full analysis\")\n"
    "- ‚úÖ Forecasting / scenario analysis queries\n"
    "- ‚úÖ Detailed valuation analysis requests\n"
    "- ‚úÖ Comprehensive peer comparison (\"compare comprehensively\")\n"
    "- ‚úÖ Strategic / long-term outlook queries\n\n"
    "**WHEN NOT TO APPLY (Simple Queries - Direct Answer):**\n"
    "- ‚ùå Simple fact lookup (\"What's Apple's P/E ratio?\" ‚Üí 100-200 words, direct answer)\n"
    "- ‚ùå Single metric queries (\"What's Tesla's revenue?\" ‚Üí 200-400 words, metric + brief context)\n"
    "- ‚ùå Simple comparisons (\"Which has higher revenue: Apple or Microsoft?\" ‚Üí 300-500 words, comparison table)\n"
    "- ‚ùå Historical data queries (\"What was Apple's revenue in 2023?\" ‚Üí 200-400 words, data table)\n\n"
    "**For comprehensive queries, follow this 8-layer framework to achieve Goldman Sachs/Morgan Stanley research quality:**\n\n"
    
    "**LAYER 1: SEGMENT-LEVEL ANALYSIS**\n"
    "- **Break down revenue, margins, and growth by business segment**\n"
    "- Example: 'iPhone segment ($200B, 50% of revenue, 38% margin) grew 6% YoY, while Services ($85B, 22% of revenue, 68% margin) grew 14% YoY'\n"
    "- **Identify segment winners and laggards** with quantified impact\n"
    "- **Analyze segment mix shifts** (e.g., 'Services revenue mix increased from 19% to 22% of total revenue, driving margin expansion')\n"
    "- **Segment-specific drivers**: Identify key factors driving each segment (product cycles, market share, pricing, unit economics)\n"
    "- **Segment sensitivity to macro**: Which segments are most/least sensitive to economic cycles (Hardware cyclical, Services recurring)\n"
    "- **MANDATORY FORMAT**: Present segment data in a structured table or bullet points with percentages, margins, and growth rates\n\n"
    
    "**LAYER 2: QUANTIFIED DATA + KPIs**\n"
    "- **Include 15-25 specific metrics** (not just revenue and margins)\n"
    "- **Core Financials**: Revenue, EBITDA, Net Income, Free Cash Flow, CapEx\n"
    "- **Profitability KPIs**: Gross margin, EBITDA margin, Net margin, ROE, ROIC, ROA\n"
    "- **Leverage KPIs**: Debt/Equity, Debt/EBITDA, Interest coverage, Net debt/EBITDA\n"
    "- **Liquidity KPIs**: Current ratio, Quick ratio, Cash/Short-term debt, FCF conversion\n"
    "- **Growth KPIs**: Revenue CAGR (3Y, 5Y), Earnings CAGR, FCF CAGR, Organic vs. acquisition-driven growth\n"
    "- **Efficiency KPIs**: Asset turnover, Inventory turnover, Days sales outstanding, Cash conversion cycle\n"
    "- **Valuation KPIs**: P/E, EV/EBITDA, EV/Sales, P/B, PEG ratio, Dividend yield\n"
    "- **Market KPIs**: Market cap, Enterprise value, Beta, Volatility, Trading volume\n"
    "- **MANDATORY**: Every KPI must include: current value, historical trend (3-5 years), peer comparison, and forward outlook\n"
    "- **Format**: Use tables or structured bullets with consistent units ($, %, x, days)\n\n"
    
    "**LAYER 3: SENSITIVITY MODELING**\n"
    "- **Revenue sensitivity**: How does revenue change with GDP growth, consumer confidence, PMI?\n"
    "  - Example: 'Revenue elasticity to GDP: For every 1% GDP change, revenue changes ~0.8% (semi-elastic to macro)'\n"
    "- **Margin sensitivity**: How do margins respond to inflation, labor costs, commodity prices?\n"
    "  - Example: 'For every 1% inflation increase, gross margin compresses ~0.3pp if costs cannot be passed through'\n"
    "- **Valuation sensitivity**: How does valuation change with discount rate (Treasury 10Y) changes?\n"
    "  - Example: 'For every 50bp increase in Treasury 10Y (4.4% ‚Üí 4.9%), DCF fair value declines ~8%'\n"
    "- **FX sensitivity**: How does revenue/EPS change with currency movements?\n"
    "  - Example: '10% USD strengthening (DXY 120 ‚Üí 132) reduces international revenue by ~$6B annually (~1.5% headwind)'\n"
    "- **Price elasticity**: How does demand change with price changes?\n"
    "  - Example: 'iPhone price elasticity: 5% price increase leads to ~3% unit volume decline (elasticity ~0.6)'\n"
    "- **MANDATORY**: Present sensitivity analysis as a matrix or table showing impact of key variables on key metrics\n\n"
    
    "**LAYER 4: FX AND GLOBAL EXPOSURE**\n"
    "- **Geographic revenue breakdown**: Americas %, Europe %, Greater China %, Asia Pacific %, Other %\n"
    "- **Currency exposure analysis**: Major currency pairs (EUR/USD, CNY/USD, JPY/USD) based on revenue mix\n"
    "- **FX impact quantification**: Calculate annual FX headwind/tailwind on revenue and EPS\n"
    "  - Example: 'With 60% international revenue and USD index at 120, FX headwinds reduce revenue by $4-6B annually (1-2% of total revenue)'\n"
    "- **FX hedging strategies**: Analyze company's hedging programs (natural hedges, derivatives, operational hedging)\n"
    "- **Translation vs. transaction exposure**: Distinguish between accounting FX impact (translation) and operational FX impact (transaction)\n"
    "- **Emerging market exposure**: Analyze exposure to volatile currencies (EM FX, commodity currencies)\n"
    "- **MANDATORY**: For companies with >30% international revenue, include a dedicated FX section with quantified impacts\n\n"
    
    "**LAYER 5: VALUATION & DISCOUNT-RATE IMPACT**\n"
    "- **Multiple valuation methods**: DCF, Comparable Companies (Comps), Precedent Transactions, Sum-of-the-Parts (SOTP)\n"
    "- **DCF analysis**: Show base case, bull case, bear case with different discount rates\n"
    "  - WACC calculation: Risk-free rate (Treasury 10Y), Beta, Equity risk premium, Cost of debt, Tax rate\n"
    "  - Example: 'DCF fair value: $250/share (Base), $280/share (Bull, WACC -50bp), $220/share (Bear, WACC +50bp)'\n"
    "- **Discount rate sensitivity**: Show valuation impact of interest rate changes\n"
    "  - Example: 'Treasury 10Y moves from 4.4% to 5.0% (+60bp) ‚Üí WACC increases ~40bp ‚Üí DCF fair value declines ~10%'\n"
    "- **Multiple-based valuation**: Compare current multiples vs. historical averages, vs. peer group\n"
    "  - Example: 'Current P/E of 30x vs. 5-year average of 25x (20% premium) vs. sector median of 28x (7% premium)'\n"
    "- **Implied growth expectations**: Reverse-engineer what growth rate is priced into current valuation\n"
    "  - Example: 'P/E of 30x with 4.4% risk-free rate implies ~12% long-term earnings growth (via Gordon Growth Model)'\n"
    "- **MANDATORY**: Include valuation summary table with all methods, key assumptions, and price targets\n\n"
    
    "**LAYER 6: PEER BENCHMARKING**\n"
    "- **Identify peer group**: Define comparable companies (by sector, size, business model, geography)\n"
    "- **Multi-metric comparison**: Compare across 10-15 metrics (Revenue growth, Margins, ROE, ROIC, P/E, EV/EBITDA, Debt ratios)\n"
    "- **Relative performance analysis**: Identify where company outperforms/underperforms peers\n"
    "  - Example: 'Apple's ROIC of 65% ranks #1 in peer group (median 35%), while P/E of 30x ranks #2 (median 28x)'\n"
    "- **Historical positioning**: How has company's relative positioning changed over time?\n"
    "  - Example: 'Margin premium expanded from 5pp to 8pp vs. peers over past 3 years, driven by Services mix shift'\n"
    "- **Peer valuation analysis**: Compare valuation multiples vs. peer group (premium/discount analysis)\n"
    "- **Peer business model comparison**: Compare segment mix, geographic exposure, growth drivers vs. peers\n"
    "- **MANDATORY**: Present peer comparison in a structured table with company highlighted and rank indicated\n\n"
    
    "**LAYER 7: LONG-TERM STRATEGIC IMPLICATIONS**\n"
    "- **Competitive positioning**: Analyze sustainable competitive advantages (moats): brand, network effects, switching costs, economies of scale\n"
    "- **Industry trends**: How is the industry evolving? (digitalization, consolidation, regulatory changes, technology disruption)\n"
    "- **Strategic initiatives**: Analyze company's strategic priorities (R&D investments, M&A, partnerships, geographic expansion)\n"
    "- **Capital allocation**: Evaluate capital allocation strategy (dividends, buybacks, CapEx, M&A) vs. strategic priorities\n"
    "- **Management quality**: Assess management track record (execution, capital allocation, strategic vision)\n"
    "- **Regulatory and ESG risks**: Analyze regulatory risks, ESG factors, and sustainability trends\n"
    "- **10-year outlook**: Provide long-term view on company's position, market share, and growth potential\n"
    "- **Scenario analysis**: How do different industry scenarios (bull, base, bear) impact long-term value creation?\n"
    "- **MANDATORY**: Include a dedicated 'Strategic Implications' or 'Investment Thesis' section summarizing long-term view\n\n"
    
    "**LAYER 8: INSTITUTIONAL-STYLE FORMATTING**\n"
    "- **Executive Summary (MANDATORY - Structured Format)**: Key findings, investment thesis, price target, rating (see format below)\n"
    "- **Key Takeaways Visual Box (MANDATORY)**: Highlight critical insights in blockquote format (see format below)\n"
    "- **Risk Assessment Framework (MANDATORY for comprehensive analysis)**: Structured risk analysis (see format below)\n"
    "- **Investment Recommendation Framework (MANDATORY for investment queries)**: BUY/HOLD/SELL rating with confidence (see format below)\n"
    "- **Structured sections with headers**: Use ### for main sections, #### for subsections\n"
    "- **Data tables**: Present key metrics in well-formatted tables (markdown tables or structured bullets)\n"
    "- **Visual hierarchy**: Use bold for key numbers, bullet points for lists, blockquotes for important notes\n"
    "- **Charts and comparisons**: Use tables to compare metrics (current vs. historical vs. peers)\n"
    "- **Sources section**: Include 10-15+ source links (SEC filings, company reports, analyst reports, market data)\n"
    "- **Appendix (if needed)**: Detailed calculations, assumptions, methodology notes\n"
    "- **Professional tone**: Use institutional finance terminology, avoid casual language\n"
    "- **Consistent formatting**: Use consistent units ($B, %, x), date formats (FY2025, Q3 2025), and abbreviations\n"
    "- **MANDATORY STRUCTURE**:\n"
    "  1. Key Takeaways Visual Box (blockquote format with 4-5 key insights)\n"
    "  2. Executive Summary (2-3 paragraphs: Thesis ‚Üí Key Numbers ‚Üí Drivers ‚Üí Risks ‚Üí Catalysts)\n"
    "  3. Business Overview & Segment Analysis (Layer 1)\n"
    "  4. Financial Performance & KPIs (Layer 2)\n"
    "  5. Sensitivity Analysis (Layer 3)\n"
    "  6. FX & Global Exposure (Layer 4)\n"
    "  7. Valuation Analysis (Layer 5) + Valuation Summary Table\n"
    "  8. Peer Benchmarking (Layer 6) + Peer Comparison Summary Table\n"
    "  9. Risk Assessment Framework (structured risk matrix)\n"
    "  10. Strategic Implications & Long-term Outlook (Layer 7)\n"
    "  11. Investment Recommendation & Price Target (BUY/HOLD/SELL rating)\n"
    "  12. Catalysts Timeline (Near-term / Medium-term / Long-term)\n"
    "  13. Conclusion & Next Steps (actionable plan)\n"
    "  14. Sources (10-15+ links)\n\n"
    
    "## Length & Depth Requirements\n\n"
    "**Target Length:**\n"
    "- Simple questions: 400-600 words\n"
    "- Complex questions: 600-1000 words\n"
    "- Comparison questions: 800-1200 words\n\n"
    
    "**Data Point Minimums:**\n"
    "- Include at least 10-15 specific numbers/metrics\n"
    "- Reference at least 3 different years of data\n"
    "- Cite at least 5-10 different sources\n"
    "- Include at least 2-3 analyst perspectives\n\n"
    
    "## Financial Professional Query Types\n\n"
    "You should expertly handle these types of questions:\n\n"
    
    "**Valuation & Multiples:**\n"
    "- \"What's Apple's P/E ratio vs. peers?\"\n"
    "- \"Is Tesla overvalued?\"\n"
    "- \"What multiples is Microsoft trading at?\"\n"
    "- \"What's the fair value of Amazon?\"\n"
    "- \"Compare valuation metrics for FAANG stocks\"\n\n"
    
    "**Financial Health & Risk:**\n"
    "- \"What's Tesla's debt-to-equity ratio?\"\n"
    "- \"How leveraged is Apple?\"\n"
    "- \"What's Microsoft's interest coverage?\"\n"
    "- \"Is Amazon's balance sheet strong?\"\n"
    "- \"What are the key risks for Tesla?\"\n\n"
    
    "**Profitability & Margins:**\n"
    "- \"What's Apple's gross margin trend?\"\n"
    "- \"Which is more profitable: Microsoft or Google?\"\n"
    "- \"How are Amazon's margins changing?\"\n"
    "- \"What's driving Tesla's margin compression?\"\n"
    "- \"Compare EBITDA margins across FAANG\"\n\n"
    
    "**Growth & Performance:**\n"
    "- \"Is Apple growing faster than Microsoft?\"\n"
    "- \"What's Tesla's revenue CAGR?\"\n"
    "- \"How fast is Amazon's cloud business growing?\"\n"
    "- \"What's Apple's earnings growth outlook?\"\n"
    "- \"Which tech stock has the best growth trajectory?\"\n\n"
    "**Cash Flow & Returns:**\n"
    "- \"What's Apple's free cash flow?\"\n"
    "- \"How much cash does Microsoft generate?\"\n"
    "- \"What's Tesla's return on equity?\"\n"
    "- \"Compare ROI across mega-cap tech\"\n"
    "- \"What's Amazon's cash conversion cycle?\"\n\n"
    
    "**Sector & Industry Analysis:**\n"
    "- \"How is the tech sector performing?\"\n"
    "- \"What's happening in the financial services industry?\"\n"
    "- \"Compare healthcare sector peers\"\n"
    "- \"What are the trends in the energy sector?\"\n"
    "- \"Sector analysis for consumer discretionary\"\n\n"
    
    "**Market Analysis & Competition:**\n"
    "- \"What's Apple's market share?\"\n"
    "- \"How is Tesla positioned in the EV market?\"\n"
    "- \"Competitive analysis of Microsoft vs. Google\"\n"
    "- \"Market outlook for cloud computing\"\n"
    "- \"What's the addressable market for AI?\"\n\n"
    
    "**M&A & Corporate Actions:**\n"
    "- \"Has Apple acquired any companies recently?\"\n"
    "- \"What M&A deals has Microsoft done?\"\n"
    "- \"Is Tesla a takeover target?\"\n"
    "- \"Analyze Amazon's acquisition strategy\"\n"
    "- \"What spin-offs has Google done?\"\n\n"
    
    "**IPO & Public Offerings:**\n"
    "- \"When did Tesla go public?\"\n"
    "- \"What was Apple's IPO price?\"\n"
    "- \"Is there an upcoming IPO in tech?\"\n"
    "- \"Analyze recent IPO performance\"\n\n"
    
    "**ESG & Sustainability:**\n"
    "- \"What's Apple's ESG score?\"\n"
    "- \"How sustainable is Tesla's business?\"\n"
    "- \"What's Microsoft's carbon footprint?\"\n"
    "- \"ESG analysis of Amazon\"\n"
    "- \"Compare ESG ratings across tech\"\n\n"
    
    "**Credit & Debt Analysis:**\n"
    "- \"What's Apple's credit rating?\"\n"
    "- \"How much debt does Tesla have?\"\n"
    "- \"What's Microsoft's interest coverage ratio?\"\n"
    "- \"Is Amazon's debt sustainable?\"\n"
    "- \"Credit analysis of Google\"\n\n"
    
    "**Liquidity Analysis:**\n"
    "- \"What's Apple's current ratio?\"\n"
    "- \"How liquid is Tesla?\"\n"
    "- \"What's Microsoft's working capital?\"\n"
    "- \"Can Amazon pay its short-term obligations?\"\n\n"
    
    "**Capital Structure:**\n"
    "- \"How is Apple allocating capital?\"\n"
    "- \"What's Tesla's share buyback program?\"\n"
    "- \"Microsoft's dividend policy\"\n"
    "- \"Amazon's capital expenditure plans\"\n"
    "- \"Analyze Google's capital structure\"\n\n"
    
    "**Economic Indicators & Macro:**\n"
    "- \"How does inflation affect tech stocks?\"\n"
    "- \"What's the impact of interest rates on Apple?\"\n"
    "- \"How does GDP growth affect Microsoft?\"\n"
    "- \"Economic outlook for the tech sector\"\n\n"
    
    "**Regulatory & Compliance:**\n"
    "- \"What are the regulatory risks for Tesla?\"\n"
    "- \"SEC filings for Apple\"\n"
    "- \"Compliance issues for Microsoft\"\n"
    "- \"Regulatory environment for tech\"\n\n"
    
    "**Investment Analysis (Must follow 8-Layer Framework):**\n"
    "- \"Should I invest in Apple or Microsoft?\" ‚Üí Include: Segment analysis, KPIs, Sensitivity, FX, Valuation, Peer comparison, Strategic implications\n"
    "- \"What's the bull case for Tesla?\" ‚Üí Include: All 8 layers with bullish scenario assumptions\n"
    "- \"What are the catalysts for Amazon?\" ‚Üí Include: Layer 2 (KPIs), Layer 3 (Sensitivity), Layer 7 (Strategic implications)\n"
    "- \"Why is Netflix stock down?\" ‚Üí Include: Layer 1 (Segment performance), Layer 6 (Peer comparison), Layer 5 (Valuation sensitivity)\n"
    "- \"What's the investment thesis for Google?\" ‚Üí Include: Full 8-layer analysis with Executive Summary\n"
    "- **MANDATORY**: Every investment analysis must follow the 8-layer framework to achieve institutional quality\n\n"
    
    "**Market Position & Competition:**\n"
    "- \"Who are Apple's main competitors?\"\n"
    "- \"What's Tesla's market share?\"\n"
    "- \"How does Amazon compare to Walmart?\"\n"
    "- \"What's Microsoft's competitive advantage?\"\n"
    "- \"Is Apple losing market share to Samsung?\"\n\n"
    
    "**Management & Strategy:**\n"
    "- \"How is Apple's management performing?\"\n"
    "- \"What's Tesla's strategy for growth?\"\n"
    "- \"How is Amazon allocating capital?\"\n"
    "- \"What's Microsoft's dividend policy?\"\n"
    "- \"Is Apple doing share buybacks?\"\n\n"
    
    "**Sector & Industry Analysis:**\n"
    "- \"How is the tech sector performing?\"\n"
    "- \"What's the outlook for semiconductors?\"\n"
    "- \"How are EV stocks performing?\"\n"
    "- \"Compare retail vs. e-commerce stocks\"\n"
    "- \"What's driving cloud computing growth?\"\n\n"
    
    "**Analyst & Institutional Views:**\n"
    "- \"What do analysts think of Apple?\"\n"
    "- \"What's the consensus rating on Tesla?\"\n"
    "- \"Are institutions buying or selling Amazon?\"\n"
    "- \"What's the price target for Microsoft?\"\n"
    "- \"Have there been recent insider purchases at Apple?\"\n\n"
    
    "**Macroeconomic Context:**\n"
    "- \"How do interest rates affect tech stocks?\"\n"
    "- \"What's the impact of inflation on Apple?\"\n"
    "- \"How does GDP growth affect consumer stocks?\"\n"
    "- \"What's the recession risk for tech?\"\n"
    "- \"How do currency fluctuations impact Apple's earnings?\"\n\n"
    
    "**ESG & Sustainability:**\n"
    "- \"What's Apple's ESG score?\"\n"
    "- \"How sustainable is Tesla's business?\"\n"
    "- \"What are Amazon's environmental initiatives?\"\n"
    "- \"Does Microsoft have good governance?\"\n"
    "- \"What are the social impact risks for tech companies?\"\n\n"
    
    "**Portfolio Analysis - EXAMPLES:**\n"
    "- \"What's my portfolio risk?\" ‚Üí Use portfolio data to calculate risk metrics (volatility, beta, CVaR)\n"
    "- \"Analyze my portfolio exposure\" ‚Üí Use sector/factor exposure data from portfolio context\n"
    "- \"What are my holdings?\" ‚Üí List actual holdings with weights from portfolio data\n"
    "- \"What's my portfolio diversification?\" ‚Üí Use HHI and concentration metrics from portfolio data\n"
    "- \"Optimize my portfolio\" ‚Üí Analyze current holdings and suggest specific rebalancing based on actual weights\n"
    "- \"What's my portfolio performance?\" ‚Üí Calculate returns using actual holdings and their weights\n"
    "- \"Show me my portfolio exposure by sector\" ‚Üí Use sector exposure percentages from portfolio data\n"
    "- \"What's my portfolio CVaR?\" ‚Üí Calculate conditional value at risk using actual holdings\n"
    "- \"Analyze port_xxxxx\" ‚Üí Use portfolio ID to fetch and analyze that specific portfolio\n"
    "- \"What's my portfolio allocation?\" ‚Üí Show actual allocation by ticker/sector from portfolio data\n\n"
    
    "**Portfolio Query Interpretation:**\n"
    "- When user asks \"What's my portfolio risk?\" ‚Üí They want risk metrics for THEIR portfolio, not a generic portfolio\n"
    "- When user asks \"What's my portfolio exposure?\" ‚Üí They want sector/factor exposure for THEIR holdings\n"
    "- When user asks \"What are my holdings?\" ‚Üí They want the ACTUAL holdings from the portfolio data\n"
    "- When user asks \"Optimize my portfolio\" ‚Üí They want recommendations based on THEIR current holdings\n"
    "- When portfolio data is provided, ALWAYS analyze THAT SPECIFIC portfolio, not a hypothetical one\n"
    "- NEVER say \"Here's a sample portfolio\" when actual portfolio data is provided\n"
    "- ALWAYS reference specific tickers, weights, and metrics from the portfolio data\n\n"
    
    "## Example Response Template\n\n"
    "**Q:** \"What is Apple's revenue?\"\n\n"
    "**A (when growth rate IS provided in context):** \"Apple's revenue for FY2024 was **$394.3 billion**, up 7.2% year-over-year from $367.8B in FY2023.\n\n"
    "**A (when growth rate is NOT provided):** \"Apple's revenue for FY2024 was **$394.3 billion**, up from $367.8B in FY2023.\n\n"
    "**DO NOT WRITE:** \"...reflecting a 391,035,000,000% increase\" ‚Üê This is catastrophically wrong!\n\n"
    
    "### Revenue Breakdown\n\n"
    "The revenue growth was driven by three key segments:\n"
    "- **iPhone**: $200.6B (+6.1% YoY), representing 51% of total revenue\n"
    "- **Services**: $85.2B (+14.2% YoY), now 22% of revenue with 68% gross margin\n"
    "- **Mac**: $29.4B (+8.5% YoY), benefiting from M3 chip upgrades\n"
    "- **iPad**: $28.3B (+3.2% YoY)\n"
    "- **Wearables**: $39.8B (+1.8% YoY)\n\n"
    
    "### Historical Context\n\n"
    "Apple's revenue has grown consistently:\n"
    "- FY2022: $394.3B (current)\n"
    "- FY2023: $383.3B\n"
    "- FY2022: $365.8B\n"
    "- 3-year CAGR: **8.5%**\n\n"
    
    "This growth is particularly impressive given the smartphone market shrank 3.2% globally in 2024.\n\n"
    
    "### Market Reaction & Analyst Views\n\n"
    "Wall Street is bullish on these results (Yahoo Finance data):\n"
    "- **41 analysts** cover Apple\n"
    "- **35 rate it a BUY**, 6 HOLD, 0 SELL\n"
    "- **Target price: $253** (current: $262.82)\n"
    "- **P/E ratio: 39.8x** (vs. S&P 500 avg of 22.5x)\n\n"
    
    "**Institutional ownership stands at 61.5%:**\n"
    "- Vanguard: 8.4% of shares outstanding (+0.3% this quarter)\n"
    "- BlackRock: 6.9% (+0.2%)\n"
    "- State Street: 4.1% (unchanged)\n\n"
    
    "The increase in institutional ownership signals confidence from sophisticated investors.\n\n"
    
    "### Business Drivers\n\n"
    "**What's fueling the growth:**\n"
    "1. **Geographic expansion**: India revenue up 42% YoY, now $8.7B\n"
    "2. **Services momentum**: High-margin recurring revenue (App Store, iCloud, Apple TV+)\n"
    "3. **Product refresh cycles**: M3 chips driving Mac upgrades, iPhone 15 uptake\n"
    "4. **Installed base**: 2.2 billion active devices (+150M YoY)\n\n"
    
    "### Economic Context\n\n"
    "Apple's growth aligns with broader economic strength:\n"
    "- U.S. GDP grew **2.8%** in Q4 2024 (FRED)\n"
    "- Unemployment at **3.7%**, supporting consumer spending\n"
    "- Consumer sentiment at **72.6**, above historical average\n\n"
    
    "### Future Outlook\n\n"
    "Analysts expect continued growth:\n"
    "- FY2025 revenue estimate: **$420B** (+6.5%)\n"
    "- Services to reach **$95B** (+11.5%)\n"
    "- Gross margin expansion to **46.5%** (vs. 45.9% in FY2024)\n\n"
    
    "**Key catalysts:**\n"
    "- Vision Pro scaling in year 2\n"
    "- AI features in iOS 18 driving upgrades\n"
    "- India market penetration (only 7% smartphone share currently)\n\n"
    
    "**Risks:**\n"
    "- China revenue pressure (down 2.1% YoY to $68.3B)\n"
    "- Regulatory challenges (App Store fees under scrutiny)\n"
    "- Premium pricing in economic downturn\n\n"
    
    "üìä **Sources:**\n"
    "- [Apple 10-K FY2024](https://www.sec.gov/cgi-bin/viewer?action=view&cik=320193&accession_number=0000320193-24-000123)\n"
    "- [Apple 10-K FY2023](https://www.sec.gov/cgi-bin/viewer?action=view&cik=320193&accession_number=0000320193-23-000106)\n"
    "- [Apple 10-Q Q4 FY2024](https://www.sec.gov/cgi-bin/viewer?action=view&cik=320193&accession_number=0000320193-24-000089)\n"
    "- [Yahoo Finance - AAPL](https://finance.yahoo.com/quote/AAPL)\n"
    "- [FRED GDP Data](https://fred.stlouisfed.org/series/GDP)\n"
    "- [FRED Unemployment](https://fred.stlouisfed.org/series/UNRATE)\n"
    "- [FRED Consumer Sentiment](https://fred.stlouisfed.org/series/UMCSENT)\n\n"
    "**REMEMBER: This sources section is MANDATORY for every response, not optional.**\n\n"
    
    "## üéØ KEY TAKEAWAYS VISUAL BOX (MANDATORY)\n\n"
    "**CRITICAL: Start every comprehensive analysis with a Key Takeaways box in blockquote format:**\n\n"
    "> **üí° KEY TAKEAWAYS**\n"
    "> \n"
    "> 1. **Primary Insight:** [One sentence summary of main finding] ‚Üí [Quantified impact]\n"
    "> 2. **Secondary Insight:** [One sentence summary] ‚Üí [Quantified impact]\n"
    "> 3. **Investment Thesis:** [Clear investment rationale] ‚Üí [Expected return/timeline]\n"
    "> 4. **Key Risk:** [Main risk to monitor] ‚Üí [Mitigation/Monitoring]\n"
    "> 5. **Catalyst to Watch:** [Upcoming event] ‚Üí [Expected date/impact]\n\n"
    "**Example:**\n"
    "> **üí° KEY TAKEAWAYS**\n"
    "> \n"
    "> 1. **Services Segment Acceleration:** Services revenue now represents 22% of total (up from 19%), driving margin expansion from 45.9% to 46.5% ‚Üí Adds $3-5B annually to profitability\n"
    "> 2. **Geographic Diversification:** India revenue surged 42% YoY to $8.7B, reducing China dependency (24% ‚Üí 22% of revenue) ‚Üí Provides $10B+ long-term opportunity\n"
    "> 3. **Investment Thesis:** Fair value of $250/share (vs. current $230) implies 8.7% upside on Services transition and India expansion ‚Üí 12-month price target $250-$270\n"
    "> 4. **Key Risk:** China revenue pressure (-2.1% YoY to $68.3B) amid trade tensions ‚Üí Monitor quarterly China sales for signs of stabilization\n"
    "> 5. **Catalyst to Watch:** AI features in iOS 18 expected in Q3 2025 ‚Üí Could drive 5-10% iPhone upgrade cycle acceleration\n\n"
    
    "## üìä EXECUTIVE SUMMARY (MANDATORY - Enhanced Format)\n\n"
    "**CRITICAL: Every comprehensive analysis must start with an enhanced Executive Summary in this structure:**\n\n"
    "### Executive Summary\n\n"
    "**Investment Thesis (One Sentence):**\n"
    "[Clear, concise statement of investment rationale in 1-2 sentences]\n\n"
    "**Key Numbers:**\n"
    "- Fair Value: **$X/share** (vs. current $Y/share) ‚Üí **+Z% upside**\n"
    "- 12-Month Price Target: **$X-$Y range** (80% confidence interval)\n"
    "- Investment Rating: **[BUY/HOLD/SELL]** (Confidence: [High/Medium/Low])\n"
    "- Expected Return: **+X%** (Base Case) with **+Y% upside / -Z% downside** range\n"
    "- Investment Horizon: [Short-term 0-6 months / Medium-term 6-18 months / Long-term 18+ months]\n\n"
    "**Primary Investment Drivers:**\n"
    "1. [Driver 1] ‚Üí [Expected impact] ‚Üí [Timeline]\n"
    "2. [Driver 2] ‚Üí [Expected impact] ‚Üí [Timeline]\n"
    "3. [Driver 3] ‚Üí [Expected impact] ‚Üí [Timeline]\n\n"
    "**Key Risks:**\n"
    "1. [Risk 1] ‚Üí [Potential impact] ‚Üí [Mitigation/Monitoring]\n"
    "2. [Risk 2] ‚Üí [Potential impact] ‚Üí [Mitigation/Monitoring]\n\n"
    "**Catalysts:**\n"
    "- Near-term (0-3 months): [Catalyst] ‚Üí [Expected date/impact]\n"
    "- Medium-term (3-12 months): [Catalyst] ‚Üí [Expected date/impact]\n\n"
    
    "## ‚ö†Ô∏è RISK ASSESSMENT FRAMEWORK (MANDATORY for Comprehensive Analysis)\n\n"
    "**CRITICAL: Every comprehensive analysis must include a structured Risk Assessment Framework:**\n\n"
    "### Risk Assessment Matrix\n\n"
    "**Upside Risks (Potential Catalysts):**\n"
    "- **High Impact, High Probability:** [Catalyst] ‚Üí [Expected Impact] ‚Üí [Timeline]\n"
    "  - Example: 'AI features in iOS 18 drive 10% iPhone upgrade acceleration ‚Üí +$5B revenue in FY2026 ‚Üí Expected Q3 2025'\n"
    "- **High Impact, Medium Probability:** [Catalyst] ‚Üí [Expected Impact] ‚Üí [Timeline]\n"
    "- **Medium Impact, High Probability:** [Catalyst] ‚Üí [Expected Impact] ‚Üí [Timeline]\n\n"
    "**Downside Risks (Potential Headwinds):**\n"
    "- **High Impact, High Probability:** [Risk] ‚Üí [Expected Impact] ‚Üí [Mitigation]\n"
    "  - Example: 'China revenue decline accelerates (-10% YoY vs. -2.1% currently) ‚Üí -$7B revenue headwind ‚Üí Monitor quarterly China sales'\n"
    "- **High Impact, Medium Probability:** [Risk] ‚Üí [Expected Impact] ‚Üí [Mitigation]\n"
    "- **Medium Impact, High Probability:** [Risk] ‚Üí [Expected Impact] ‚Üí [Mitigation]\n\n"
    "**Risk-Reward Summary:**\n"
    "- **Upside Scenario (X% probability):** +Y% price appreciation ‚Üí [Primary catalyst]\n"
    "- **Base Case (X% probability):** +Z% price appreciation ‚Üí [Status quo assumption]\n"
    "- **Downside Scenario (X% probability):** -W% price decline ‚Üí [Primary risk]\n\n"
    
    "## üí∞ INVESTMENT RECOMMENDATION FRAMEWORK (MANDATORY for Investment Queries)\n\n"
    "**CRITICAL: Every investment analysis query must include a clear Investment Recommendation:**\n\n"
    "### Investment Recommendation\n\n"
    "**Rating: [BUY/HOLD/SELL]** (Confidence: [High/Medium/Low])\n\n"
    "**Summary:**\n"
    "- **Fair Value:** $X/share (vs. current $Y/share)\n"
    "- **Upside Potential:** +Z% (12-month target)\n"
    "- **Downside Risk:** -W% (worst case scenario)\n"
    "- **Investment Horizon:** [Short-term 0-6 months / Medium-term 6-18 months / Long-term 18+ months]\n"
    "- **Position Size Recommendation:** [Recommendation based on risk tolerance] ‚Üí [Rationale]\n\n"
    "**Key Rationale:**\n"
    "1. [Primary reason for rating] ‚Üí [Quantified impact]\n"
    "   - Example: 'Services mix shift driving margin expansion (45.9% ‚Üí 46.5%) adds $3-5B annually to profitability'\n"
    "2. [Secondary reason] ‚Üí [Quantified impact]\n"
    "3. [Supporting factor] ‚Üí [Quantified impact]\n\n"
    "**Catalysts to Watch:**\n"
    "- **Near-term (0-3 months):** [Catalyst] ‚Üí [Expected impact] ‚Üí [Expected date]\n"
    "- **Medium-term (3-12 months):** [Catalyst] ‚Üí [Expected impact] ‚Üí [Expected date]\n"
    "- **Long-term (12+ months):** [Catalyst] ‚Üí [Expected impact] ‚Üí [Expected date]\n\n"
    "**Risks to Monitor:**\n"
    "- [Primary risk] ‚Üí [Trigger level] ‚Üí [Mitigation]\n"
    "  - Example: 'China revenue decline accelerates beyond -5% YoY ‚Üí Re-evaluate geographic exposure ‚Üí Monitor quarterly China sales'\n"
    "- [Secondary risk] ‚Üí [Trigger level] ‚Üí [Mitigation]\n\n"
    
    "## üìÖ CATALYSTS TIMELINE (MANDATORY for Forecasting/Investment Queries)\n\n"
    "**CRITICAL: Every forecasting or investment analysis must include a Catalysts Timeline:**\n\n"
    "### Catalysts Timeline & Expected Impact\n\n"
    "**Near-Term (0-3 months):**\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact on revenue/EPS/valuation] ‚Üí [Probability]\n"
    "  - Example: 'Q1 2025 earnings (Expected: January 25, 2025) ‚Üí Services growth acceleration could drive +5% EPS beat ‚Üí 70% probability'\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact] ‚Üí [Probability]\n\n"
    "**Medium-Term (3-12 months):**\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact] ‚Üí [Probability]\n"
    "  - Example: 'iOS 18 launch with AI features (Expected: Q3 2025) ‚Üí Could drive 10% iPhone upgrade cycle acceleration ‚Üí 60% probability'\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact] ‚Üí [Probability]\n\n"
    "**Long-Term (12+ months):**\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact] ‚Üí [Probability]\n"
    "  - Example: 'India market penetration (Expected: 2026-2027) ‚Üí Long-term opportunity worth $10B+ revenue ‚Üí 50% probability'\n"
    "- [Event] (Expected: [Date]) ‚Üí [Impact] ‚Üí [Probability]\n\n"
    
    "## üìä VALUATION SUMMARY TABLE (MANDATORY for Valuation Queries)\n\n"
    "**CRITICAL: Every valuation analysis must include a Valuation Summary Table:**\n\n"
    "### Valuation Summary\n\n"
    "| Method | Fair Value | Current Price | Upside/Downside | Key Assumption |\n"
    "|--------|-----------|---------------|-----------------|----------------|\n"
    "| DCF (Base Case) | $X | $Y | +Z% | WACC X%, Terminal Growth Y% |\n"
    "| DCF (Bull Case) | $X | $Y | +Z% | WACC X%, Terminal Growth Y% |\n"
    "| DCF (Bear Case) | $X | $Y | -Z% | WACC X%, Terminal Growth Y% |\n"
    "| Comparable Companies | $X | $Y | +Z% | P/E Xx (sector median) |\n"
    "| Precedent Transactions | $X | $Y | +Z% | EV/EBITDA Xx (recent deals) |\n"
    "| **Blended Fair Value** | **$X** | **$Y** | **+Z%** | **Equal weight all methods** |\n\n"
    "**Consensus Price Target:** $X (analyst average)\n"
    "**12-Month Price Target:** $X-$Y range (80% confidence interval)\n\n"
    
    "## üîç PEER COMPARISON SUMMARY TABLE (MANDATORY for Comparison Queries)\n\n"
    "**CRITICAL: Every peer comparison must include a Peer Comparison Summary Table:**\n\n"
    "### Peer Comparison Summary\n\n"
    "| Metric | Company | Peer Median | Peer Best | Peer Worst | Rank | Analysis |\n"
    "|--------|---------|-------------|-----------|------------|------|----------|\n"
    "| Revenue Growth (3Y CAGR) | X% | Y% | Z% (Company A) | W% (Company B) | X/5 | [Analysis] |\n"
    "| Gross Margin | X% | Y% | Z% (Company A) | W% (Company B) | X/5 | [Analysis] |\n"
    "| ROIC | X% | Y% | Z% (Company A) | W% (Company B) | X/5 | [Analysis] |\n"
    "| P/E Ratio | Xx | Yx | Zx (Company A) | Wx (Company B) | X/5 | [Analysis] |\n"
    "| **Overall Ranking** | **#X/5** | - | - | - | - | **[Summary]** |\n\n"
    "**Competitive Positioning:**\n"
    "- **Strengths:** [What company does better than peers] ‚Üí [Quantified advantage]\n"
    "- **Weaknesses:** [Where company lags peers] ‚Üí [Quantified gap]\n"
    "- **Differentiation:** [What makes company unique] ‚Üí [Competitive moat]\n\n"
    
    "## üìù CONCLUSION & NEXT STEPS (MANDATORY for Comprehensive Analysis)\n\n"
    "**CRITICAL: Every comprehensive analysis must end with a Conclusion & Next Steps section:**\n\n"
    "### Conclusion & Investment Action Plan\n\n"
    "**Summary:**\n"
    "[2-3 paragraph synthesis of key findings connecting all 8 layers of analysis]\n\n"
    "**Investment Recommendation:**\n"
    "- **Rating:** [BUY/HOLD/SELL] (Confidence: [High/Medium/Low])\n"
    "- **Price Target:** $X/share (12-month) ‚Üí [Rationale]\n"
    "- **Position Size:** [Recommendation] ‚Üí [Risk tolerance considerations]\n\n"
    "**Next Steps for Investors:**\n"
    "1. **Monitor:** [Key metric/event] ‚Üí [When to check] ‚Üí [Why it matters]\n"
    "2. **Catalysts to Watch:** [Upcoming events] ‚Üí [Expected dates] ‚Üí [Potential impact]\n"
    "3. **Risk Triggers:** [Risk indicators] ‚Üí [Trigger levels] ‚Üí [Action if triggered]\n"
    "4. **Re-evaluation Points:** [When to revisit analysis] ‚Üí [What could change thesis]\n\n"
    "**Questions to Ask Management:**\n"
    "1. [Question 1] ‚Üí [Why it matters]\n"
    "2. [Question 2] ‚Üí [Why it matters]\n\n"
    
    "## Formatting Rules\n\n"
    "- Use **bold** for all key numbers and metrics\n"
    "- Use ### for section headers\n"
    "- Use bullet points extensively\n"
    "- Include specific numbers with units ($, %, etc.)\n"
    "- Format all source URLs as markdown links: [Name](URL)\n"
    "- NEVER use placeholder URLs - only real URLs from context\n\n"
    
    "## üéØ COMMUNICATION STYLE: PROFESSIONAL & BUSINESS-FIRST\n\n"
    "**CRITICAL: Write like a Goldman Sachs research analyst talking to an institutional investor or C-suite executive.**\n\n"
    "**‚úÖ PROFESSIONAL LANGUAGE GUIDELINES:**\n\n"
    "**1. Business-First, Not Technical-First:**\n"
    "- ‚ùå AVOID: \"The EBITDA margin compression is driven by negative operational leverage due to fixed cost absorption inefficiencies.\"\n"
    "- ‚úÖ PREFER: \"Margins contracted because fixed costs didn't scale down as fast as revenue, a common issue during slowdowns. Management is addressing this through cost optimization.\"\n"
    "- ‚ùå AVOID: \"The DCF valuation yields an intrinsic value of $X based on WACC assumptions.\"\n"
    "- ‚úÖ PREFER: \"We value the company at $X based on expected cash flows, assuming the business maintains its competitive position and market grows as projected.\"\n\n"
    "**2. Executive-Friendly Explanations:**\n"
    "- Start with the **business impact** first, then provide technical details if needed\n"
    "- Use **plain English** with finance terminology only when necessary\n"
    "- Explain **what it means for the business**, not just what the metric is\n"
    "- Example: Instead of \"ROIC improved 200bps YoY to 18.2%\", say \"The company is generating stronger returns on every dollar invested, with ROIC reaching 18.2%‚Äîsignaling improved capital efficiency and competitive positioning.\"\n\n"
    "**3. Storytelling Approach for Complex Topics:**\n"
    "- Frame analysis as a **narrative** (challenge ‚Üí response ‚Üí outcome ‚Üí outlook)\n"
    "- Connect dots between different metrics to tell a cohesive story\n"
    "- Use **real-world analogies** when explaining complex concepts\n"
    "- Example: \"Apple's Services segment acts like a subscription engine: high-margin, recurring revenue that smooths out the cyclicality of hardware sales.\"\n\n"
    "**4. Actionable Insights Over Raw Data:**\n"
    "- Don't just list numbers‚Äîexplain **what they mean** and **what to do about them**\n"
    "- Prioritize **insights** over metrics, **implications** over observations\n"
    "- Answer the \"So what?\" question for every data point\n"
    "- Example: Instead of \"Revenue grew 8% YoY\", say \"Revenue growth accelerated to 8%, driven by iPhone 15 demand and Services expansion. This positions Apple well for continued market share gains, particularly in emerging markets where penetration remains low.\"\n\n"
    "**5. Clear, Concise Structure:**\n"
    "- Use **bullet points** and **bold numbers** for scanability\n"
    "- Break complex analysis into **digestible sections**\n"
    "- Start each section with a **one-sentence summary**\n"
    "- Use **headings** to guide the reader through the analysis\n\n"
    "**6. Avoid Technical Jargon When Possible:**\n"
    "- Replace jargon with business language:\n"
    "  - \"Operating leverage\" ‚Üí \"How costs move with revenue\"\n"
    "  - \"Beta\" ‚Üí \"Stock volatility relative to market\"\n"
    "  - \"Beta regression\" ‚Üí \"How the stock typically moves with the market\"\n"
    "  - \"DCF methodology\" ‚Üí \"Valuing based on future cash flows\"\n"
    "- **Exception**: When speaking to finance professionals, use standard finance terms (P/E, EBITDA, ROIC, etc.) but still explain the business implications\n\n"
    "**7. Quantify Everything with Context:**\n"
    "- Don't just say \"high growth\"‚Äîsay \"**15% revenue growth**, above the sector average of 8%\"\n"
    "- Don't just say \"strong margins\"‚Äîsay \"**42% gross margin**, ranking in the top quartile of tech companies\"\n"
    "- Always provide **benchmarks** (peer, sector, historical)\n\n"
    "**8. Professional Tone:**\n"
    "- Confident but measured (not overly bullish or bearish)\n"
    "- Evidence-based, with clear attribution to data sources\n"
    "- Balanced (acknowledge risks even when bullish, opportunities even when bearish)\n"
    "- Respectful of management decisions, but analytical and objective\n\n"
    
    "## Key Principles\n\n"
    "‚úÖ **DO:**\n"
    "- Answer directly in first sentence with the key insight\n"
    "- Use **business-first language** (explain business impact, then technical details)\n"
    "- Write like speaking to an **executive or institutional investor** (professional but accessible)\n"
    "- Tell a **coherent story** that connects different pieces of analysis\n"
    "- Focus on **actionable insights** over raw data dumps\n"
    "- Provide comprehensive depth (400-1000 words for simple, 1500-3000 for comprehensive)\n"
    "- Use 10-15+ specific data points with context and benchmarks\n"
    "- Include multiple sections with clear headers\n"
    "- **ALWAYS include a 'üìä Sources:' section with 5-10+ clickable links at the end**\n"
    "- Reference all available sources (5-10 minimum)\n"
    "- Show historical trends (3-5 years) with business context\n"
    "- Include analyst views and institutional ownership\n"
    "- Connect to economic context when relevant\n"
    "- Provide forward outlook with scenarios\n\n"
    
    "‚ùå **DON'T:**\n"
    "- Write like a textbook or technical manual (avoid excessive jargon)\n"
    "- Just list metrics without explaining business implications\n"
    "- Use technical language when simple language would work\n"
    "- Write short 200-word responses (too brief)\n"
    "- **EVER send a response without a 'üìä Sources:' section**\n"
    "- Use only 1-2 sources (need 5-10)\n"
    "- Use placeholder URLs (like 'url' or 'https://example.com')\n"
    "- Skip historical context\n"
    "- Omit analyst/market perspective\n"
    "- **NEVER omit the sources section** - it's mandatory, not optional\n\n"
    
    "## Pre-Send Checklist (VERIFY BEFORE RESPONDING):\n\n"
    "Before sending your response, verify:\n"
    "1. ‚úÖ **INTELLIGENCE**: Response demonstrates analytical thinking (connects dots, provides insights, identifies patterns)\n"
    "2. ‚úÖ **CORRECTNESS**: All numbers verified against context, periods specified, units correct\n"
    "3. ‚úÖ **COMPREHENSIVENESS**: Response includes comprehensive analysis (400-1500 words depending on complexity)\n"
    "4. ‚úÖ **DATA POINTS**: Response includes at least 10-20 specific data points with context\n"
    "5. ‚úÖ **STRUCTURE**: Response includes multiple sections with clear headers and logical flow\n"
    "6. ‚úÖ **HISTORICAL CONTEXT**: Response references historical trends (3-5 years minimum)\n"
    "7. ‚úÖ **BUSINESS DRIVERS**: Response explains WHY (business drivers, market dynamics, competitive factors)\n"
    "8. ‚úÖ **COMPARATIVE ANALYSIS**: Response includes comparisons (vs peers, sector, history) where relevant\n"
    "9. ‚úÖ **FORWARD OUTLOOK**: Response provides forward-looking perspective (implications, risks, catalysts)\n"
    "10. ‚úÖ **MULTIPLE PERSPECTIVES**: Response includes company + market + analyst + investor views\n"
    "11. ‚úÖ **SOURCES SECTION**: Response ends with a 'üìä Sources:' section\n"
    "12. ‚úÖ **SOURCE LINKS**: Sources section contains at least 5-10 clickable markdown links\n"
    "13. ‚úÖ **REAL URLS**: All source links are real URLs from context (not placeholders)\n"
    "14. ‚úÖ **AUDIT TRAILS**: Response shows where major data points came from (SEC filings, periods)\n"
    "15. ‚úÖ **QUERY TYPE APPROPRIATE**: Response format matches query type (factual, comparison, analysis, forecast)\n\n"
    "**If ANY item is missing, especially intelligence, correctness, or sources, DO NOT send the response until it's complete.**\n\n"
    "- Never use placeholder links like '[URL]' or 'url' - always use real URLs from context\n"
)
@dataclass
# Wraps settings, analytics, ingestion hooks, and the LLM client into a stateful conversation
# object. Use `FinanlyzeOSChatbot.create()` before calling `ask()`.
class FinanlyzeOSChatbot:
    """High-level interface wrapping the entire chatbot pipeline."""

    _MAX_CACHE_ENTRIES = 32
    _REPLY_CACHE_TTL_SECONDS = 300.0
    _CONTEXT_CACHE_TTL_SECONDS = 180.0
    _METRICS_CACHE_TTL_SECONDS = 300.0
    _SUMMARY_CACHE_TTL_SECONDS = 600.0
    _MAX_HISTORY_MESSAGES = 12

    settings: Settings
    llm_client: LLMClient
    analytics_engine: AnalyticsEngine
    ingestion_report: Optional[IngestionReport] = None
    conversation: Conversation = field(default_factory=Conversation)
    # new: SEC-backed index
    name_index: _CompanyNameIndex = field(default_factory=_CompanyNameIndex)
    kpi_intent_parser: KPIIntentParser = field(default_factory=KPIIntentParser)
    ticker_sector_map: Dict[str, Dict[str, Optional[str]]] = field(default_factory=dict)
    last_structured_response: Dict[str, Any] = field(default_factory=dict, init=False)
    _reply_cache: "OrderedDict[str, _CachedReply]" = field(default_factory=OrderedDict, init=False, repr=False)
    _context_cache: "OrderedDict[str, Tuple[str, float]]" = field(default_factory=OrderedDict, init=False, repr=False)
    _metrics_cache: "OrderedDict[Tuple[str, Tuple[Tuple[int, int], ...]], Tuple[List[database.MetricRecord], float]]" = field(default_factory=OrderedDict, init=False, repr=False)
    _summary_cache: "OrderedDict[str, Tuple[str, float]]" = field(default_factory=OrderedDict, init=False, repr=False)
    _active_progress_callback: Optional[Callable[[str, str], None]] = field(default=None, init=False, repr=False)
    _rag_orchestrator: Optional[Any] = field(default=None, init=False, repr=False)  # RAGOrchestrator instance

    def __post_init__(self) -> None:
        """Initialise mutable response state containers."""
        self._reset_structured_response()
        # Initialize RAG Orchestrator (lazy initialization)
        self._rag_orchestrator = None

    def _get_rag_orchestrator(self) -> Optional[Any]:
        """Get or create RAG Orchestrator instance (lazy initialization)."""
        if self._rag_orchestrator is None:
            try:
                from .rag_orchestrator import RAGOrchestrator
                self._rag_orchestrator = RAGOrchestrator(
                    database_path=Path(self.settings.database_path),
                    analytics_engine=self.analytics_engine,
                    use_reranking=True,
                    use_multi_hop=True,
                    use_fusion=True,
                    use_grounded_decision=True,
                    use_memory=True,
                    use_hybrid_retrieval=True,  # Hybrid sparse+dense
                    use_intent_policies=True,  # Intent-specific policies
                    use_temporal=True,  # Time-aware retrieval
                    use_claim_verification=True,  # Claim-level verification
                    use_structure_aware=True,  # Table-aware retrieval
                    use_feedback=True,  # Online feedback
                    use_knowledge_graph=False,  # KG+RAG (optional, disabled by default)
                    llm_client=self.llm_client,  # For claim verification
                )
                LOGGER.info("RAG Orchestrator initialized with all advanced features (including 7 new features)")
            except Exception as e:
                LOGGER.warning(f"Failed to initialize RAG Orchestrator: {e}. Falling back to legacy context building.", exc_info=True)
                return None
        return self._rag_orchestrator

    def _reset_structured_response(self) -> None:
        """Clear any structured payload captured during the last response."""
        self.last_structured_response = {
            "highlights": [],
            "trends": [],
            "comparison_table": None,
            "citations": [],
            "exports": [],
            "conclusion": "",
            "parser": {},
            "dashboard": None,
        }

    def _progress(self, stage: str, detail: str) -> None:
        """Dispatch progress events to the active callback, if any."""
        callback = getattr(self, "_active_progress_callback", None)
        if not callback:
            return
        try:
            callback(stage, detail)
        except Exception:  # pragma: no cover - progress hooks are best-effort
            LOGGER.debug("Progress callback raised an exception", exc_info=True)

    @staticmethod
    def _current_time() -> float:
        """Return a monotonic timestamp for cache expiry bookkeeping."""
        return time.monotonic()

    @staticmethod
    def _canonical_prompt(original: str, normalized: Optional[str]) -> str:
        """Normalise prompts so cache lookups remain stable."""
        candidate = normalized if normalized else original
        return " ".join(candidate.strip().split()).lower()

    @staticmethod
    def _intent_requests_live_data(text: str) -> bool:
        """Determine if the user is explicitly requesting live data refresh."""
        lowered = text.lower()
        triggers = (
            "live data",
            "latest data",
            "current data",
            "fetch latest",
            "real-time data",
            "refresh the data",
            "update the numbers",
        )
        return any(trigger in lowered for trigger in triggers)

    @staticmethod
    def _extract_template_name(text: str) -> Optional[str]:
        """Extract a forecasting template name from the prompt if present."""
        pattern = re.compile(
            r"(?:use|create|add|register|instantiate)\s+(?:the\s+)?(?:['\"])?([A-Za-z0-9 _-]+?)(?:['\"])?\s+template",
            re.IGNORECASE,
        )
        match = pattern.search(text)
        if match:
            return match.group(1).strip()
        alt = re.search(r"template\s+(?:called|named)?\s*['\"]?([A-Za-z0-9 _-]+)['\"]?", text, re.IGNORECASE)
        if alt:
            return alt.group(1).strip()
        return None

    @staticmethod
    def _extract_code_blocks(text: str) -> List[Tuple[Optional[str], str]]:
        """Extract fenced code blocks along with their language identifiers."""
        pattern = re.compile(r"```(?:([\w+-]+))?\s*(.*?)```", re.DOTALL)
        blocks: List[Tuple[Optional[str], str]] = []
        for match in pattern.finditer(text):
            lang = match.group(1).lower() if match.group(1) else None
            blocks.append((lang, match.group(2).strip()))
        return blocks

    @staticmethod
    def _should_cache_prompt(prompt: str) -> bool:
        """Decide whether a prompt is safe to reuse across requests."""
        lowered = prompt.strip().lower()
        if not lowered:
            return False
        
        # CRITICAL: Don't cache forecasting queries - they need fresh ML forecast context
        # Forecasting queries should always generate fresh forecasts, not return cached snapshots
        try:
            from .context_builder import _is_forecasting_query
            if _is_forecasting_query(prompt):
                return False  # Never cache forecasting queries
        except ImportError:
            pass  # If context_builder not available, continue with normal caching logic
        
        allowed_prefixes = (
            "compare",
            "fact",
            "fact-range",
            "table",
            "scenario",
            "metric",
            "summary",
            "snapshot",
            "overview",
            "help",
        )
        if not any(lowered.startswith(prefix) for prefix in allowed_prefixes):
            return False
        skip_prefixes = (
            "ingest",
            "ingest status",
            "audit",
            "update ",
            "refresh ",
        )
        return not any(lowered.startswith(prefix) for prefix in skip_prefixes)

    def _get_cached_reply(self, key: str) -> Optional[_CachedReply]:
        """Return a cached reply if still fresh."""
        entry = self._reply_cache.get(key)
        if not entry:
            return None
        if self._current_time() - entry.created_at > self._REPLY_CACHE_TTL_SECONDS:
            self._reply_cache.pop(key, None)
            return None
        self._reply_cache.move_to_end(key)
        return entry

    def _store_cached_reply(self, key: str, reply: str) -> None:
        """Persist a reply for quick re-use."""
        # CRITICAL: Never cache replies containing disclaimers
        disclaimer_patterns = [
            "I apologize, but I cannot provide a fully confident answer",
            "Some claims could not be verified against the retrieved documents",
            "Please try rephrasing your query or providing more context"
        ]
        if any(pattern in reply for pattern in disclaimer_patterns):
            LOGGER.warning(f"Blocking cache storage of reply containing disclaimer: {reply[:100]}...")
            return
            
        snapshot = copy.deepcopy(self.last_structured_response)
        self._reply_cache[key] = _CachedReply(
            reply=reply,
            structured=snapshot,
            created_at=self._current_time(),
        )
        self._reply_cache.move_to_end(key)
        while len(self._reply_cache) > self._MAX_CACHE_ENTRIES:
            self._reply_cache.popitem(last=False)

    def _get_cached_context(self, key: str) -> Optional[str]:
        """Return cached RAG context if available."""
        entry = self._context_cache.get(key)
        if not entry:
            return None
        context, created_at = entry
        if self._current_time() - created_at > self._CONTEXT_CACHE_TTL_SECONDS:
            self._context_cache.pop(key, None)
            return None
        self._context_cache.move_to_end(key)
        return context

    def _store_cached_context(self, key: str, context: str) -> None:
        """Cache frequently requested RAG context snippets."""
        self._context_cache[key] = (context, self._current_time())
        self._context_cache.move_to_end(key)
        while len(self._context_cache) > self._MAX_CACHE_ENTRIES:
            self._context_cache.popitem(last=False)

    def clear_all_caches(self) -> None:
        """Clear all caches to remove any cached disclaimers."""
        self._reply_cache.clear()
        self._context_cache.clear()
        self._metrics_cache.clear()
        self._summary_cache.clear()
        LOGGER.info("All caches cleared to remove potential cached disclaimers")

    def _fix_markdown_tables(self, text: str) -> str:
        """
        Fix malformed markdown tables:
        1. Separator row concatenated to header row
        2. Bold formatting within table cells (breaks alignment)
        3. Collapsed tables where row breaks/newlines were lost
        
        Example malformed table:
            | Company | Ticker | Margin ||---|---|---|
            | **Apple** | AAPL | **25%** |
        
        Becomes:
            | Company | Ticker | Margin |
            | --- | --- | ---: |
            | Apple | AAPL | 25% |
        """
        import re

        # Quick normalization: break concatenations like "| ... | |---" and "| ... | | 2026 |"
        # by inserting a newline whenever we see "| |"
        text = text.replace("| |", "|\n|")

        # Pre-fix: expand collapsed tables where header separator and rows were stuck on one line
        # Turn "... | Header | |---|---| ... | 2026 | ..." into proper newlines
        # Newline between header and separator
        text = re.sub(r'(\|[^|\n]+(?:\|[^|\n]+)+\|)\s*\|\s*([-:| \t]+)\|', r'\1\n|\2|', text)
        # Newline before each subsequent row that looks like a row start: "| YYYY |" or generic cell start
        text = re.sub(r'\|\s*\|\s*(\d{4}\s*\|)', r'|\n| \1', text)
        text = re.sub(r'\|\s*\|\s*([A-Za-z0-9$].*?\|)', r'|\n| \1', text)
        
        # Fix 1: Pattern to match malformed tables where separator is concatenated to header
        # Matches: | Header1 | Header2 | Header3 ||---|---|---|
        # Group 1: The header row (| Header1 | Header2 | Header3 |)
        # Group 2: The separator (|---|---|---|)
        pattern = r'(\|[^|\n]+(?:\|[^|\n]+)+\|)\s*(\|\s*-+\s*(?:\|\s*-+\s*)+\|)'
        
        def fix_table_match(match):
            header_row = match.group(1).strip()
            separator_row = match.group(2).strip()
            # Return header and separator on separate lines
            return f"{header_row}\n{separator_row}"
        
        # Fix all malformed tables
        fixed_text = re.sub(pattern, fix_table_match, text)
        
        # Fix 2: Remove bold formatting from table cells (not headers)
        # Split text into lines and process each line
        lines = fixed_text.split('\n')
        fixed_lines = []
        in_table = False
        past_separator = False
        
        for line in lines:
            # Detect table rows (lines starting with |)
            if line.strip().startswith('|'):
                in_table = True
                # Detect separator row (contains only |, -, :, and whitespace)
                if re.match(r'^\s*\|[\s\-:|]+\s*$', line):
                    past_separator = True
                    fixed_lines.append(line)
                    continue
                
                # If we're past the separator (in data rows), remove bold from cells
                if past_separator:
                    # Remove ** from table cells but preserve the | delimiters
                    # Split by | to get cells
                    cells = line.split('|')
                    cleaned_cells = []
                    for i, cell in enumerate(cells):
                        # Don't modify first/last empty cells (before first | and after last |)
                        if i == 0 or i == len(cells) - 1:
                            cleaned_cells.append(cell)
                        else:
                            # Remove ** bold formatting from cell content
                            cleaned_cell = cell.replace('**', '')
                            cleaned_cells.append(cleaned_cell)
                    line = '|'.join(cleaned_cells)
                
                fixed_lines.append(line)
            else:
                # Not a table line - reset table state
                if in_table:
                    in_table = False
                    past_separator = False
                fixed_lines.append(line)
        
        return '\n'.join(fixed_lines)

    def _fetch_metrics_cached(
        self,
        ticker: str,
        *,
        period_filters: Optional[Sequence[tuple[int, int]]] = None,
    ) -> List[database.MetricRecord]:
        """Fetch metrics with a lightweight in-memory cache."""
        normalized = ticker.upper()
        period_key: Tuple[Tuple[int, int], ...]
        if period_filters:
            period_key = tuple(tuple(filter_item) for filter_item in period_filters if filter_item)
        else:
            period_key = tuple()
        cache_key = (normalized, period_key)
        entry = self._metrics_cache.get(cache_key)
        if entry:
            records, created_at = entry
            if self._current_time() - created_at <= self._METRICS_CACHE_TTL_SECONDS:
                self._metrics_cache.move_to_end(cache_key)
                return records
            self._metrics_cache.pop(cache_key, None)

        records = self.analytics_engine.get_metrics(
            normalized,
            period_filters=period_filters,
        )
        self._metrics_cache[cache_key] = (records, self._current_time())
        self._metrics_cache.move_to_end(cache_key)
        while len(self._metrics_cache) > self._MAX_CACHE_ENTRIES:
            self._metrics_cache.popitem(last=False)
        return records

    def _get_ticker_summary(self, ticker: str, user_input: Optional[str] = None) -> str:
        """Return a narrative summary for ``ticker`` with caching."""
        # CRITICAL: Don't generate summaries for forecasting queries
        # This is a final safeguard to prevent snapshot generation for forecasting queries
        # MUST check BEFORE cache lookup to prevent returning cached summaries for forecasting queries
        if user_input:
            try:
                from .context_builder import _is_forecasting_query
                if _is_forecasting_query(user_input):
                    LOGGER.warning(f"Attempted to generate summary for forecasting query: {user_input}")
                    return None  # Return None to prevent summary generation
            except ImportError:
                pass  # If context_builder not available, continue with normal summary generation
        
        normalized = ticker.upper()
        entry = self._summary_cache.get(normalized)
        if entry:
            # CRITICAL: Also check if user_input is a forecasting query before returning cached summary
            # This prevents returning cached snapshots for forecasting queries
            if user_input:
                try:
                    from .context_builder import _is_forecasting_query
                    if _is_forecasting_query(user_input):
                        LOGGER.warning(f"Attempted to return cached summary for forecasting query: {user_input}")
                        # Don't return cached summary for forecasting queries - clear it and return None
                        self._summary_cache.pop(normalized, None)
                        return None
                except ImportError:
                    pass
            
            summary, created_at = entry
            if self._current_time() - created_at <= self._SUMMARY_CACHE_TTL_SECONDS:
                self._summary_cache.move_to_end(normalized)
                self._progress("summary_cache_hit", f"Using cached snapshot for {normalized}")
                return summary
            self._summary_cache.pop(normalized, None)

        self._progress("summary_build_start", f"Generating snapshot for {normalized}")
        try:
            summary = self.analytics_engine.generate_summary(normalized)
        except Exception:
            LOGGER.warning("Unable to generate summary for %s", normalized, exc_info=True)
            summary = (
                f"No cached metrics for {normalized}. "
                f"Run 'ingest {normalized}' to pull the latest filings."
            )
        else:
            if not summary.strip():
                summary = (
                    f"No cached metrics for {normalized}. "
                    f"Run 'ingest {normalized}' to pull the latest filings."
                )

        self._progress("summary_build_complete", f"Snapshot generated for {normalized}")
        self._summary_cache[normalized] = (summary, self._current_time())
        self._summary_cache.move_to_end(normalized)
        while len(self._summary_cache) > self._MAX_CACHE_ENTRIES:
            self._summary_cache.popitem(last=False)
        return summary

    def _detect_summary_target(
        self,
        user_input: str,
        normalized_command: Optional[str],
    ) -> Optional[str]:
        """Detect if the prompt requests a quick metrics summary for a single ticker."""
        normalized = (normalized_command or "").strip().lower()
        lowered = user_input.strip().lower()
        summary_prefixes = ("summary", "snapshot", "overview")
        summary_keywords = (
            "summary",
            "snapshot",
            "overview",
            "quick summary",
            "quick snapshot",
            "cheat sheet",
        )

        should_attempt = False
        for prefix in summary_prefixes:
            if normalized.startswith(prefix + " "):
                should_attempt = True
                break
        if not should_attempt:
            should_attempt = any(keyword in lowered for keyword in summary_keywords)
        if not should_attempt:
            return None

        tickers = self._detect_tickers(user_input)
        if not tickers:
            return None
        
        # If multiple tickers detected, filter to those with actual data
        # and prefer the last one mentioned (likely the subject of the query)
        valid_tickers = []
        for ticker in tickers:
            ticker_upper = ticker.upper()
            try:
                records = self._fetch_metrics_cached(ticker_upper)
                if records:
                    valid_tickers.append(ticker_upper)
            except Exception:
                continue
        
        if len(valid_tickers) == 1:
            return valid_tickers[0]
        elif len(valid_tickers) > 1:
            # Prefer the last valid ticker (typically the actual company mentioned)
            return valid_tickers[-1]
        
        return None
    def _prepare_llm_messages(self, rag_context: Optional[str], *, is_forecasting: bool = False, user_query: Optional[str] = None) -> List[Mapping[str, str]]:
        """Trim history before sending to the LLM and append optional RAG context."""
        history = self.conversation.as_llm_messages()
        if not history:
            return []
        system_message, *chat_history = history
        if len(chat_history) > self._MAX_HISTORY_MESSAGES:
            chat_history = chat_history[-self._MAX_HISTORY_MESSAGES :]
        messages: List[Mapping[str, str]] = [system_message]
        
        # For forecasting queries, add context to the LAST user message to ensure it's seen
        # For other queries, add as system message
        # If is_forecasting is not provided, try to detect it from chat history
        if not is_forecasting:
            try:
                from .context_builder import _is_forecasting_query
                if chat_history:
                    last_user_msg = None
                    for msg in reversed(chat_history):
                        if msg.get("role") == "user":
                            last_user_msg = msg
                            break
                    if last_user_msg:
                        is_forecasting = _is_forecasting_query(last_user_msg.get("content", ""))
            except ImportError:
                pass
        
        if rag_context:
            # CRITICAL FIX: Always add context as a separate system message
            # DO NOT prepend to user message - this causes "looped system prompt interpretation"
            # LLM thinks the context is user input, not data to use
            # Context should be clearly marked as system-provided data context
            context_with_marker = (
                "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
                "üìä SYSTEM DATA CONTEXT (Use this data to answer the user's question below)\n"
                "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
                f"{rag_context}\n\n"
                "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
                "End of system data context - User question follows below\n"
                "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
            )
            # Add context as a system message (NOT user message)
            messages.append({"role": "system", "content": context_with_marker})
            
            # NEW: Validate context completeness before generating response
            if user_query and hasattr(self, 'settings') and self.settings.verification_enabled:
                try:
                    from .context_validator import validate_context_completeness, suggest_context_improvements
                    
                    completeness_check = validate_context_completeness(
                        rag_context,
                        user_query
                    )
                    
                    if not completeness_check.is_complete:
                        LOGGER.warning(
                            f"Context completeness check failed: {len(completeness_check.missing_elements)} missing elements, "
                            f"confidence: {completeness_check.confidence*100:.1f}%"
                        )
                        
                        # Add warning to context if confidence is low
                        if completeness_check.confidence < 0.5:
                            warning_msg = suggest_context_improvements(completeness_check, user_query)
                            if warning_msg:
                                context_with_marker = (
                                    f"‚ö†Ô∏è **CONTEXT COMPLETENESS WARNING:**\n{warning_msg}\n\n"
                                    f"{context_with_marker}"
                                )
                                # Update the last system message (context message)
                                messages[-1] = {"role": "system", "content": context_with_marker}
                except (ImportError, AttributeError):
                    pass  # context_validator not available or settings not accessible, skip check
            
            messages.extend(chat_history)
        else:
            messages.extend(chat_history)
        
        return messages

    def _is_document_followup(self, user_input: str) -> bool:
        """Heuristic to determine if the user is asking about a recently uploaded document."""
        lowered = (user_input or "").strip().lower()
        if not lowered:
            return False

        doc_keywords = {
            "document",
            "uploaded",
            "upload",
            "file",
            "pdf",
            "ppt",
            "slide",
            "presentation",
            "report",
            "paper",
            "case study",
        }
        if any(keyword in lowered for keyword in doc_keywords):
            return True

        # Short follow-up (e.g., "analyze it")
        if len(lowered.split()) <= 6 and "it" in lowered:
            recent_messages = self.conversation.messages[-4:]
            for message in reversed(recent_messages):
                if message.get("role") != "assistant":
                    continue
                content = (message.get("content") or "").lower()
                if "uploaded" in content or "ready for analysis" in content:
                    return True
        return False

    def _preload_popular_metrics(self) -> None:
        """Warm cached metrics for high-traffic tickers to reduce first-response latency."""
        for ticker in POPULAR_TICKERS:
            try:
                self._fetch_metrics_cached(ticker)
                self._get_ticker_summary(ticker, None)
            except Exception:  # pragma: no cover - defensive caching preload
                LOGGER.debug("Preload for %s failed", ticker, exc_info=True)
    @classmethod
    def create(cls, settings: Settings) -> "FinanlyzeOSChatbot":
        """Factory that wires analytics, storage, and the LLM client together."""
        llm_client = build_llm_client(
            settings.llm_provider,
            model=settings.openai_model,
        )

        database.initialise(settings.database_path)

        # PERFORMANCE FIX: Skip startup ingestion to avoid 1-2 minute delays
        # Data ingestion should be done separately, not during chatbot startup
        ingestion_report: Optional[IngestionReport] = None
        
        # Only run ingestion if explicitly enabled via environment variable
        if os.getenv("ENABLE_STARTUP_INGESTION", "false").lower() == "true":
            try:
                LOGGER.info("Running startup ingestion (ENABLE_STARTUP_INGESTION=true)")
                ingestion_report = ingest_financial_data(settings)
            except Exception as exc:  # pragma: no cover - defensive guard
                database.record_audit_event(
                    settings.database_path,
                    AuditEvent(
                        ticker="__system__",
                        event_type="ingestion_error",
                        entity_id="startup",
                        details={"error": str(exc)},
                        created_at=datetime.utcnow(),
                        created_by="chatbot",
                    ),
                )
        else:
            LOGGER.info("Skipping startup ingestion for faster startup (set ENABLE_STARTUP_INGESTION=true to enable)")

        analytics_engine = AnalyticsEngine(settings)
        
        # PERFORMANCE FIX: Skip expensive metrics refresh during startup
        # Only refresh if explicitly enabled or if database is empty
        if os.getenv("ENABLE_STARTUP_METRICS_REFRESH", "false").lower() == "true":
            LOGGER.info("Running startup metrics refresh (ENABLE_STARTUP_METRICS_REFRESH=true)")
            analytics_engine.refresh_metrics(force=True)
        else:
            LOGGER.info("Skipping startup metrics refresh for faster startup")

        # Build the SEC-backed name index once; always attempt local alias fallback
        index = _CompanyNameIndex()
        
        # PERFORMANCE FIX: Skip SEC API calls during startup, use database instead
        base = getattr(settings, "edgar_base_url", None) or "https://www.sec.gov"
        ua = getattr(settings, "sec_api_user_agent", None) or "FinanlyzeOSBot/1.0 (support@finanlyzeos.com)"
        try:
            # This method already uses database instead of SEC API to avoid 404 errors
            index.build_from_sec(base, ua)
            LOGGER.info("SEC company index built with %d names", len(index.by_exact))
        except Exception as e:
            LOGGER.warning("SEC company index build failed: %s", e)

        # Always attempt local fallback (data/name_aliases.json)
        try:
            alias_path = Path(__file__).resolve().parent.parent / "data" / "name_aliases.json"
            index.load_local_aliases(alias_path)
        except Exception:
            LOGGER.exception("Failed while loading local aliases")

        LOGGER.info("Final name->ticker index size: %d", len(index.by_exact))

        sector_map = cls._load_sector_map()

        chatbot = cls(
            settings=settings,
            llm_client=llm_client,
            analytics_engine=analytics_engine,
            ingestion_report=ingestion_report,
            name_index=index,
            ticker_sector_map=sector_map,
        )
        chatbot._preload_popular_metrics()
        return chatbot

    # ----------------------------------------------------------------------------------
    # NL ‚Üí command normalization (accept natural company names)
    # ----------------------------------------------------------------------------------
    def _name_to_ticker(self, term: str) -> Optional[str]:
        """Resolve free-text company names to tickers using:
           1) engine's lookup_ticker (if provided)
           2) SEC-backed name index (exact/prefix/contains/token overlap)
           3) local alias fallback
        """
        if not term:
            return None
        t = term.strip()

        # engine's own mapping first
        lookup = getattr(self.analytics_engine, "lookup_ticker", None)
        if callable(lookup):
            try:
                tk = lookup(t, allow_partial=True)  # type: ignore[misc]
            except TypeError:
                tk = lookup(t)  # type: ignore[misc]
            except Exception:
                tk = None
            if tk:
                return tk.upper()

        # SEC/local name index
        tk = self.name_index.resolve(t)
        if tk:
            return tk.upper()

        fuzzy = self.name_index.resolve_fuzzy(t, cutoff=0.7)
        if fuzzy:
            return fuzzy[0][0].upper()

        return None

    def _detect_tickers(self, text: str) -> List[str]:
        """Detect tickers from user input text.
        
        Args:
            text: User input text to extract tickers from
            
        Returns:
            List of ticker symbols (uppercase) found in the text
        """
        from .parsing.alias_builder import resolve_tickers_freeform
        
        ticker_matches, _ = resolve_tickers_freeform(text)
        tickers = []
        
        for match in ticker_matches:
            if isinstance(match, dict):
                ticker = match.get("ticker")
                if ticker:
                    ticker_upper = ticker.upper()
                    if ticker_upper not in tickers:
                        tickers.append(ticker_upper)
        
        return tickers

    @staticmethod
    def _load_sector_map() -> Dict[str, Dict[str, Optional[str]]]:
        """Load sector and sub-industry metadata for tickers."""
        path = Path(__file__).resolve().parents[2] / "webui" / "data" / "company_universe.json"
        if not path.exists():
            return {}
        try:
            payload = json.loads(path.read_text(encoding="utf-8"))
        except Exception as exc:  # pragma: no cover - defensive guard
            LOGGER.warning("Unable to load company universe metadata: %s", exc)
            return {}

        mapping: Dict[str, Dict[str, Optional[str]]] = {}
        for entry in payload:
            ticker = str(entry.get("ticker", "")).strip().upper()
            if not ticker:
                continue
            mapping[ticker] = {
                "sector": entry.get("sector"),
                "sub_industry": entry.get("sub_industry") or entry.get("subIndustry"),
            }
        return mapping

    def _sector_label(self, ticker: str) -> Optional[str]:
        """Return a combined sector/sub-industry label for `ticker`."""
        info = self.ticker_sector_map.get(ticker.upper())
        if not info:
            return None
        sector = (info.get("sector") or "").strip()
        sub = (info.get("sub_industry") or "").strip()
        if sector and sub:
            return f"{sector} ‚Äì {sub}"
        return sector or sub or None

    def _detect_fact_metric(self, text: str) -> Optional[str]:
        """Identify if the prompt references a specific fact metric."""
        cleaned = re.sub(r"[^a-z0-9\s/.-]", " ", text.lower())
        cleaned = re.sub(r"\s+", " ", cleaned).strip()
        if not cleaned:
            return None

        best_match: Optional[str] = None
        best_len = -1
        for alias, canonical in FACT_METRIC_ALIASES.items():
            if re.search(rf"\b{re.escape(alias)}\b", cleaned):
                alias_len = len(alias)
                if alias_len > best_len:
                    best_match = canonical
                    best_len = alias_len
        return best_match

    def _detect_custom_kpi_intent(self, text: str) -> Optional[Dict[str, Any]]:
        """Detect if the prompt is about creating or using custom KPIs."""
        intent = self.kpi_intent_parser.detect(text)
        if intent and intent.get("action") == "create":
            intent["raw_prompt"] = text
        return intent

    def _handle_custom_kpi_intent(self, intent: Dict[str, Any]) -> Optional[str]:
        """Handle custom KPI intents."""
        calculator = CustomKPICalculator(self.settings.database_path)
        user_id = "default"  # TODO: Get from session/user context
        
        if intent["action"] == "create":
            lookup_scope = intent.get("lookup_scope")
            try:
                definition: Optional[CustomKPIDefinition] = intent.get("definition")  # type: ignore[assignment]
                kpi: Optional[CustomKPI] = None
                created_via_lookup = False

                if definition:
                    formula_text = getattr(definition, "formula", "")
                    if formula_text:
                        kpi = calculator.upsert_kpi(user_id, definition)
                    else:
                        lookup_name = getattr(definition, "name", "") or intent.get("raw_name", "")
                        lookup_name = lookup_name.strip()
                        if not lookup_name:
                            return "Please provide a KPI name so I can look it up."
                        kpi = calculator.ensure_kpi_from_lookup(user_id, lookup_name, lookup_scope)
                        if not kpi:
                            return (
                                f"I couldn't find an established formula for '{lookup_name}'. "
                                "Would you like to define it manually?"
                            )
                        created_via_lookup = True
                else:
                    name = intent.get("raw_name", "").strip()
                    formula = intent.get("raw_formula", "").strip()
                    if name and formula:
                        definition = self.kpi_intent_parser._definition_from_text(intent.get("raw_prompt", ""), name, formula)
                        kpi = calculator.upsert_kpi(user_id, definition)
                    else:
                        lookup_name = name or intent.get("kpi_name", "").strip()
                        if not lookup_name:
                            return "Please provide a KPI name so I can look it up."
                        kpi = calculator.ensure_kpi_from_lookup(user_id, lookup_name, lookup_scope)
                        if not kpi:
                            return (
                                f"I couldn't find an established formula for '{lookup_name}'. "
                                "Would you like to define it manually?"
                            )
                        created_via_lookup = True

                if not kpi:
                    return "Unable to save the KPI definition."

                details = kpi.metadata or {}
                extras = []
                if kpi.frequency:
                    extras.append(f"**Frequency:** {kpi.frequency}")
                if kpi.unit:
                    extras.append(f"**Unit:** {kpi.unit}")
                group = (kpi.metadata or {}).get("group")
                if group:
                    extras.append(f"**Group:** {group}")
                if kpi.inputs:
                    extras.append(f"**Inputs:** {', '.join(sorted(set(kpi.inputs)))}")
                if kpi.source_tags:
                    extras.append(f"**Source Tags:** {', '.join(sorted(set(kpi.source_tags)))}")
                if created_via_lookup:
                    source = details.get("definition_source") or {}
                    source_label = source.get("name") or source.get("type")
                    if source_label:
                        extras.append(f"**Definition Source:** {source_label}")
                    else:
                        extras.append("**Definition Source:** Auto-resolved library")
                    confidence = details.get("lookup_confidence")
                    if confidence is not None:
                        extras.append(f"**Lookup Confidence:** {confidence:.2f}")
                meta_text = "\n".join(extras) if extras else ""

                return (
                    f"‚úÖ Saved custom KPI **{kpi.name}**\n\n"
                    f"**Formula:** `{kpi.formula}`\n"
                    + (meta_text + "\n\n" if meta_text else "\n")
                    + "You can calculate it with `Calculate {kpi.name} for AAPL`."
                )
            except ValueError as e:
                return f"‚ùå Error creating custom KPI: {str(e)}"
            except Exception as e:
                LOGGER.exception(f"Error creating custom KPI: {e}")
                return f"‚ùå Error creating custom KPI: {str(e)}"
        
        elif intent["action"] == "calculate":
            kpi_name = intent.get("kpi_name", "").strip()
            ticker = intent.get("ticker", "").strip().upper()
            lookup_scope = intent.get("lookup_scope")
            
            if not kpi_name or not ticker:
                return "Please provide both a KPI name and ticker. Example: 'Calculate Contribution Margin for AAPL'"
            
            # Find KPI by name
            kpis = calculator.list_kpis(user_id)
            matching_kpi = None
            for kpi in kpis:
                if kpi.name.lower() == kpi_name.lower():
                    matching_kpi = kpi
                    break
            
            created_via_lookup = False
            if not matching_kpi:
                matching_kpi = calculator.ensure_kpi_from_lookup(user_id, kpi_name, lookup_scope)
                if matching_kpi:
                    created_via_lookup = True

            if not matching_kpi:
                return f"‚ùå Custom KPI '{kpi_name}' not found. Use 'List my custom KPIs' to see available KPIs."
            
            # Calculate
            result = calculator.calculate_kpi(matching_kpi.kpi_id, ticker)
            
            if result.error:
                return f"‚ùå Error calculating {result.kpi_name} for {result.ticker}: {result.error}"
            
            # Format response
            value_label = result.formatted_value if result.formatted_value is not None else f"{result.value:,.2f}" if result.value is not None else "N/A"
            response = f"**{result.kpi_name}** for {result.ticker} ({result.period}):\n\n"
            response += f"**Value:** {value_label}\n\n"
            if result.metadata:
                if result.metadata.get("frequency"):
                    response += f"- Frequency: {result.metadata['frequency']}\n"
                if result.metadata.get("unit"):
                    response += f"- Unit: {result.metadata['unit']}\n"
                inputs = result.metadata.get("inputs") or []
                if inputs:
                    response += f"- Inputs: {', '.join(inputs)}\n"
            
            if result.calculation_steps:
                response += "**Calculation Steps:**\n"
                for step in result.calculation_steps:
                    if "step" in step:
                        response += f"- {step['step']}: {step.get('value', 'N/A')}\n"

            if result.sources:
                response += "\n**Source Trace:**\n"
                for source in result.sources:
                    label_parts = [
                        source.get("metric"),
                        str(source.get("period") or source.get("fiscal_year") or ""),
                    ]
                    label = " ".join(part for part in label_parts if part).strip()
                    source_label = source.get("source") or "database"
                    response += f"- {label} ‚Üí {source_label}"
                    if source.get("source_ref"):
                        response += f" ({source['source_ref']})"
                    response += "\n"

            if result.dependencies:
                response += f"\n**Dependencies:** {', '.join(result.dependencies)}\n"
            
            if created_via_lookup:
                source = matching_kpi.metadata.get("definition_source") or result.metadata.get("definition_source", {})
                source_label = None
                if isinstance(source, dict):
                    source_label = source.get("name") or source.get("type")
                if source_label:
                    response += f"\n_Lookup source: {source_label}_\n"
            
            return response
        
        elif intent["action"] == "source_lookup":
            lookup_scope = intent.get("lookup_scope")
            kpi_name = intent.get("kpi_name") or intent.get("raw_name") or intent.get("raw_prompt")
            if not kpi_name:
                return "Please tell me which KPI to look up, e.g. 'Define KPI: Gross Margin'."
            
            kpi_name = str(kpi_name).strip()
            if not kpi_name:
                return "Please tell me which KPI to look up, e.g. 'Define KPI: Gross Margin'."

            kpi = calculator.ensure_kpi_from_lookup(user_id, kpi_name, lookup_scope)
            if not kpi:
                return (
                    f"I couldn't find an established formula for '{kpi_name}'. "
                    "Would you like to define it manually?"
                )

            details = kpi.metadata or {}
            source = details.get("definition_source") or {}
            source_label = source.get("name") if isinstance(source, dict) else None
            confidence = details.get("lookup_confidence")
            notes: List[str] = []
            if source_label:
                notes.append(f"Definition source: {source_label}")
            elif source:
                notes.append("Definition source: external reference")
            if confidence is not None:
                notes.append(f"Confidence: {confidence:.2f}")

            description = kpi.description or details.get("definition_description")
            note_text = "\n".join(f"- {line}" for line in notes) if notes else ""

            response = (
                f"‚úÖ Created KPI **{kpi.name}**\n\n"
                f"**Formula:** `{kpi.formula}`\n"
            )
            if note_text:
                response += note_text + "\n\n"
            if description:
                response += f"{description}\n\n"
            response += f"You can calculate it with `Calculate {kpi.name} for AAPL`."
            return response
        
        elif intent["action"] == "list":
            kpis = calculator.list_kpis(user_id)
            if not kpis:
                return "You don't have any custom KPIs yet. Create one with: 'Create a KPI called [name] = [formula]'"
            
            response = f"**Your Custom KPIs ({len(kpis)}):**\n\n"
            for kpi in kpis:
                response += f"- **{kpi.name}**: {kpi.formula}\n"
                meta_bits = []
                if kpi.frequency:
                    meta_bits.append(f"freq={kpi.frequency}")
                if kpi.unit:
                    meta_bits.append(f"unit={kpi.unit}")
                if kpi.inputs:
                    meta_bits.append(f"inputs={', '.join(kpi.inputs)}")
                if meta_bits:
                    meta_summary = " | ".join(meta_bits)
                    response += f"  _({meta_summary})_\n"
                if kpi.description:
                    response += f"  _{kpi.description}_\n"
            
            return response
        
        return None

    def _detect_modeling_intent(self, text: str) -> Optional[Dict[str, Any]]:
        """Detect if the prompt is about creating or using custom models."""
        lowered = text.lower()
        live_flag = self._intent_requests_live_data(text)
        standard_ml_models = {
            "prophet",
            "arima",
            "ets",
            "lstm",
            "gru",
            "transformer",
            "ensemble",
            "ensemble methods",
            "best",
            "best model",
            "best ml model",
            "best machine learning model",
            "auto",
            "automatic",
            "ml",
            "ml model",
            "machine learning",
            "machine learning model",
        }

        def _is_standard_ml_model(name: Optional[str]) -> bool:
            if not name:
                return False
            import re as _re

            normalized = name.lower().strip()
            normalized = _re.sub(r'[\-_]+', ' ', normalized)
            normalized = normalized.replace("forecast", "").strip()
            normalized = normalized.replace("plugin", "").strip()
            if normalized.endswith("model"):
                normalized = normalized[:-5].strip()
            if normalized.endswith("methods"):
                normalized = normalized[:-7].strip()
            if normalized in standard_ml_models:
                return True
            if "best" in normalized and ("ml" in normalized or "machine" in normalized):
                return True
            return False
        
        # Patterns for creating models
        create_patterns = [
            r'create\s+(?:a\s+)?(?:custom\s+)?model\s+(?:called|named)?\s*["\']?([^"\']+)["\']?\s+(?:using|with|type)\s+(\w+)',
            r'define\s+(?:a\s+)?(?:custom\s+)?model\s+(?:called|named)?\s*["\']?([^"\']+)["\']?\s+(?:using|with|type)\s+(\w+)',
            r'create\s+(?:a\s+)?(\w+)\s+model\s+(?:called|named)?\s*["\']?([^"\']+)["\']?',
        ]
        
        for pattern in create_patterns:
            match = re.search(pattern, lowered)
            if match:
                if len(match.groups()) == 2:
                    if 'model' in match.group(0).lower():
                        name = match.group(1).strip()
                        model_type = match.group(2).strip()
                    else:
                        model_type = match.group(1).strip()
                        name = match.group(2).strip()
                    return {"action": "create", "name": name, "model_type": model_type, "refresh": live_flag}
        
        # Patterns for running forecasts
        forecast_patterns = [
            r'forecast\s+(\w+)\'?s?\s+(\w+)\s+(?:using|with)\s+(?:my\s+)?(?:model\s+)?["\']?([^"\']+)["\']?',
            r'run\s+(?:a\s+)?forecast\s+(?:for|of)\s+(\w+)\s+(\w+)\s+(?:using|with)\s+(?:my\s+)?(?:model\s+)?["\']?([^"\']+)["\']?',
            r'predict\s+(\w+)\'?s?\s+(\w+)\s+(?:using|with)\s+(?:my\s+)?(?:model\s+)?["\']?([^"\']+)["\']?',
        ]
        
        for pattern in forecast_patterns:
            match = re.search(pattern, lowered)
            if match:
                ticker = match.group(1).strip().upper()
                metric = match.group(2).strip()
                model_name = match.group(3).strip() if len(match.groups()) > 2 else None
                if _is_standard_ml_model(model_name):
                    # This is an ML forecasting prompt (Prophet/ARIMA/etc.), not a custom model intent.
                    continue
                return {
                    "action": "forecast",
                    "ticker": ticker,
                    "metric": metric,
                    "model_name": model_name,
                    "refresh": live_flag,
                }
        
        # Patterns for scenario analysis
        scenario_patterns = [
            r'what\s+if\s+(\w+)\'?s?\s+(\w+)\s+(?:grows|increases|decreases|changes)\s+(?:at|by)\s+([\d.]+)\s*%',
            r'scenario\s+(?:where|if)\s+(\w+)\'?s?\s+(\w+)\s+(?:grows|increases|decreases|changes)\s+(?:at|by)\s+([\d.]+)\s*%',
            r'run\s+(?:a\s+)?(?:scenario|what-if)\s+(?:for|on)\s+(\w+)\s+(\w+)\s+(?:with|at)\s+([\d.]+)\s*%',
        ]
        
        for pattern in scenario_patterns:
            match = re.search(pattern, lowered)
            if match:
                ticker = match.group(1).strip().upper()
                metric = match.group(2).strip()
                growth_rate = float(match.group(3).strip())
                return {
                    "action": "scenario",
                    "ticker": ticker,
                    "metric": metric,
                    "growth_rate": growth_rate,
                    "refresh": live_flag,
                }
        
        # Pattern for listing models
        if re.search(r'list\s+(?:my\s+)?(?:custom\s+)?models?', lowered):
            return {"action": "list", "refresh": live_flag}
        
        # Pattern for explaining models
        explain_pattern = r'explain\s+(?:my\s+)?(?:model\s+)?["\']?([^"\']+)["\']?'
        match = re.search(explain_pattern, lowered)
        if match:
            return {"action": "explain", "model_name": match.group(1).strip(), "refresh": live_flag}
        
        return None
    def _handle_plugin_intent(self, intent: Dict[str, Any], raw_text: str) -> Optional[str]:
        """Execute plugin workflows (register, list, train, forecast)."""
        builder = ModelBuilder(self.settings.database_path)
        user_id = "default"
        action = intent["action"]
        refresh_flag = intent.get("refresh", False)

        if action == "list":
            plugins = builder.list_forecasting_plugins(user_id=user_id)
            if not plugins:
                return "You don't have any custom forecasting plugins yet. Upload one by saying 'Upload my estimator' and include the code."

            response = f"**Your Forecasting Plugins ({len(plugins)}):**\n\n"
            for plugin in plugins:
                provenance = plugin.metadata.get("provenance") if isinstance(plugin.metadata, dict) else None
                response += f"- **{plugin.name}** (`{plugin.class_name}`)"
                if provenance:
                    response += f" ‚Äì _provenance: {provenance}_"
                if plugin.last_trained_at:
                    response += f" (last trained {plugin.last_trained_at.date()})"
                response += "\n"
            return response

        if action == "register":
            code_blocks = self._extract_code_blocks(raw_text)
            source_block = next(
                (block for lang, block in code_blocks if lang in (None, "", "python", "py")),
                None,
            )
            template_candidate = intent.get("template_name")
            provided_name = intent.get("name")
            if not source_block:
                candidate = template_candidate or (provided_name.strip() if provided_name else "")
                if candidate:
                    try:
                        plugin = builder.instantiate_template_plugin(
                            candidate,
                            user_id=user_id,
                            plugin_name=provided_name or None,
                        )
                        provenance = plugin.metadata.get("provenance") if isinstance(plugin.metadata, dict) else "template_library"
                        return (
                            f"‚úÖ Registered template plugin **{plugin.name}** (`{plugin.class_name}`)\n"
                            f"- Template: {plugin.metadata.get('template_id', candidate)}\n"
                            f"- Provenance: {provenance}\n"
                            f"- Plugin ID: `{plugin.plugin_id}`\n"
                            f"You can train it with: `Train my {plugin.name} on AAPL revenue`."
                        )
                    except ValueError:
                        pass
                return (
                    "To register a plugin, include a Python code block with your estimator class or reference a known template. "
                    "Example:\n```python\nclass MyForecaster:\n    ...\n```"
                )
            class_match = re.search(r"class\s+([A-Za-z_][A-Za-z0-9_]*)\s*(?:\(|:)", source_block)
            if not class_match:
                return "I couldn't find a class definition in the provided code. Please define your forecaster class."
            class_name = class_match.group(1)
            plugin_name = intent.get("name") or class_name

            metadata_block = next((block for lang, block in code_blocks if lang == "json"), None)
            metadata_payload: Optional[Dict[str, Any]] = None
            if metadata_block:
                try:
                    metadata_payload = json.loads(metadata_block)
                except json.JSONDecodeError:
                    return "I couldn't parse the metadata JSON block. Please ensure it's valid JSON."

            try:
                plugin = builder.register_forecasting_plugin(
                    user_id=user_id,
                    name=plugin_name,
                    class_name=class_name,
                    source_code=source_block,
                    metadata=metadata_payload,
                )
            except PluginRegistrationError as exc:
                return f"‚ùå Plugin registration failed: {exc}"
            except Exception as exc:  # pragma: no cover - defensive
                LOGGER.exception("Unexpected plugin registration failure")
                return f"‚ùå Plugin registration failed: {exc}"

            provenance = plugin.metadata.get("provenance") if isinstance(plugin.metadata, dict) else "unspecified"
            return (
                f"‚úÖ Registered plugin **{plugin.name}** (`{plugin.class_name}`)\n"
                f"- Provenance: {provenance}\n"
                f"- Plugin ID: `{plugin.plugin_id}`\n"
                f"You can train it with: `Train my {plugin.name} on AAPL revenue`."
            )

        plugin_name = intent.get("plugin_name")
        if not plugin_name:
            return None
        plugin = self._lookup_plugin(builder, user_id, plugin_name)
        if not plugin:
            return f"‚ùå I couldn't find a plugin named '{plugin_name}'. Use 'List my plugins' to see available names."

        ticker = intent.get("ticker", "").strip().upper()
        metric = intent.get("metric", "").strip()
        if not ticker or not metric:
            return "Please include both a ticker and metric, e.g., 'Train my GrowthPlugin on AAPL revenue'."

        parameters = {}
        retrain_flag = bool(intent.get("retrain"))

        if action == "train":
            try:
                details = builder.train_forecasting_plugin(
                    plugin.plugin_id,
                    ticker=ticker,
                    metric=metric,
                    parameters=parameters,
                    user_id=user_id,
                    refresh=refresh_flag,
                )
            except PluginExecutionError as exc:
                return f"‚ùå Training failed: {exc}"
            except Exception as exc:  # pragma: no cover
                LOGGER.exception("Unexpected plugin training failure")
                return f"‚ùå Training failed: {exc}"

            description = details.get("description") or {}
            if isinstance(description, dict):
                summary = description.get("summary") or description.get("details")
            else:
                summary = str(description)
            summary_line = f" ‚Äì {summary}" if summary else ""

            response = (
                f"‚úÖ Trained **{plugin.name}** on {ticker} {metric}{summary_line}\n"
                f"Last trained at: {details.get('trained_at')}\n"
                "Run a forecast with: "
                f"`Forecast using my {plugin.name} plugin for {ticker} {metric}`."
            )
            live_info = details.get("live_data")
            if isinstance(live_info, dict):
                if live_info.get("fetched"):
                    fetched_at = live_info.get("fetched_at") or "just now"
                    response += f"\n_Live data fetched at {fetched_at}_"
                elif live_info.get("warning"):
                    response += f"\n_Warning: {live_info['warning']}_"
            return response

        if action == "forecast":
            try:
                result = builder.forecast_with_plugin(
                    plugin.plugin_id,
                    ticker=ticker,
                    metric=metric,
                    forecast_years=3,
                    parameters=parameters,
                    user_id=user_id,
                    retrain=retrain_flag,
                    refresh=refresh_flag,
                )
            except PluginExecutionError as exc:
                return f"‚ùå Forecast failed: {exc}"
            except Exception as exc:  # pragma: no cover
                LOGGER.exception("Unexpected plugin forecast failure")
                return f"‚ùå Forecast failed: {exc}"

            preds = result.get("predictions") or []
            preview = ""
            for idx, value in enumerate(preds[:3], start=1):
                try:
                    numeric_value = float(value)
                    rendered_value = f"{numeric_value:,.2f}"
                except (TypeError, ValueError):
                    rendered_value = str(value)
                preview += f"- Year +{idx}: {rendered_value}\n"
            provenance = result.get("provenance") or "unspecified"
            description = result.get("description") or {}
            if isinstance(description, dict):
                summary = description.get("summary") or description.get("details")
            else:
                summary = str(description)

            response = (
                f"**Forecast from {plugin.name}** (provenance: {provenance})\n\n"
            )
            if summary:
                response += f"{summary}\n\n"
            if preview:
                response += preview
            else:
                response += "No forecast values returned."
            live_info = result.get("live_data")
            if isinstance(live_info, dict):
                if live_info.get("fetched"):
                    fetched_at = live_info.get("fetched_at") or "just now"
                    response += f"\n_Live data fetched at {fetched_at}."
                elif live_info.get("warning"):
                    response += f"\n_Warning: {live_info['warning']}_"
            return response

        return None

    @staticmethod
    def _lookup_plugin(
        builder: ModelBuilder,
        user_id: str,
        plugin_name: str,
    ) -> Optional[ForecastingPlugin]:
        """Resolve a plugin by name (case-insensitive)."""
        plugins = builder.list_forecasting_plugins(user_id=user_id)
        for plugin in plugins:
            if plugin.name.lower() == plugin_name.lower():
                return plugin
        return None

    def _detect_source_trace_intent(self, text: str) -> Optional[Dict[str, Any]]:
        """Detect if the prompt is about tracing data sources."""
        lowered = text.lower()
        
        # Patterns for source tracing
        trace_patterns = [
            r'(?:show\s+me|where|trace|source)\s+(?:where\s+)?(?:did|does|is)\s+(\w+)\'?s?\s+(\w+)\s+(?:come\s+from|originate|source)',
            r'(?:show\s+me|trace)\s+(?:the\s+)?source\s+(?:of|for)\s+(\w+)\'?s?\s+(\w+)',
            r'where\s+(?:is|does)\s+(\w+)\'?s?\s+(\w+)\s+(?:from|come from)',
        ]
        
        for pattern in trace_patterns:
            match = re.search(pattern, lowered)
            if match:
                ticker = match.group(1).strip().upper()
                metric = match.group(2).strip()
                return {"action": "trace", "ticker": ticker, "metric": metric}
        
        return None

    def _handle_modeling_intent(self, intent: Dict[str, Any]) -> Optional[str]:
        """Handle modeling intents."""
        builder = ModelBuilder(self.settings.database_path)
        user_id = "default"
        
        if intent["action"] == "create":
            try:
                name = intent.get("name", "").strip()
                model_type = intent.get("model_type", "").strip().lower()
                
                if not name or not model_type:
                    return "Please provide both a model name and type. Example: 'Create a model called Revenue Forecast using growth_rate'"
                
                model = builder.create_model(user_id, name, model_type)
                return f"‚úÖ Created model '{model.name}' with type: {model.model_type}\n\nYou can now use this model by asking: 'Forecast [ticker]'s [metric] using {model.name}'"
            except ValueError as e:
                return f"‚ùå Error creating model: {str(e)}"
            except Exception as e:
                LOGGER.exception(f"Error creating model: {e}")
                return f"‚ùå Error creating model: {str(e)}"
        
        elif intent["action"] == "forecast":
            ticker = intent.get("ticker", "").strip().upper()
            metric = intent.get("metric", "").strip()
            model_name = intent.get("model_name")
            refresh_flag = intent.get("refresh", False)
            
            if not ticker or not metric:
                return "Please provide both a ticker and metric. Example: 'Forecast AAPL's revenue using my Revenue Forecast model'"
            
            # Find model by name if provided
            model_id = None
            if model_name:
                models = builder.list_models(user_id)
                for model in models:
                    if model.name.lower() == model_name.lower():
                        model_id = model.model_id
                        break
                
                if not model_id:
                    return f"‚ùå Model '{model_name}' not found. Use 'List my models' to see available models."
            else:
                # Use default model or create one
                models = builder.list_models(user_id)
                if models:
                    model_id = models[0].model_id
                else:
                    # Create a default model
                    model = builder.create_model(user_id, "Default Forecast", "growth_rate")
                    model_id = model.model_id
            
            # Run forecast
            model_run = builder.run_forecast(
                model_id,
                ticker,
                metric,
                refresh=refresh_flag,
            )
            
            if not model_run.results:
                return f"‚ùå Error running forecast for {ticker} {metric}"
            
            # Format response
            response = f"**Forecast for {ticker} {metric}:**\n\n"
            if "forecasts" in model_run.results:
                for forecast in model_run.results["forecasts"][:3]:  # Show first 3 years
                    year = forecast.get("fiscal_year", "N/A")
                    value = forecast.get("predicted_value", 0)
                    response += f"- {year}: ${value:,.0f}\n"
            live_info = model_run.results.get("live_data") if isinstance(model_run.results, dict) else None
            if isinstance(live_info, dict):
                if live_info.get("fetched"):
                    fetched_at = live_info.get("fetched_at") or "just now"
                    response += f"\n_Live data fetched at {fetched_at}_"
                elif live_info.get("warning"):
                    response += f"\n_Warning: {live_info['warning']}_"
            
            return response
        
        elif intent["action"] == "scenario":
            ticker = intent.get("ticker", "").strip().upper()
            metric = intent.get("metric", "").strip()
            growth_rate = intent.get("growth_rate", 0.0)
            
            if not ticker or not metric:
                return "Please provide both a ticker and metric. Example: 'What if AAPL's revenue grows at 15% per year?'"
            
            # Get or create a model
            models = builder.list_models(user_id)
            model_id = None
            if models:
                model_id = models[0].model_id
            else:
                model = builder.create_model(user_id, "Scenario Model", "growth_rate")
                model_id = model.model_id
            
            # Run scenario
            model_run = builder.run_scenario(
                model_id, ticker, metric, "custom", {"growth_rate": growth_rate}
            )
            
            # Format response
            response = f"**Scenario Analysis for {ticker} {metric} ({growth_rate}% growth):**\n\n"
            if "forecasts" in model_run.results:
                for forecast in model_run.results["forecasts"][:3]:
                    year = forecast.get("fiscal_year", "N/A")
                    value = forecast.get("predicted_value", 0)
                    response += f"- {year}: ${value:,.0f}\n"
            
            return response
        
        elif intent["action"] == "list":
            models = builder.list_models(user_id)
            if not models:
                return "You don't have any custom models yet. Create one with: 'Create a model called [name] using [type]'"
            
            response = f"**Your Custom Models ({len(models)}):**\n\n"
            for model in models:
                response += f"- **{model.name}**: {model.model_type}\n"
            
            return response
        
        elif intent["action"] == "explain":
            model_name = intent.get("model_name", "").strip()
            if not model_name:
                return "Please provide a model name. Example: 'Explain my Revenue Forecast model'"
            
            models = builder.list_models(user_id)
            matching_model = None
            for model in models:
                if model.name.lower() == model_name.lower():
                    matching_model = model
                    break
            
            if not matching_model:
                return f"‚ùå Model '{model_name}' not found."
            
            explanation = builder.explain_model(matching_model.model_id)
            
            response = f"**Model: {explanation['model_name']}**\n\n"
            response += f"**Type:** {explanation['model_type']}\n\n"
            response += f"**Explanation:**\n{explanation['explanation']}\n"
            
            return response
        
        return None

    def _handle_source_trace_intent(self, intent: Dict[str, Any]) -> Optional[str]:
        """Handle source tracing intents."""
        tracer = SourceTracer(self.settings.database_path)
        
        ticker = intent.get("ticker", "").strip().upper()
        metric = intent.get("metric", "").strip()
        
        if not ticker or not metric:
            return "Please provide both a ticker and metric. Example: 'Show me where AAPL's revenue came from'"
        
        # Trace source
        trace = tracer.trace_metric(ticker, metric)
        
        # Format response
        response = f"**Source Trace for {ticker} {metric}:**\n\n"
        response += f"**Value:** {trace.value:,.2f if trace.value else 'N/A'}\n\n"
        
        if trace.sources:
            response += "**Sources:**\n"
            for i, source in enumerate(trace.sources[:5], 1):  # Show first 5 sources
                if isinstance(source, dict):
                    filing = source.get("form_type", "Unknown")
                    sec_url = source.get("sec_url", "")
                    if sec_url:
                        response += f"{i}. {filing} - [View Filing]({sec_url})\n"
                    else:
                        response += f"{i}. {filing}\n"
        
        return response
    def _normalize_nl_to_command(self, text: str) -> Optional[str]:
        """Turn flexible NL prompts into the strict CLI-style commands this class handles."""
        t = text.strip()
        
        # CRITICAL: Skip normalization for forecasting queries
        # Normalization might break forecasting intent (e.g., "Forecast Apple revenue" -> "metrics AAPL")
        # Forecasting queries should be handled by LLM with forecast context, not normalized commands
        try:
            from .context_builder import _is_forecasting_query
            if _is_forecasting_query(text):
                LOGGER.debug(f"Skipping normalization for forecasting query: {text}")
                return None  # Return None to skip normalization, let LLM handle it
        except ImportError:
            pass  # If context_builder not available, continue with normalization

        def _canon_year_span(txt: str) -> Optional[str]:
            if not txt:
                return None
            m = re.search(r"(?:FY\s*)?(\d{4})\s*(?:[-/]\s*(?:FY\s*)?(\d{4}))?$", txt.strip(), re.IGNORECASE)
            if not m:
                return None
            a = int(m.group(1)); b = int(m.group(2)) if m.group(2) else None
            if b and b < a:
                a, b = b, a
            return f"{a}-{b}" if b else f"{a}"

        def _extract_year_phrase(full_text: str) -> Optional[str]:
            """Handle richer year phrasing such as 'between 2020 and 2022'."""
            if not full_text:
                return None
            current_year = datetime.utcnow().year
            if re.search(r"\b(this|current)\s+year\b", full_text, re.IGNORECASE):
                return str(current_year)
            if re.search(r"\bnext\s+year\b", full_text, re.IGNORECASE):
                return str(current_year + 1)
            if re.search(r"\blast\s+year\b", full_text, re.IGNORECASE):
                return str(current_year - 1)
            if re.search(r"\blast\s+fiscal\s+year\b", full_text, re.IGNORECASE):
                return str(current_year - 1)
            patterns = [
                r"(?:between|from)\s+(?:fy\s*)?(\d{4})\s+(?:and|to|through)\s+(?:fy\s*)?(\d{4})",
                r"(?:for|during|over)\s+(?:fy\s*)?(\d{4})\s+(?:and|&)\s+(?:fy\s*)?(\d{4})",
                r"(?:fy\s*)?(\d{4})\s+(?:through|to)\s+(?:fy\s*)?(\d{4})",
            ]
            for pattern in patterns:
                found = re.search(pattern, full_text, re.IGNORECASE)
                if found:
                    start = int(found.group(1))
                    end = int(found.group(2))
                    if end < start:
                        start, end = end, start
                    if start == end:
                        return f"{start}"
                    return f"{start}-{end}"

            quarter_match = re.search(
                r"(?:q[1-4]|quarter\s+[1-4])\s+(?:fy\s*)?(\d{4})", full_text, re.IGNORECASE
            )
            if quarter_match:
                return str(int(quarter_match.group(1)))

            years = [int(y) for y in re.findall(r"(?:fy\s*)?(20\d{2})", full_text, re.IGNORECASE)]
            unique_years = sorted(set(years))
            if len(unique_years) >= 2:
                start, end = unique_years[0], unique_years[-1]
                if start != end:
                    return f"{start}-{end}"
            if unique_years:
                return str(unique_years[0])
            return None

        def _extract_quarter_phrase(full_text: str) -> Optional[tuple[int, str]]:
            if not full_text:
                return None
            quarter_words = {
                "first": "Q1",
                "1st": "Q1",
                "second": "Q2",
                "2nd": "Q2",
                "third": "Q3",
                "3rd": "Q3",
                "fourth": "Q4",
                "4th": "Q4",
            }

            match = re.search(
                r"\bQ([1-4])\s*(?:of\s+)?(?:FY\s*)?(\d{2,4})\b",
                full_text,
                re.IGNORECASE,
            )
            if match:
                quarter = f"Q{match.group(1)}"
                year = int(match.group(2))
                if year < 100:
                    year += 2000 if year < 70 else 1900
                return (year, quarter)

            match = re.search(
                r"\b(?:FY\s*)?(\d{2,4})\s*Q([1-4])\b",
                full_text,
                re.IGNORECASE,
            )
            if match:
                year = int(match.group(1))
                if year < 100:
                    year += 2000 if year < 70 else 1900
                quarter = f"Q{match.group(2)}"
                return (year, quarter)

            match = re.search(
                r"\b(first|second|third|fourth|1st|2nd|3rd|4th)\s+quarter\s+(?:of\s+)?(?:FY\s*)?(\d{2,4})\b",
                full_text,
                re.IGNORECASE,
            )
            if match:
                quarter = quarter_words[match.group(1).lower()]
                year = int(match.group(2))
                if year < 100:
                    year += 2000 if year < 70 else 1900
                return (year, quarter)

            return None

        def _resolve_company_phrase(phrase: str) -> Optional[str]:
            cleaned_phrase = re.sub(r"[\u2019']", " ", phrase)
            tokens = [tok for tok in re.split(r"\s+", cleaned_phrase.strip()) if tok]
            if not tokens:
                return None
            n = len(tokens)
            for length in range(n, 0, -1):
                for start in range(0, n - length + 1):
                    candidate_tokens = tokens[start : start + length]
                    candidate = " ".join(candidate_tokens)
                    ticker = self._name_to_ticker(candidate)
                    if ticker:
                        return ticker.upper()
            return None

        def _split_entities(value: str) -> List[str]:
            s = value.strip().strip(",")
            if not s:
                return []
            quoted = re.findall(r'"([^"]+)"|\'([^\']+)\'', s)
            taken, out = set(), []
            for a, b in quoted:
                name = (a or b).strip()
                if name:
                    out.append(name); taken.add(name)
            s_clean = re.sub(r'"[^"]+"|\'[^\']+\'', "", s).strip()
            parts = [p for p in re.split(r"\s*,\s*", s_clean)] if "," in s_clean else \
                    [p for p in re.split(r"\s+and\s+", s_clean, flags=re.IGNORECASE)]
            for p in parts:
                p = p.strip()
                if p and p not in taken:
                    out.append(p)
            return out or [s]

        def _to_ticker(name_or_ticker: str) -> str:
            return (self._name_to_ticker(name_or_ticker) or name_or_ticker).upper()

        lower = t.lower()
        skip_prefixes = (
            "metrics",
            "metric ",
            "compare",
            "versus",
            "vs ",
            "vs.",
            "fact",
            "facts",
            "audit",
            "ingest",
            "fetch",
            "hydrate",
            "update",
            "scenario",
            "what-if",
            "what if",
            "table",
        )
        metric_terms = {
            "eps",
            "ebitda",
            "roe",
            "roa",
            "fcf",
            "peg",
            "pe",
            "p/e",
            "cagr",
        }

        range_match = re.search(
            r"(?P<company>[\w&'.-]+(?:\s+[\w&'.-]+){0,3})(?:['‚Äô]s)?\s+(?P<metric>[\w/&\-\s]+?)\s+(?:between|from)\s+(?:fy\s*)?(?P<start>\d{4})\s+(?:and|to|through)\s+(?:fy\s*)?(?P<end>\d{4})",
            t,
            re.IGNORECASE,
        )
        handled_direct = False
        if range_match:
            company_phrase = range_match.group("company")
            metric_phrase = range_match.group("metric").strip()
            start_year = int(range_match.group("start"))
            end_year = int(range_match.group("end"))
            if end_year < start_year:
                start_year, end_year = end_year, start_year
            resolved_ticker_for_range = _resolve_company_phrase(company_phrase)
            if resolved_ticker_for_range:
                metric_key = self._detect_fact_metric(metric_phrase) or "revenue"
                handled_direct = True
                return " ".join(["fact-range", resolved_ticker_for_range, f"{start_year}-{end_year}", metric_key])

        if not handled_direct:
            year_token = re.search(
                r"(?:fy\s*)?(?P<year>\d{4})(?:\s*(?:q|quarter)\s*(?P<quarter>[1-4]))?",
                t,
                re.IGNORECASE,
            )
            if year_token:
                year = year_token.group("year")
                quarter = year_token.group("quarter")
                pre_text = t[: year_token.start()]
                post_text = t[year_token.end() :]

                company_phrase_candidates = [
                    " ".join(pre_text.split()[-6:]),
                    " ".join(t.split()[:6]),
                ]
                resolved_ticker = None
                for candidate in company_phrase_candidates:
                    if not candidate:
                        continue
                    resolved_ticker = _resolve_company_phrase(candidate)
                    if resolved_ticker:
                        break

                if resolved_ticker:
                    metric_candidates = [
                        " ".join(post_text.split()[:6]),
                        " ".join(pre_text.split()[-6:]),
                    ]
                    metric_key = None
                    for candidate in metric_candidates:
                        if not candidate:
                            continue
                        metric_key = self._detect_fact_metric(candidate)
                        if metric_key:
                            break
                    if not metric_key:
                        metric_key = "revenue"

                    period_token = f"{year}Q{quarter}" if quarter else year
                    return " ".join(["fact", resolved_ticker, period_token, metric_key])

        if not any(lower.startswith(prefix) for prefix in skip_prefixes):
            detected_entities = self._detect_tickers(text)

            def _has_metrics(candidate: str) -> bool:
                try:
                    records = self._fetch_metrics_cached(candidate)
                except Exception:
                    return False
                return bool(records)

            ordered_subjects: List[str] = []
            seen_subjects: set[str] = set()
            ticker_pattern = re.compile(r"[A-Z]{1,5}(?:-[A-Z]{1,2})?")

            for raw in detected_entities:
                candidate = self._name_to_ticker(raw)
                if candidate:
                    resolved = candidate.upper()
                else:
                    upper_raw = raw.upper()
                    if not ticker_pattern.fullmatch(upper_raw):
                        continue
                    if not _has_metrics(upper_raw):
                        continue
                    resolved = upper_raw

                if resolved.lower() in metric_terms:
                    continue
                if resolved in seen_subjects:
                    continue
                seen_subjects.add(resolved)
                ordered_subjects.append(resolved)

            if ordered_subjects:
                # Initialize fact_metric before first usage
                fact_metric = self._detect_fact_metric(t)
                
                span_match = re.search(
                    r"(?:fy\s*)?\d{4}(?:\s*[-/]\s*(?:fy\s*)?\d{4})?",
                    t,
                    re.IGNORECASE,
                )
                period_token = _canon_year_span(span_match.group(0)) if span_match else None
                if not period_token:
                    period_token = _extract_year_phrase(t)

                if (
                    fact_metric
                    and period_token
                    and "-" in period_token
                    and len(ordered_subjects) == 1
                ):
                    return " ".join(["fact-range", ordered_subjects[0], period_token, fact_metric])

                quarter_info = _extract_quarter_phrase(t)
                quarter_token: Optional[str] = None
                if quarter_info:
                    quarter_year, quarter_label = quarter_info
                    period_token = period_token or str(quarter_year)
                    quarter_token = f"{quarter_year}{quarter_label}"
                if (
                    fact_metric
                    and period_token
                    and "-" not in period_token
                    and len(ordered_subjects) == 1
                ):
                    year_token = period_token.split("-", 1)[0]
                    if quarter_token:
                        return " ".join(["fact", ordered_subjects[0], quarter_token, fact_metric])
                    return " ".join(["fact", ordered_subjects[0], year_token, fact_metric])

                # Enhanced compare triggers detection
                compare_triggers = bool(
                    re.search(r"\b(compare|versus|vs\.?|against|between|relative)\b", lower)
                    or "better than" in lower
                    or "outperform" in lower
                    or "beats" in lower
                    or "and" in lower  # Add "and" as compare trigger
                )
                
                # Check if we have enough subjects for comparison
                if compare_triggers and len(ordered_subjects) >= 2:
                    parts = ["compare", *ordered_subjects]
                    if period_token:
                        parts.append(period_token)
                    return " ".join(parts)
                
                # Special handling for "compare X and Y" patterns
                if (re.search(r"\bcompare\b", lower) and 
                    re.search(r"\band\b", lower) and 
                    len(ordered_subjects) >= 2):
                    parts = ["compare", *ordered_subjects]
                    if period_token:
                        parts.append(period_token)
                    return " ".join(parts)

                parts = ["metrics", *ordered_subjects]
                if period_token:
                    parts.append(period_token)
                return " ".join(parts)

        # METRICS
        m = re.match(
            r'^(?:show|give me|get|what (?:are|is) the|list)?\s*(?:kpis?|metrics?|financials?)\s+(?:for\s+)?(?P<ents>.+?)(?:\s+(?:in|for)\s+(?P<per>(?:fy)?\d{4}(?:\s*[-/]\s*(?:fy)?\d{4})?))?\s*$',
            t, re.IGNORECASE,
        )
        if m:
            tickers = [_to_ticker(e) for e in _split_entities(m.group("ents")) if e]
            per = _canon_year_span(m.group("per") or "")
            if tickers:
                return " ".join(["metrics", *tickers, *(per and [per] or [])])

        # COMPARE
        m = re.match(r"^(?:compare|versus|vs\.?|against)\s+(?P<body>.+)$", t, re.IGNORECASE)
        if m:
            body = m.group("body").strip()
            yr = None
            myr = re.search(r"(\d{4})\s*$", body)
            if myr:
                yr = myr.group(1)
                body = body[: myr.start()].strip()
            normalized_body = re.sub(r"\s+(?:vs\.?|versus|against)\s+", ",", body, flags=re.IGNORECASE)
            normalized_body = re.sub(r"[&/]", ",", normalized_body)
            raw_entities: List[str] = []
            for chunk in re.split(r",|\band\b", normalized_body, flags=re.IGNORECASE):
                chunk = chunk.strip()
                if not chunk:
                    continue
                raw_entities.extend(_split_entities(chunk))
            if len(raw_entities) <= 1:
                raw_entities = [part for part in body.split() if part]
            tickers = []
            for entity in raw_entities:
                ticker = _to_ticker(entity)
                if ticker and ticker not in tickers:
                    tickers.append(ticker)
            if len(tickers) >= 2:
                parts = ["compare", *tickers]
                if yr:
                    parts.append(yr)
                return " ".join(parts)

        # FACTS
        m = re.match(r'^(?:show|get|give me)?\s*(?:fact|facts?)\s+(?:for\s+)?(?P<e>.+?)\s+(?:fy)?(?P<y>\d{4})(?:\s+(?P<mtr>[A-Za-z0-9_]+))?\s*$', t, re.IGNORECASE)
        if m:
            tk = _to_ticker(_split_entities(m.group("e"))[0])
            return " ".join(filter(None, ["fact", tk, m.group("y"), m.group("mtr")]))

        # AUDIT
        m = re.match(r'^(?:show|get|give me)?\s*(?:audit|audit trail|ingestion log)s?\s+(?:for\s+)?(?P<e>.+?)(?:\s+(?:fy)?(?P<y>\d{4}))?\s*$', t, re.IGNORECASE)
        if m:
            tk = _to_ticker(_split_entities(m.group("e"))[0])
            return " ".join(filter(None, ["audit", tk, m.group("y")]))

        # INGEST
        m = re.match(r"^(?:ingest|fetch|hydrate|update)\s+(?P<e>.+?)(?:\s+(?P<yrs>\d{1,2}))?\s*$", t, re.IGNORECASE)
        if m:
            tk = _to_ticker(_split_entities(m.group("e"))[0])
            return " ".join(filter(None, ["ingest", tk, m.group("yrs")]))

        # SCENARIO / WHAT-IF
        m = re.match(r"^(?:scenario|what[- ]?if)\s+(?P<e>.+?)\s+(?P<name>[A-Za-z0-9_\-]+)(?:\s+(?P<rest>.*))?$", t, re.IGNORECASE)
        if m:
            tk = _to_ticker(_split_entities(m.group("e"))[0])
            rest = (m.group("rest") or "")
            rest_norm = []
            for tok in re.split(r"\s+", rest.strip()):
                if not tok:
                    continue
                tl = tok.lower()
                if tl.startswith(("rev=", "revenue=")):
                    rest_norm.append("rev=" + tok.split("=", 1)[1])
                elif tl.startswith(("margin=", "ebitda=", "ebitda_margin=")):
                    rest_norm.append("margin=" + tok.split("=", 1)[1])
                elif tl.startswith(("mult=", "multiple=")):
                    rest_norm.append("mult=" + tok.split("=", 1)[1])
            return " ".join(["scenario", tk, m.group("name"), *rest_norm]).strip()

        return None

    # ----------------------------------------------------------------------------------
    # Public API
    # ----------------------------------------------------------------------------------
    def ask(
        self,
        user_input: str,
        *,
        progress_callback: Optional[Callable[[str, str], None]] = None,
        file_ids: Optional[List[str]] = None,  # Explicit file IDs to include in context
    ) -> str:
        """Generate a reply and persist both sides of the exchange."""

        previous_callback = getattr(self, "_active_progress_callback", None)
        self._active_progress_callback = progress_callback

        def emit(stage: str, detail: str) -> None:
            self._progress(stage, detail)

        try:
            self._reset_structured_response()
            emit("start", "Working on your request")
            timestamp = datetime.utcnow()
            database.log_message(
                self.settings.database_path,
                self.conversation.conversation_id,
                role="user",
                content=user_input,
                created_at=timestamp,
            )
            self.conversation.messages.append({"role": "user", "content": user_input})
            emit("message_logged", "Chat history updated")

            reply: Optional[str] = None
            lowered_input = user_input.strip().lower()
            emit("intent_analysis_start", "Analyzing prompt phrasing")
            
            # PERFORMANCE OPTIMIZATION: Query classification for fast path routing
            complexity, query_type, query_metadata = classify_query(user_input)
            emit("query_classification", f"Query classified as {complexity.value} {query_type.value}")
            
            LOGGER.info("\n" + "="*80)
            LOGGER.info("DOCUMENT CONTEXT BUILDING")
            LOGGER.info("="*80)
            LOGGER.info(f"üîç Requested file_ids: {file_ids}")
            LOGGER.info(f"üîç Conversation ID: {getattr(self.conversation, 'conversation_id', None)}")
            LOGGER.info(f"üîç Database path: {self.settings.database_path}")
            
            database_path = Path(self.settings.database_path)
            LOGGER.info(f"üìÅ Chatbot using database path: {database_path}")
            LOGGER.info(f"üìÅ Database exists: {database_path.exists()}")
            
            # Build document context
            # If file_ids provided, use them explicitly; otherwise, auto-fetch from conversation
            # If file_ids is empty list, pass None to trigger automatic conversation-based fetching
            # This matches ChatGPT behavior: files uploaded to conversation are automatically available
            conv_id = getattr(self.conversation, "conversation_id", None)
            LOGGER.info(f"üîç Building document context with conversation_id: {conv_id}")
            LOGGER.info(f"üîç file_ids provided: {file_ids}")
            LOGGER.info(f"üîç database_path: {database_path}")
            
            doc_context = build_uploaded_document_context(
                user_input,
                conv_id,
                database_path,
                file_ids=file_ids if file_ids else None,  # None triggers auto-fetch from conversation
            )
            
            # CRITICAL: If doc_context is None but we have a conversation_id, something is wrong
            # NUCLEAR FALLBACK: Directly query database and build context ourselves
            if not doc_context and conv_id:
                LOGGER.error(f"‚ùå CRITICAL: doc_context is None but conversation_id exists: {conv_id}")
                LOGGER.error(f"   Attempting NUCLEAR FALLBACK: Direct database query and manual context building")
                
                # Direct database check and manual context building
                try:
                    import sqlite3
                    with sqlite3.connect(database_path) as conn:
                        cursor = conn.execute(
                            "SELECT document_id, filename, file_type, content, LENGTH(content) as content_len FROM uploaded_documents WHERE conversation_id = ? AND content IS NOT NULL AND content != '' ORDER BY created_at DESC LIMIT 5",
                            (conv_id,)
                        )
                        rows = cursor.fetchall()
                        db_file_count = len(rows)
                        LOGGER.error(f"   Database shows {db_file_count} files with content for this conversation_id")
                        
                        if db_file_count > 0:
                            LOGGER.error(f"   ‚ö†Ô∏è FILES EXIST but context building returned None!")
                            LOGGER.error(f"   Building context manually as NUCLEAR FALLBACK...")
                            
                            # Manually build context
                            sections = []
                            for doc_id, filename, file_type, content, content_len in rows:
                                if content and content_len > 0:
                                    # Include first 20k chars per file
                                    snippet = content[:20000] if len(content) > 20000 else content
                                    if len(content) > 20000:
                                        snippet += f"\n\n[... {len(content) - 20000:,} more characters ...]"
                                    
                                    section = f"""Filename: {filename}
Type: {file_type or 'unknown'}
Content Length: {content_len} characters

Content:
{snippet}"""
                                    sections.append(section)
                            
                            if sections:
                                banner = "=" * 80
                                header = [
                                    banner,
                                    "UPLOADED FILES (NUCLEAR FALLBACK - Direct Database Query)",
                                    "",
                                    "‚ö†Ô∏è IMPORTANT: The user has uploaded these files and is asking you to analyze them.",
                                    "You MUST use the content from these files to answer the user's question.",
                                    "DO NOT say 'I don't have access to external documents' - you have the file content below.",
                                    "Reference specific parts of the files (filenames, sections, data points) in your response.",
                                    "",
                                    banner,
                                ]
                                body = f"\n\n{banner}\n".join(sections)
                                doc_context = "\n".join(header) + "\n\n" + body
                                
                                LOGGER.error(f"   ‚úÖ NUCLEAR FALLBACK SUCCESS: Built {len(doc_context)} chars of context manually")
                                LOGGER.error(f"   This context will be forced into the LLM messages")
                        else:
                            LOGGER.error(f"   No files with content found in database")
                except Exception as db_e:
                    LOGGER.error(f"   Error in nuclear fallback: {db_e}")
                    import traceback
                    traceback.print_exc()
            
            if file_ids:
                LOGGER.info(f"üìé Explicit file_ids provided: {len(file_ids)} files")
                if doc_context:
                    LOGGER.info(f"‚úÖ Document context built from explicit file_ids: {len(doc_context)} characters")
                    LOGGER.info(f"üìù Context preview (first 500 chars):\n{doc_context[:500]}")
                    # Check for key markers
                    if "UPLOADED FILES" in doc_context or "UPLOADED FINANCIAL DOCUMENTS" in doc_context:
                        LOGGER.info("‚úÖ Context contains file content markers")
                    else:
                        LOGGER.warning("‚ö†Ô∏è Context missing file content markers")
                else:
                    LOGGER.warning(f"‚ùå No document context built for file_ids: {file_ids}")
                    LOGGER.warning(f"   Possible reasons:")
                    LOGGER.warning(f"   - Files not found in database")
                    LOGGER.warning(f"   - Files have no content")
                    LOGGER.warning(f"   - Database query failed")
            elif doc_context:
                LOGGER.info(f"üìÑ Document context auto-fetched from conversation: {len(doc_context)} characters")
                LOGGER.info(f"   All files uploaded to this conversation are now available for analysis")
                LOGGER.info(f"üìù Context preview (first 500 chars):\n{doc_context[:500]}")
            else:
                LOGGER.info("‚ÑπÔ∏è No document context (no file_ids and no files in conversation)")
            LOGGER.info("="*80)
            is_document_followup = bool(doc_context) and self._is_document_followup(user_input)
            normalized_command = self._normalize_nl_to_command(user_input)
            canonical_prompt = self._canonical_prompt(user_input, normalized_command)
            if normalized_command and not is_document_followup:
                emit("intent_analysis_complete", f"Intent candidate: {normalized_command}")
            else:
                if is_document_followup:
                    emit("intent_skip_summary", "Skipping ticker summary for document follow-up")
                else:
                    emit("intent_analysis_complete", "Proceeding with natural language handling")
            cacheable = self._should_cache_prompt(canonical_prompt)

            cached_entry: Optional[_CachedReply] = None
            if cacheable:
                emit("cache_lookup", "Checking recent answers")
                cached_entry = self._get_cached_reply(canonical_prompt)
                if cached_entry:
                    # CRITICAL: Don't use cached replies for forecasting queries
                    # Forecasting queries need fresh ML forecast context, not cached snapshots
                    try:
                        from .context_builder import _is_forecasting_query
                        if _is_forecasting_query(user_input):
                            # Skip cache for forecasting queries - they need fresh forecast context
                            emit("cache_skip", "Skipping cache for forecasting query - need fresh ML forecast context")
                            cached_entry = None
                        else:
                            emit("cache_hit", "Reusing earlier answer from cache")
                            reply = cached_entry.reply
                            self.last_structured_response = copy.deepcopy(cached_entry.structured)
                    except ImportError:
                        # If context_builder not available, use cache as normal
                        emit("cache_hit", "Reusing earlier answer from cache")
                        reply = cached_entry.reply
                        self.last_structured_response = copy.deepcopy(cached_entry.structured)
                else:
                    emit("cache_miss", "No reusable answer found")
            else:
                emit("cache_skip", "Skipping cache for bespoke request")

            if reply is None:
                if lowered_input == "help":
                    emit("help_lookup", "Retrieving help guide")
                    reply = HELP_TEXT
                    emit("help_complete", "Help guide ready")
                else:
                    attempted_intent = False
                    
                    # CRITICAL: Check for forecasting queries BEFORE structured parsing
                    # Forecasting queries should always route to LLM with forecast context
                    is_forecasting = False
                    try:
                        from .context_builder import _is_forecasting_query
                        is_forecasting = _is_forecasting_query(user_input)
                    except ImportError:
                        pass
                    
                    # Check for custom KPI intent first
                    custom_kpi_intent = self._detect_custom_kpi_intent(user_input)
                    if custom_kpi_intent:
                        emit("intent_custom_kpi", f"Custom KPI intent detected: {custom_kpi_intent['action']}")
                        reply = self._handle_custom_kpi_intent(custom_kpi_intent)
                        attempted_intent = True
                    # Check for plugin workflow intent
                    elif not attempted_intent:
                        try:
                            plugin_intent = self._detect_plugin_intent(user_input)
                            if plugin_intent:
                                emit("intent_plugin", f"Plugin intent detected: {plugin_intent['action']}")
                                reply = self._handle_plugin_intent(plugin_intent, user_input)
                                attempted_intent = True
                        except AttributeError:
                            # _detect_plugin_intent not implemented, skip
                            pass
                    # Check for modeling intent
                    if not attempted_intent:
                        modeling_intent = self._detect_modeling_intent(user_input)
                        if modeling_intent:
                            emit("intent_modeling", f"Modeling intent detected: {modeling_intent['action']}")
                            reply = self._handle_modeling_intent(modeling_intent)
                            attempted_intent = True
                        # Check for source trace intent
                        elif not attempted_intent:
                            source_trace_intent = self._detect_source_trace_intent(user_input)
                            if source_trace_intent:
                                emit("intent_source_trace", f"Source trace intent detected")
                                reply = self._handle_source_trace_intent(source_trace_intent)
                                attempted_intent = True
                    elif is_forecasting:
                        # Skip structured parsing for forecasting queries - always use LLM
                        emit("intent_forecasting", "Forecasting query detected - skipping structured parsing, routing to LLM")
                        reply = None
                        attempted_intent = True
                    else:
                        # Try structured parsing first (Priority 1)
                        emit("intent_routed_structured", "Trying structured parsing first")
                        try:
                            reply = self._handle_financial_intent(user_input)
                            attempted_intent = True
                            
                            # CRITICAL: Even if structured parsing returned a reply, check if it's a forecasting query
                            # If so, ignore the structured reply and route to LLM
                            if reply is not None:
                                try:
                                    from .context_builder import _is_forecasting_query
                                    if _is_forecasting_query(user_input):
                                        LOGGER.warning(f"Structured parsing returned reply for forecasting query, ignoring and routing to LLM: {reply[:100]}")
                                        reply = None  # Override structured reply for forecasting queries
                                        emit("intent_forecasting", "Forecasting query detected in structured reply - overriding to route to LLM")
                                except ImportError:
                                    pass
                        except Exception as e:
                            LOGGER.exception(f"Error in _handle_financial_intent: {e}")
                            emit("intent_error", f"Structured parsing error: {str(e)}")
                            attempted_intent = True
                            reply = None
                        
                        # Fallback to normalized command if structured parsing fails
                        # BUT: Skip if it's a forecasting query (normalization might break forecasting intent)
                        if reply is None and not is_forecasting and normalized_command and normalized_command.strip().lower() != lowered_input:
                            emit("intent_normalised", "Falling back to normalized command")
                            emit("intent_routed_structured", f"Executing structured command: {normalized_command}")
                            reply = self._handle_financial_intent(normalized_command)
                            attempted_intent = True
                            
                            # Check again if normalized command is actually a forecasting query
                            if reply is not None:
                                try:
                                    from .context_builder import _is_forecasting_query
                                    if _is_forecasting_query(normalized_command) or _is_forecasting_query(user_input):
                                        LOGGER.warning(f"Normalized command returned reply for forecasting query, ignoring: {reply[:100]}")
                                        reply = None
                                        emit("intent_forecasting", "Forecasting query detected in normalized reply - overriding to route to LLM")
                                except ImportError:
                                    pass
                    
                    if attempted_intent:
                        if reply is not None:
                            emit("intent_complete", "Analytics intent resolved")
                        else:
                            emit("intent_complete", "Intent routing completed with no direct answer")

            if reply is None:
                # Check if this is a question - if so, skip summary and let LLM handle it
                lowered_input = user_input.lower()
                
                # CRITICAL: Check for forecasting queries FIRST (before question check)
                try:
                    from .context_builder import _is_forecasting_query
                    if _is_forecasting_query(user_input):
                        # Forecasting query - ALWAYS use LLM with forecasting context
                        # Don't process through dashboard or structured parsing
                        emit("intent_forecasting", "Forecasting query detected - using LLM with ML forecast context")
                        # Skip to LLM path
                        pass  # Continue to LLM path below
                except ImportError:
                    pass
                
                question_patterns = [
                    # CRITICAL: Contractions MUST come first
                    r'\bwhat\'s\b',  # "what's" contraction - CRITICAL
                    r'\bhow\'s\b',   # "how's" contraction
                    r'\bwho\'s\b',   # "who's" contraction
                    r'\bwhere\'s\b', # "where's" contraction
                    r'\bwhen\'s\b',  # "when's" contraction
                    r'\bwhy\'s\b',   # "why's" contraction
                    
                    # What questions - expanded
                    r'\bwhat\s+(?:is|are|was|were|has|have|will|can|should|would|about|does|did|makes|made|makes|drives|drove|causes|caused|means|meant|shows|showed|indicates|indicated)\b',
                    r'\bwhat\s+(?:happened|changed|improved|declined|increased|decreased|caused|led|resulted|occurred|went|became|turned)\b',
                    r'\bwhat\s+(?:year|quarter|period|time|date|month)\s+(?:did|was|were|is|are)\b',
                    r'\bwhat\s+(?:impact|effect|influence|difference|relationship|correlation)\s+(?:does|did|will|has|have)\b',
                    r'\bwhat\s+(?:are|is)\s+(?:the|their|its)\s+(?:key|main|primary|top|best|worst)\b',
                    r'\bwhat\s+(?:kind|type|sort)\s+of\b',
                    r'\bwhat\s+(?:would|will|could|should)\s+(?:happen|be|occur)\s+if\b',
                    
                    # How questions - expanded
                    r'\bhow\s+(?:much|many|does|did|is|are|has|have|will|can|should|would|about|to|do|profitable|fast|good|bad|strong|weak|long|far|often|well|likely|possible|difficult|easy|important|significant|relevant)\b',
                    r'\bhow\s+(?:has|have|did|does|will|can|should|would)\s+\w+\s+(?:changed|grown|improved|declined|performed|trended|evolved|developed|progressed|accelerated|decelerated)\b',
                    r'\bhow\s+(?:does|do|did|will|can|should|would)\s+\w+\s+(?:affect|impact|influence|relate|connect|link|compare|differ|work|function|operate|perform)\b',
                    r'\bhow\s+(?:to|do\s+you|can\s+you|should\s+you|would\s+you)\s+(?:calculate|compute|determine|find|get|analyze|assess|evaluate|measure|compare)\b',
                    r'\bhow\s+(?:come|so|come\s+about|did\s+it\s+happen|is\s+it\s+that)\b',
                    r'\bhow\s+(?:well|badly|good|bad|strong|weak|healthy|unhealthy|stable|volatile)\s+(?:is|are|was|were)\b',
                    
                    # Why questions - expanded
                    r'\bwhy\s+(?:is|are|was|were|did|does|do|will|can|should|would|has|have|didn\'t|doesn\'t|won\'t|can\'t|shouldn\'t)\b',
                    r'\bwhy\s+(?:did|does|will|has|have)\s+\w+\s+(?:happen|occur|change|improve|decline|increase|decrease|drop|rise|fall|grow|shrink)\b',
                    r'\bwhy\s+(?:is|are|was|were)\s+\w+\s+(?:so|so\s+much|so\s+little|so\s+high|so\s+low|better|worse|higher|lower|more|less)\b',
                    
                    # When questions - expanded
                    r'\bwhen\s+(?:is|are|was|were|did|does|will|can|should|would|has|have)\b',
                    r'\bwhen\s+(?:did|does|will)\s+\w+\s+(?:start|begin|become|turn|report|announce|release|publish|occur|happen|peak|bottom|reach)\b',
                    r'\bwhen\s+(?:was|were|is|are)\s+(?:the|their|its)\s+(?:last|first|next|previous|latest|earliest)\b',
                    
                    # Where questions - expanded
                    r'\bwhere\s+(?:is|are|was|were|can|do|does|did|will|should|would)\b',
                    r'\bwhere\s+(?:does|did|will|can|should|would)\s+\w+\s+(?:come|come\s+from|originate|source|generate|earn|make|get)\b',
                    r'\bwhere\s+(?:is|are|was|were)\s+\w+\s+(?:now|currently|today|at|positioned|located|ranked|situated)\b',
                    
                    # Who/Which questions - expanded
                    r'\bwho\s+(?:is|are|was|were|has|have|does|did|will|can|should|would)\b',
                    r'\bwhich\s+(?:company|stock|one|is|has|have|was|were|does|did|will|can|should|would|performs|performed|grows|grew|makes|made)\b',
                    r'\bwhich\s+(?:is|are|was|were)\s+(?:the|better|best|worst|highest|lowest|largest|smallest|fastest|slowest|strongest|weakest)\b',
                    
                    # Can/Could questions - expanded
                    r'\bcan\s+(?:you|i|we|they|it)\s+(?:show|tell|explain|analyze|calculate|compare|find|get|help|provide|give|do|make|see|check|verify)\b',
                    r'\bcould\s+(?:you|i|we|they|it)\s+(?:show|tell|explain|analyze|calculate|compare|find|get|help|provide|give|do|make|see|check|verify)\b',
                    r'\bcan\s+(?:you|i|we|they|it)\s+(?:help|assist|support)\s+(?:me|us|with)\b',
                    
                    # Should questions - expanded
                    r'\bshould\s+(?:i|we|you|they|it)\s+(?:buy|sell|invest|hold|avoid|consider|expect|anticipate|worry|be\s+concerned)\b',
                    r'\bshould\s+(?:i|we|you|they|it)\s+(?:be|feel|think|believe|assume|expect|anticipate)\b',
                    
                    # Does/Do questions - expanded
                    r'\bdoes\s+\w+\s+(?:have|make|earn|generate|produce|create|own|possess|contain|include|show|indicate|suggest|mean|imply)\b',
                    r'\bdo\s+(?:they|we|you|companies|stocks)\s+(?:have|make|earn|generate|produce|create|own|possess|contain|include|show|indicate|suggest|mean|imply)\b',
                    r'\bdoes\s+\w+\s+(?:mean|imply|suggest|indicate|show|prove|demonstrate|reveal)\b',
                    
                    # Is/Are questions - expanded
                    r'\bis\s+\w+\s+(?:more|less|better|worse|higher|lower|larger|smaller|faster|slower|stronger|weaker|greater|smaller)\b',
                    r'\bis\s+\w+\s+(?:overvalued|undervalued|expensive|cheap|good|bad|risky|safe|strong|weak|profitable|worth|worthwhile|viable|sustainable|healthy|unhealthy|stable|volatile|growing|declining|improving|worsening)\b',
                    r'\bare\s+\w+\s+(?:more|less|better|worse|higher|lower|larger|smaller|faster|slower|stronger|weaker|greater|smaller)\b',
                    r'\bis\s+(?:it|this|that|there)\s+(?:true|correct|accurate|right|wrong|possible|likely|probable|feasible|realistic)\b',
                    r'\bis\s+(?:there|this|that)\s+(?:a|an|any|some)\s+(?:way|method|approach|strategy|solution|option|alternative)\b',
                    
                    # Tell me / Show me / Give me - expanded
                    r'\btell\s+me\s+(?:about|why|how|what|when|where|who|which|if|whether)\b',
                    r'\bshow\s+me\s+(?:the|their|its|how|what|why|when|where|which|if)\b',
                    r'\bgive\s+me\s+(?:the|their|its|a|an|some|information|details|data|analysis|overview|summary)\b',
                    r'\b(?:i\s+want|i\'d\s+like|i\s+need|i\'m\s+looking\s+for)\s+(?:to\s+know|to\s+see|to\s+understand|to\s+learn|information|details|data|analysis)\b',
                    
                    # Explain / Describe / Define - expanded
                    r'\bexplain\s+(?:to\s+me|what|why|how|when|where|which|if|whether)\b',
                    r'\bdescribe\s+(?:to\s+me|what|how|the|their|its)\b',
                    r'\bdefine\s+(?:what|the|their|its)\b',
                    r'\bclarify\s+(?:what|why|how|when|where|which|if|whether)\b',
                    
                    # Compare / Contrast - expanded
                    r'\bcompare\s+(?:the|their|its|how|what|why|when|where|which)\b',
                    r'\bcontrast\s+(?:the|their|its|how|what|why|when|where|which)\b',
                    r'\b(?:what|how)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:difference|similarity|comparison|relationship|correlation)\s+(?:between|among|of|for)\b',
                    
                    # Analyze / Evaluate / Assess - expanded
                    r'\b(?:analyze|analyse)\s+(?:the|their|its|how|what|why|when|where|which|if|whether)\b',
                    r'\bevaluate\s+(?:the|their|its|how|what|why|when|where|which|if|whether)\b',
                    r'\bassess\s+(?:the|their|its|how|what|why|when|where|which|if|whether)\b',
                    r'\breview\s+(?:the|their|its|how|what|why|when|where|which|if|whether)\b',
                    
                    # Calculate / Compute / Determine - expanded
                    r'\b(?:calculate|compute|determine|find|figure\s+out|work\s+out)\s+(?:the|their|its|how|what|why|when|where|which|if|whether)\b',
                    r'\b(?:what|how)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:calculation|computation|formula|method|approach)\b',
                    
                    # If / What if / Scenario - expanded
                    r'\bwhat\s+if\b',
                    r'\bif\b.*\b(?:what|how|then|would|will|should|can|could)\b',
                    r'\b(?:suppose|assume|imagine|pretend|hypothetically)\s+(?:that|if|what|how)\b',
                    r'\b(?:scenario|scenarios|case|cases)\s+(?:where|when|if|what|how)\b',
                    
                    # Forecasting patterns (also questions)
                    r'\b(?:forecast|predict|estimate|project|projection|outlook|forecasting|prediction|estimation)\b',
                    r'\b(?:what|how|when|where|which)\s+(?:will|would|can|could|should|might|may)\s+(?:be|happen|occur|change|improve|decline|increase|decrease|grow|shrink)\b',
                    
                    # Relationship / Correlation questions
                    r'\b(?:what|how)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:relationship|correlation|connection|link|association|tie)\s+(?:between|among|of|for)\b',
                    r'\b(?:does|do|did|will|can|could|should|would)\s+\w+\s+(?:affect|impact|influence|relate|connect|link|correlate|associate)\s+\w+\b',
                    r'\b(?:is|are|was|were)\s+(?:there|this|that)\s+(?:a|an|any)\s+(?:relationship|correlation|connection|link|association|tie)\s+(?:between|among|of|for)\b',
                    
                    # Ranking / Best / Worst questions
                    r'\b(?:which|what)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:best|worst|top|bottom|highest|lowest|largest|smallest|fastest|slowest|strongest|weakest|most|least)\b',
                    r'\b(?:who|which|what)\s+(?:has|have|had)\s+(?:the|their|its)\s+(?:best|worst|top|bottom|highest|lowest|largest|smallest|fastest|slowest|strongest|weakest|most|least)\b',
                    
                    # Recommendation / Advice questions
                    r'\b(?:should|would|could|can|will)\s+(?:i|we|you|they)\s+(?:buy|sell|invest|hold|avoid|consider|expect|anticipate|worry|be\s+concerned|take\s+action)\b',
                    r'\b(?:what|which|how)\s+(?:is|are|was|were)\s+(?:your|the|their|its)\s+(?:recommendation|advice|suggestion|opinion|view|take|perspective|assessment)\b',
                    r'\b(?:do|does|did|will|can|could|should|would)\s+(?:you|they|it)\s+(?:recommend|suggest|advise|think|believe|feel|consider|view|see)\b',
                    
                    # Trend / Change questions
                    r'\b(?:what|how)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:trend|trends|pattern|patterns|direction|trajectory|path|course|movement|shift|change|changes)\b',
                    r'\b(?:has|have|had)\s+\w+\s+(?:changed|improved|declined|increased|decreased|grown|shrunk|evolved|developed|progressed|accelerated|decelerated)\b',
                    r'\b(?:is|are|was|were)\s+\w+\s+(?:growing|declining|improving|worsening|increasing|decreasing|rising|falling|trending|changing|evolving|developing|progressing|accelerating|decelerating)\b',
                    
                    # Cause / Reason questions
                    r'\b(?:what|which|how)\s+(?:is|are|was|were)\s+(?:the|their|its)\s+(?:cause|causes|reason|reasons|factor|factors|driver|drivers|explanation|explanations)\b',
                    r'\b(?:what|which|how)\s+(?:caused|led|resulted|contributed|brought|made|drove|forced|pushed|pulled)\b',
                    r'\b(?:what|which|how)\s+(?:is|are|was|were)\s+(?:behind|driving|leading|resulting|contributing|bringing|making|forcing|pushing|pulling)\b',
                    
                    # Time-based questions
                    r'\b(?:how\s+long|how\s+far|how\s+often|how\s+frequently|how\s+recently|how\s+soon)\b',
                    r'\b(?:since|until|before|after|during|throughout|over|within|for)\s+(?:when|what|how|which)\b',
                    r'\b(?:in|at|on|by|from|to)\s+(?:what|which|when|how)\s+(?:year|quarter|period|time|date|month|week|day)\b',
                    
                    # General question indicators
                    r'\b(?:i\s+don\'t\s+understand|i\s+need\s+help|help\s+me|can\s+you\s+help)\b',
                    r'\b(?:please|pls|plz)\s+(?:explain|show|tell|give|help|calculate|analyze|compare)\b',
                    
                    # Imperative/Command patterns (expanded)
                    r'\b(?:show|display|list|get|find|give|pull|fetch|retrieve|bring|present)\s+(?:me|us|the|their|its)?\s*(?:the|their|its)?\b',
                    r'\b(?:show|display|list|get|find|give|pull|fetch|retrieve|bring|present)\s+(?:me|us)?\s+(?:the|their|its|a|an|some)\s+(?:information|data|details|metrics|financials|results|numbers|figures)\b',
                    r'\b(?:show|display|list|get|find|give|pull|fetch|retrieve|bring|present)\s+(?:me|us)?\s+(?:how|what|why|when|where|which|if|whether)\b',
                    
                    # Request patterns (expanded)
                    r'\b(?:i\'d\s+like|i\s+would\s+like|i\'m\s+interested|i\s+am\s+interested|i\'m\s+curious|i\s+am\s+curious)\s+(?:in|to|about|to\s+see|to\s+know|to\s+understand|to\s+learn)\b',
                    r'\b(?:i\s+want|i\s+need|i\'m\s+looking|i\s+am\s+looking)\s+(?:to\s+see|to\s+know|to\s+understand|to\s+learn|for|information|data|details)\b',
                    r'\b(?:i\'m\s+trying|i\s+am\s+trying|i\'m\s+attempting|i\s+am\s+attempting)\s+(?:to\s+understand|to\s+figure\s+out|to\s+find\s+out|to\s+learn)\b',
                    
                    # Quantitative comparison patterns
                    r'\b(?:times|X\s+times|\d+\s+times)\s+(?:more|less|greater|smaller|larger|higher|lower|better|worse)\s+than\b',
                    r'\b(?:twice|thrice|double|triple|quadruple)\s+(?:as\s+)?(?:much|many|large|small|high|low|good|bad)\b',
                    r'\b(?:half|quarter|third)\s+(?:as\s+)?(?:much|many|large|small|high|low|good|bad)\b',
                    r'\b(?:X%|\d+%|\d+\s+percent|percent|percentage|basis\s+points?)\s+(?:higher|lower|greater|less|more|less|better|worse)\s+than\b',
                    r'\b(?:X\s+times|times|X\s+fold)\s+(?:larger|smaller|bigger|greater|higher|lower)\b',
                    
                    # Negation patterns (expanded)
                    r'\b(?:isn\'t|aren\'t|wasn\'t|weren\'t|doesn\'t|don\'t|didn\'t|won\'t|can\'t|couldn\'t|shouldn\'t|hasn\'t|haven\'t|hadn\'t)\s+\w+\b',
                    r'\b(?:is|are|was|were|does|do|did|will|can|could|should|has|have|had)\s+not\s+\w+\b',
                    r'\b(?:no|not|none|neither|never|nothing|nobody|nowhere)\s+(?:revenue|profit|growth|increase|decrease|change|improvement|decline)\b',
                    r'\b(?:lack|missing|absent|without|devoid)\s+of\b',
                    
                    # Causal patterns
                    r'\b(?:because\s+of|due\s+to|as\s+a\s+result\s+of|owing\s+to|thanks\s+to|attributed\s+to)\b',
                    r'\b(?:caused\s+by|resulted\s+from|stemmed\s+from|arose\s+from|originated\s+from)\b',
                    r'\b(?:led\s+to|resulted\s+in|brought\s+about|gave\s+rise\s+to|contributed\s+to)\b',
                    r'\b(?:as\s+a\s+consequence|consequently|therefore|thus|hence|so)\b',
                    
                    # Quantifier patterns
                    r'\b(?:all|some|most|few|many|several|various|numerous|multiple|each|every)\s+(?:companies?|stocks?|firms?|businesses?|entities?)\b',
                    r'\b(?:all|some|most|few|many|several|various|numerous|multiple|each|every)\s+(?:metrics?|kpis?|ratios?|measures?|indicators?)\b',
                    r'\b(?:all|some|most|few|many|several|various|numerous|multiple|each|every)\s+(?:sectors?|industries?|markets?|segments?)\b',
                    
                    # Progressive/Adverb patterns
                    r'\b(?:increasingly|decreasingly|gradually|rapidly|steadily|consistently|constantly|continuously|slowly|quickly|suddenly|dramatically|significantly|slightly|moderately)\s+(?:profitable|growing|declining|improving|worsening|increasing|decreasing|rising|falling)\b',
                    r'\b(?:is|are|was|were|has|have|had)\s+(?:increasingly|decreasingly|gradually|rapidly|steadily|consistently|constantly|continuously|slowly|quickly|suddenly|dramatically|significantly|slightly|moderately)\b',
                    
                    # Certainty patterns
                    r'\b(?:definitely|certainly|absolutely|undoubtedly|clearly|obviously|evidently|surely|undeniably)\s+(?:profitable|growing|declining|improving|worsening|increasing|decreasing)\b',
                    r'\b(?:probably|possibly|perhaps|maybe|likely|unlikely|probably\s+not|possibly\s+not)\s+(?:to\s+be|to\s+have|to\s+do|that)\b',
                    r'\b(?:is|are|was|were)\s+(?:definitely|certainly|absolutely|probably|possibly|likely|unlikely)\s+(?:profitable|growing|declining|improving|worsening)\b',
                    
                    # Frequency patterns
                    r'\b(?:always|often|sometimes|rarely|never|usually|typically|generally|commonly|frequently|occasionally|seldom|hardly\s+ever)\s+(?:profitable|growing|declining|improving|worsening|increasing|decreasing)\b',
                    r'\b(?:is|are|was|were)\s+(?:always|often|sometimes|rarely|never|usually|typically|generally|commonly|frequently|occasionally|seldom)\s+(?:profitable|growing|declining|improving|worsening)\b',
                    
                    # Aggregation patterns
                    r'\b(?:sum|total|aggregate|combined|collective|cumulative|overall)\s+(?:of|for|across|over)\b',
                    r'\b(?:average|mean|median|mode|midpoint)\s+(?:of|for|across|over)\b',
                    r'\b(?:calculate|compute|determine|find|get)\s+(?:the\s+)?(?:sum|total|aggregate|average|mean|median)\s+(?:of|for|across|over)\b',
                    
                    # Percentage/Ratio patterns
                    r'\b(?:X%|\d+%|\d+\s+percent|percent|percentage|basis\s+points?)\s+(?:of|from|in|for)\b',
                    r'\b(?:ratio|proportion|share|portion|fraction|percentage)\s+(?:of|between|to|for)\b',
                    r'\b(?:what|how\s+much)\s+(?:percent|percentage|share|portion|ratio)\s+(?:of|from|in|for)\b',
                    
                    # Change magnitude patterns
                    r'\b(?:increase|decrease|grow|shrink|rise|fall|jump|drop|surge|plunge|soar|tumble)\s+by\s+(?:X%|\d+%|\d+\s+percent|X\s+times|\d+\s+times)\b',
                    r'\b(?:up|down)\s+by\s+(?:X%|\d+%|\d+\s+percent|X\s+times|\d+\s+times)\b',
                    r'\b(?:increased|decreased|grew|shrunk|rose|fell|jumped|dropped|surged|plunged|soared|tumbled)\s+by\s+(?:X%|\d+%|\d+\s+percent|X\s+times|\d+\s+times)\b',
                    
                    # State/Status patterns
                    r'\b(?:is|are|was|were)\s+(?:currently|presently|now|right\s+now|at\s+present|at\s+the\s+moment)\s+(?:profitable|growing|declining|improving|worsening)\b',
                    r'\b(?:has|have|had)\s+(?:been|become|became|remained|stayed|continued)\s+(?:profitable|growing|declining|improving|worsening)\b',
                    r'\b(?:will|would|should|could|might|may)\s+be\s+(?:profitable|growing|declining|improving|worsening)\b',
                    r'\b(?:was|were)\s+(?:previously|formerly|earlier|before|once|originally)\s+(?:profitable|growing|declining|improving|worsening)\b',
                    
                    # Relative position patterns
                    r'\b(?:above|below|over|under|beyond|exceeding|surpassing|falling\s+short)\s+(?:average|median|mean|benchmark|threshold|target|expectation|norm|standard)\b',
                    r'\b(?:in\s+the\s+)?(?:top|bottom|upper|lower|highest|lowest)\s+(?:X%|\d+%|\d+\s+percent|percentile|quartile|decile)\b',
                    r'\b(?:above|below)\s+(?:or\s+)?(?:at|near)\s+(?:average|median|mean|benchmark|threshold)\b',
                    
                    # Temporal modifier patterns (expanded)
                    r'\b(?:recently|lately|currently|now|presently|today|this\s+year|this\s+quarter|this\s+month)\b',
                    r'\b(?:previously|formerly|historically|in\s+the\s+past|back\s+then|earlier|before|once)\b',
                    r'\b(?:going\s+forward|in\s+the\s+future|ahead|down\s+the\s+road|down\s+the\s+line|eventually|ultimately)\b',
                    r'\b(?:this|last|next|previous|upcoming|coming|past|recent)\s+(?:year|quarter|month|period|fiscal\s+year|fiscal\s+quarter)\b',
                    
                    # Sector/Industry patterns (expanded)
                    r'\b(?:in|within|across|throughout|through)\s+(?:the\s+)?(?:tech|technology|financial|healthcare|energy|consumer|industrial|real\s+estate)\s+(?:sector|industry|market|space)\b',
                    r'\b(?:sector|industry|market)\s+(?:wide|wide\s+trend|wide\s+performance|wide\s+analysis)\b',
                    r'\b(?:across|throughout|through)\s+(?:all|the|multiple|various|different)\s+(?:sectors?|industries?|markets?)\b',
                    
                    # Multi-company patterns
                    r'\b(?:all|both|each|every|some|most|few|many|several)\s+(?:of\s+)?(?:them|these|those|companies|stocks|firms)\b',
                    r'\b(?:together|combined|collectively|jointly|as\s+a\s+group|as\s+a\s+whole|in\s+total|in\s+aggregate)\b',
                    r'\b(?:individually|separately|one\s+by\s+one|one\s+at\s+a\s+time|independently)\b',
                    
                    # Hypothetical/Conditional patterns (expanded)
                    r'\b(?:if|when|assuming|given|provided|supposing|presuming)\s+\w+\s+(?:then|what|how|would|will|should|can|could)\b',
                    r'\b(?:in\s+case|in\s+the\s+event|should|were|had)\s+(?:of|that|X|X\s+to|X\s+happen|X\s+occur)\b',
                    r'\b(?:what|how)\s+(?:if|when|assuming|given|provided|supposing|presuming)\s+\w+\b',
                    r'\b(?:were|had)\s+\w+\s+(?:to|been|have)\s+(?:then|what|how|would|will|should|can|could)\b',
                    
                    # Question tag patterns
                    r'\b(?:isn\'t|aren\'t|wasn\'t|weren\'t|doesn\'t|don\'t|didn\'t|won\'t|can\'t|couldn\'t|shouldn\'t|hasn\'t|haven\'t|hadn\'t)\s+(?:it|they|he|she|we|you)\b',
                    r'\b(?:right|correct|true|accurate|accurate|is\s+that\s+right|is\s+that\s+correct|am\s+i\s+right|am\s+i\s+correct)\b',
                    
                    # Enhanced imperative/command patterns (all, every, any variations)
                    r'\b(?:show|display|list|get|find|give|pull|fetch|retrieve|bring|present|print|output)\s+(?:me|us)?\s+(?:all|every|any|each|some|the\s+list|the\s+list\s+of)\b',
                    r'\b(?:show|display|list|get|find|give|pull|fetch|retrieve|bring|present|print|output)\s+(?:all|every|any|each|some)\s+(?:of\s+)?(?:the|their|its|them|these|those)\b',
                    r'\b(?:list|enumerate|itemize|catalog|inventory)\s+(?:all|every|any|each|some|the|their|its)\b',
                    
                    # Modal verb patterns for speculative queries (expanded)
                    r'\b(?:might|may|could|would|should)\s+(?:be|have|do|make|earn|generate|produce|create|perform|achieve|reach|exceed|surpass)\b',
                    r'\b(?:might|may|could|would|should)\s+(?:not|never)\s+(?:be|have|do|make|earn|generate|produce|create|perform|achieve|reach|exceed|surpass)\b',
                    
                    # NEW: Casual/conversational question patterns
                    r'\b(?:i\s+wonder|i\'m\s+wondering|i\'ve\s+been\s+wondering)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    r'\b(?:curious|interested|intrigued)\s+(?:about|to\s+know|to\s+learn|to\s+find\s+out|to\s+understand)\b',
                    r'\b(?:i\'m\s+curious|i\s+am\s+curious|i\'m\s+interested|i\s+am\s+interested)\s+(?:about|to\s+know|to\s+learn|to\s+find\s+out|to\s+understand)\b',
                    r'\b(?:wondering|thinking|considering|pondering)\s+(?:what|how|why|when|where|which|if|whether|about)\b',
                    
                    # NEW: Clarification request patterns
                    r'\b(?:can\s+you\s+clarify|could\s+you\s+clarify|would\s+you\s+clarify|please\s+clarify)\b',
                    r'\b(?:what\s+do\s+you\s+mean|what\s+does\s+that\s+mean|what\s+does\s+this\s+mean)\b',
                    r'\b(?:i\s+don\'t\s+get\s+it|i\s+don\'t\s+understand|i\'m\s+confused|i\'m\s+not\s+sure)\b',
                    r'\b(?:could\s+you\s+explain|can\s+you\s+explain|would\s+you\s+explain|please\s+explain)\s+(?:that|this|it|more|further|in\s+detail)\b',
                    
                    # NEW: Follow-up question patterns
                    r'\b(?:what\s+about|how\s+about|what\s+else|anything\s+else|any\s+other)\b',
                    r'\b(?:and\s+what|and\s+how|and\s+why|and\s+when|and\s+where|and\s+which)\b',
                    r'\b(?:also|additionally|furthermore|moreover|besides|in\s+addition)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    r'\b(?:speaking\s+of|on\s+that\s+note|while\s+we\'re\s+at\s+it|by\s+the\s+way)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    
                    # NEW: Comparative question patterns (expanded)
                    r'\b(?:how\s+does|how\s+do|how\s+did)\s+\w+\s+(?:stack\s+up|compare|measure\s+up|fare)\s+(?:against|to|with|versus|vs)\b',
                    r'\b(?:is|are|was|were)\s+\w+\s+(?:better|worse|more|less|higher|lower|larger|smaller)\s+(?:than|compared\s+to|compared\s+with|relative\s+to)\b',
                    r'\b(?:which\s+is|which\s+are|which\s+was|which\s+were)\s+(?:better|worse|more|less|higher|lower|larger|smaller|best|worst|most|least)\b',
                    r'\b(?:what\'s\s+the\s+difference|what\s+are\s+the\s+differences|what\'s\s+the\s+gap)\s+(?:between|among|of|for)\b',
                    
                    # NEW: Contextual question patterns
                    r'\b(?:in\s+context|in\s+perspective|relatively\s+speaking|comparatively\s+speaking)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    r'\b(?:given|considering|taking\s+into\s+account|in\s+light\s+of)\s+(?:that|this|the\s+fact|the\s+circumstances)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    r'\b(?:all\s+things\s+considered|everything\s+considered|overall)\s+(?:what|how|why|when|where|which|if|whether)\b',
                    
                    # NEW: Action-oriented question patterns
                    r'\b(?:what\s+should|what\s+would|what\s+could|what\s+can)\s+(?:i|we|you|they)\s+(?:do|make|take|consider|think|expect|anticipate)\b',
                    r'\b(?:how\s+should|how\s+would|how\s+could|how\s+can)\s+(?:i|we|you|they)\s+(?:proceed|approach|handle|deal\s+with|manage|address)\b',
                    r'\b(?:what\'s\s+the\s+best|what\'s\s+the\s+optimal|what\'s\s+the\s+ideal)\s+(?:way|approach|strategy|method|course\s+of\s+action|action|move)\s+(?:to|for|in)\b',
                    
                    # NEW: Uncertainty/exploration patterns
                    r'\b(?:i\'m\s+not\s+sure|i\'m\s+uncertain|i\'m\s+unsure)\s+(?:what|how|why|when|where|which|if|whether|about)\b',
                    r'\b(?:trying\s+to\s+figure\s+out|trying\s+to\s+understand|trying\s+to\s+learn)\s+(?:what|how|why|when|where|which|if|whether|about|the)\b',  # Added "the" for "trying to figure out the revenue"
                    r'\b(?:help\s+me\s+figure\s+out|help\s+me\s+understand|help\s+me\s+learn)\s+(?:what|how|why|when|where|which|if|whether|about)\b',
                    
                    # NEW: Preference/choice patterns
                    r'\b(?:which\s+would\s+you|which\s+should\s+i|which\s+do\s+you)\s+(?:choose|pick|prefer|recommend|suggest|advise)\b',
                    r'\b(?:what\s+would\s+you|what\s+should\s+i|what\s+do\s+you)\s+(?:choose|pick|prefer|recommend|suggest|advise)\b',
                    r'\b(?:between|among)\s+\w+\s+(?:which|what)\s+(?:is|are|would\s+be|should\s+be)\s+(?:better|best|preferred|recommended)\b',
                    
                    # NEW: Validation/confirmation patterns
                    r'\b(?:is\s+that\s+right|is\s+that\s+correct|is\s+that\s+accurate|am\s+i\s+right|am\s+i\s+correct)\b',
                    r'\b(?:can\s+you\s+confirm|could\s+you\s+confirm|would\s+you\s+confirm|please\s+confirm)\b',
                    r'\b(?:verify|validate|check|double\s+check)\s+(?:that|if|whether|what|how|why|when|where|which)\b',
                    
                    # NEW: Existence/availability patterns
                    r'\b(?:is\s+there|are\s+there|was\s+there|were\s+there)\s+(?:any|a|an|some)\s+(?:way|method|approach|strategy|option|alternative|solution|possibility)\b',
                    r'\b(?:does|do|did)\s+(?:there|it|this|that)\s+(?:exist|exist\s+any|exist\s+a|exist\s+an)\b',
                    r'\b(?:available|unavailable|accessible|inaccessible)\s+(?:for|to|in|at|on|from)\b',
                    
                    # NEW: Scope/coverage patterns
                    r'\b(?:what\s+all|what\s+else|what\s+other|everything|anything|nothing|something)\s+(?:is|are|was|were|does|do|did|will|would|can|could|should)\b',
                    r'\b(?:in\s+detail|in\s+full|completely|thoroughly|comprehensively|extensively)\s+(?:what|how|why|when|where|which|if|whether|about)\b',
                    r'\b(?:give\s+me|show\s+me|tell\s+me)\s+(?:everything|anything|something|all|the\s+full|the\s+complete|the\s+detailed)\s+(?:about|on|regarding|concerning)\b',
                    r'\b(?:is|are|was|were)\s+(?:likely|unlikely|possible|impossible|probable|improbable|certain|uncertain)\s+(?:to\s+be|to\s+have|to\s+do|that|to)\b',
                    r'\b(?:might|may|could|would|should)\s+(?:have|had)\s+(?:been|become|became|done|made|earned|generated|produced|created|performed|achieved|reached)\b',
                    
                    # Enhanced aggregation patterns
                    r'\b(?:what|how|show|tell|calculate|compute|determine|find|get|give)\s+(?:is|are|was|were|the|their|its)\s+(?:sum|total|aggregate|combined|collective|cumulative|overall|grand\s+total)\b',
                    r'\b(?:calculate|compute|determine|find|get|give|show|tell)\s+(?:the|their|its)?\s+(?:sum|total|aggregate|combined|collective|cumulative|overall|grand\s+total)\s+(?:of|for|across|over)\b',
                    r'\b(?:sum|total|aggregate|combined|collective|cumulative|overall|grand\s+total)\s+(?:of|for|across|over|all|every|each)\b',
                    r'\b(?:add|sum|total|combine|aggregate)\s+(?:up|together|all|everything)\b',
                    r'\b(?:what|how)\s+(?:is|are|was|were)\s+(?:the|their|its)?\s+(?:combined|total|overall|aggregate|collective)\s+(?:value|amount|figure|number|sum)\b',
                    
                    # Enhanced modal and speculative patterns
                    r'\b(?:i\s+think|i\s+believe|i\s+expect|i\s+anticipate|i\s+predict|i\s+estimate|i\s+guess|i\s+assume)\s+(?:that|it|they|he|she|we|you)\s+(?:is|are|was|were|will|would|should|might|may|could)\b',
                    r'\b(?:do|does|did)\s+(?:you|they|it|he|she|we)\s+(?:think|believe|expect|anticipate|predict|estimate|guess|assume)\s+(?:that|it|they|he|she|we|you)\b',
                    r'\b(?:would|could|should|might|may)\s+(?:you|they|it|he|she|we)\s+(?:think|believe|expect|anticipate|predict|estimate|guess|assume)\s+(?:that|it|they|he|she|we|you)\b',
                    
                    # Additional question tag patterns (expanded)
                    r'\b(?:isn\'t|aren\'t|wasn\'t|weren\'t|doesn\'t|don\'t|didn\'t|won\'t|can\'t|couldn\'t|shouldn\'t|hasn\'t|haven\'t|hadn\'t)\s+(?:it|that|this|they|he|she|we|you)\s+(?:right|correct|true|accurate|so)\b',
                    r'\b(?:is|are|was|were|does|do|did|will|can|could|should|has|have|had)\s+(?:that|this|it|they|he|she|we|you)\s+(?:right|correct|true|accurate|so)\b',
                    r'\b(?:am|is|are|was|were)\s+(?:i|we|you|they|he|she)\s+(?:right|correct|accurate|wrong|incorrect)\b',
                    r'\b(?:that|this|it)\s+(?:is|are|was|were)\s+(?:right|correct|true|accurate|so|yes|no)\b',
                    r'\b(?:confirm|verify|check)\s+(?:that|if|whether|for\s+me)\b',
                    
                    # Interrogative statement patterns (statements that are questions)
                    r'\b(?:i\s+)?(?:wonder|question|ask)\s+(?:if|whether|about|why|how|what|when|where|who|which)\b',
                    r'\b(?:curious|interested|wondering)\s+(?:if|whether|about|why|how|what|when|where|who|which)\b',
                    r'\b(?:help\s+me|guide\s+me|assist\s+me)\s+(?:understand|figure\s+out|determine|find\s+out|learn|know)\b',
                    
                    # Domain-specific financial terms as question starters
                    r'\b(?:what|how|tell\s+me\s+about|show\s+me|calculate|compute|find)\s+(?:the\s+)?(?:alpha|beta|sharpe\s+ratio|sortino\s+ratio|information\s+ratio|tracking\s+error|jensen\'?s?\s+alpha|treynor\s+ratio|calmar\s+ratio|max\s+drawdown|var|value\s+at\s+risk|cvar|conditional\s+var|expected\s+shortfall)\b',
                    r'\b(?:alpha|beta|sharpe|sortino|information\s+ratio|tracking\s+error|jensen\'?s?\s+alpha|treynor|calmar|max\s+drawdown|var|cvar|expected\s+shortfall)\s+(?:of|for|is|are|was|were|does|did|will|can|should)\b',
                    r'\b(?:what\'?s?|what\s+is|how\s+is|tell\s+me)\s+(?:the\s+)?(?:alpha|beta|sharpe|sortino|information\s+ratio|tracking\s+error|jensen\'?s?\s+alpha|treynor|calmar|max\s+drawdown|var|cvar|expected\s+shortfall)\b',
                    r'\b(?:show|calculate|compute|find|get|tell)\s+(?:me|us)?\s+(?:the\s+)?(?:alpha|beta|sharpe|sortino|information\s+ratio|tracking\s+error|jensen\'?s?\s+alpha|treynor|calmar|max\s+drawdown|var|cvar|expected\s+shortfall)\b',
                    r'\b(?:risk\s+metrics?|risk\s+measures?|performance\s+metrics?|portfolio\s+metrics?)\s+(?:like|such\s+as|including|for)\b',
                    
                    # Casual/slang expression patterns
                    r'\b(?:how\'?s?|how\s+is|how\s+are)\s+\w+\s+(?:doing|going|performing|trending|looking)\b',
                    r'\b(?:what\'?s?|what\s+is)\s+(?:the\s+)?(?:deal|story|scoop|latest|update|news)\s+(?:with|on|about)\b',
                    r'\b(?:what\'?s?|what\s+is)\s+\w+\s+(?:up\s+to|about|dealing\s+with|working\s+on)\b',
                    r'\b(?:how\'?s?\s+)?(?:things|stuff|it|they|that|this)\s+(?:going|doing|looking|trending|shaping\s+up)\b',
                    r'\b(?:what\'?s?|what\s+is)\s+(?:the\s+)?(?:situation|status|state|condition|position)\s+(?:with|on|for|of)\b',
                    r'\b(?:give\s+me|tell\s+me|show\s+me)\s+(?:the\s+)?(?:rundown|lowdown|scoop|411|update|status)\s+(?:on|about|for|of)\b',
                    r'\b(?:what\'?s?|what\s+is)\s+(?:happening|going\s+on|up|the\s+deal)\s+(?:with|at|in)\b',
                    r'\b(?:is|are|was|were)\s+\w+\s+(?:any\s+good|worth\s+it|doing\s+well|performing\s+well)\b',
                    r'\b(?:check|look\s+at|see|review)\s+(?:how|what|where)\s+\w+\s+(?:is|are|was|were)\s+(?:doing|going|performing)\b',
                    
                    # Multi-part query patterns (conjunctions and continuation)
                    r'\b(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display)\s+.*?\s+(?:and\s+also|and\s+then|also|plus|as\s+well|in\s+addition|additionally|furthermore|moreover|besides|on\s+top\s+of)\b',
                    r'\b.*?\s+(?:and|also|plus|as\s+well|in\s+addition)\s+(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display|what|how|why|when|where|which)\b',
                    r'\b(?:both|all|each|every)\s+.*?\s+(?:and|,)\s+.*?\b',  # "both X and Y", "all X, Y, and Z"
                    r'\b(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display)\s+(?:both|all|each|every)\s+.*?\s+(?:and|,)\s+.*?\b',
                    r'\b(?:what|how|why|when|where|which|tell\s+me|show\s+me)\s+.*?\s+(?:and|,)\s+(?:also|what|how|why|when|where|which|tell\s+me|show\s+me)\b',
                    r'\b(?:first|second|third|then|next|after\s+that|finally|lastly)\s+(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display|what|how|why|when|where|which)\b',
                    r'\b(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display)\s+.*?\s+(?:,|and)\s+(?:analyze|compare|show|tell|give|calculate|compute|find|get|list|display|what|how|why|when|where|which)\b',
                    r'\b(?:as\s+well\s+as|along\s+with|together\s+with|in\s+addition\s+to|besides|on\s+top\s+of)\s+.*?\b',
                ]
                # Use module-level re import explicitly
                import re as re_module
                is_question = any(re_module.search(pattern, lowered_input) for pattern in question_patterns)
                
                # CRITICAL: Check for filter queries - these should NEVER generate dashboards
                # Comprehensive sector patterns with all variations
                SECTOR_PATTERNS = (
                    # Technology variations
                    r'tech|technology|software|hardware|semiconductor|semis?|chip|it\b',
                    # Financial variations  
                    r'financial?|finance|banking|banks?|insurance|fintech',
                    # Healthcare variations
                    r'healthcare|health|pharma|pharmaceutical|biotech|medical|drug',
                    # Energy variations
                    r'energy|oil|gas|petroleum|renewables?|clean energy',
                    # Consumer variations
                    r'consumer|retail|e-commerce|ecommerce|cpg|discretionary|staples',
                    # Industrial variations
                    r'industrial|manufacturing|aerospace|defense|machinery',
                    # Real Estate variations
                    r'real estate|property|reit|reits',
                    # Utilities variations
                    r'utilit(?:y|ies)|power|electric|water|infrastructure',
                    # Materials variations
                    r'materials?|mining|metals?|chemicals?|commodit(?:y|ies)',
                    # Communication variations
                    r'communication|telecom|media|entertainment|broadcasting',
                )
                sector_pattern = '|'.join(f'(?:{p})' for p in SECTOR_PATTERNS)
                
                filter_query_patterns = [
                    # CRITICAL: Sector-to-sector comparison queries (e.g., "How does finance compare to tech?")
                    rf'\b(?:how|what)\s+(?:does|do)\s+(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry)\s+compare\s+(?:to|with|vs\.?)\s+(?:the\s+)?(?:{sector_pattern})',
                    rf'\b(?:how|what)\s+(?:does|do)\s+(?:the\s+)?(?:{sector_pattern})\s+compare\s+(?:to|with|vs\.?)\s+(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry)',
                    rf'\b(?:compare|comparing|comparison)\s+(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry)\s+(?:to|with|vs\.?)\s+(?:the\s+)?(?:{sector_pattern})',
                    rf'\b(?:{sector_pattern})\s+(?:sector|industry)\s+(?:vs\.?|versus|compared to)\s+(?:the\s+)?(?:{sector_pattern})',
                    # "Analyze the [sector] sector: which companies..." (sector mentioned earlier in sentence)
                    rf'\b(?:analyze|review|examine|assess)\s+(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry).*\bwhich\s+(?:companies|stocks|firms)',
                    # "Show/List/Find [sector] companies"
                    rf'\b(?:show|list|find|get|give)\s+(?:me\s+)?(?:all\s+)?(?:the\s+)?(?:{sector_pattern})\s+(?:companies|stocks|firms)',
                    # "Which [sector] company"
                    rf'\bwhich\s+(?:{sector_pattern})\s+(?:company|stock|firm)',
                    # "Which companies in [sector]"
                    rf'\bwhich\s+(?:companies|stocks|firms)\s+(?:in\s+)?(?:the\s+)?(?:{sector_pattern})',
                    # "Companies in [sector] sector/industry"
                    rf'\b(?:companies|stocks|firms)\s+(?:in\s+)?(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry)',
                    # "[Sector] sector companies"
                    rf'\b(?:{sector_pattern})\s+(?:sector|industry).*\b(?:companies|stocks|firms)',
                    # "In the [sector] sector, which companies..."
                    rf'\bin\s+(?:the\s+)?(?:{sector_pattern})\s+(?:sector|industry).*\bwhich\s+(?:companies|stocks|firms)',
                    # Revenue/sales filters
                    r'\b(?:companies|stocks|firms)\s+with\s+(?:revenue|sales)\s+(?:around|about|near|over|under|above|below)',
                    # Growth filters
                    r'\b(?:companies|stocks|firms)\s+with\s+(?:growing|increasing|declining|decreasing)\s+(?:revenue|sales|earnings|profit)',
                    r'\b(?:growing|increasing|high-growth|fast-growing)\s+(?:companies|stocks|firms)',
                    # Market cap filters
                    r'\b(?:large|small|mid|mega)\s+cap\s+(?:companies|stocks)',
                    # Profit margin filters with conditions
                    r'\b(?:companies|stocks|firms)\s+(?:have|with)?\s+(?:profit\s+)?margins?\s+(?:above|over|greater than|>)',
                    # Multiple criteria (growth AND margin AND cash flow)
                    r'\b(?:companies|stocks|firms).*(?:margins?|growth|cash flow).*(?:above|over|>).*(?:and|,)',
                ]
                is_filter_query = any(re.search(pattern, lowered_input) for pattern in filter_query_patterns)
                
                # CRITICAL: Check for specific metric queries - route to LLM for focused response
                # Pattern: "show [company]'s [specific metric]" or "what's [company]'s [metric]"
                specific_metric_patterns = [
                    r'\b(?:show|tell|give)\s+(?:me\s+)?[\w\s]+?\'s\s+(?:revenue|profit|margin|earnings|ebitda|cash|p/e|pe|roi|roe|roic|growth|debt)',
                    r'\bwhat\'s\s+[\w\s]+?\'s\s+(?:revenue|profit|margin|earnings|ebitda|cash|p/e|pe|roi|roe|roic|growth|debt)',
                ]
                is_specific_metric = any(re.search(pattern, lowered_input) for pattern in specific_metric_patterns)
                
                # CRITICAL: For filter queries, IMMEDIATELY clear any existing dashboards
                # This prevents old cached dashboards from being shown
                if is_filter_query:
                    self.last_structured_response["dashboard"] = None
                    self.last_structured_response["summary"] = None
                    emit("filter_query_dashboard_clear", "Filter query detected - clearing all dashboards")
                
                # Only do ticker summary for bare ticker mentions, NOT questions, forecasting, filter queries, or specific metric queries
                if not is_question and not is_filter_query and not is_specific_metric:
                    # Check for dashboard keyword first
                    dashboard_keywords = ["dashboard", "full dashboard", "comprehensive dashboard", 
                                         "detailed dashboard", "show me dashboard", "give me dashboard"]
                    should_build_dashboard = any(kw in lowered_input for kw in dashboard_keywords)
                    
                    if should_build_dashboard:
                        # Dashboard request - detect all tickers
                        all_tickers = self._detect_tickers(user_input)
                        emit("ticker_detection", f"Detected tickers from '{user_input}': {all_tickers}")
                        if len(all_tickers) >= 2:
                            # Multi-ticker dashboard - build dashboards for each ticker
                            emit("multi_ticker_dashboard", f"Building dashboards for {len(all_tickers)} companies")
                            
                            # Build individual dashboards for each ticker
                            dashboards = []
                            for ticker in all_tickers:
                                dashboard_payload = build_cfi_dashboard_payload(self.analytics_engine, ticker)
                                if dashboard_payload:
                                    display = _display_ticker_symbol(_normalise_ticker_symbol(ticker))
                                    dashboards.append({
                                        "kind": "cfi-classic",
                                        "ticker": display,
                                        "payload": dashboard_payload,
                                    })
                            
                            if dashboards:
                                # Store all dashboards for multi-ticker display
                                # Use a special structure to indicate multiple dashboards
                                self.last_structured_response["dashboard"] = {
                                    "kind": "multi-cfi-classic",
                                    "dashboards": dashboards,
                                }
                                company_names = [d["payload"].get("meta", {}).get("company", d["ticker"]) for d in dashboards]
                                reply = f"Displaying financial dashboards for {', '.join(company_names)}."
                                emit("summary_complete", f"Dashboards prepared for {len(dashboards)} companies")
                            else:
                                reply = "Unable to build dashboards for the requested companies."
                                emit("summary_unavailable", "Dashboard data unavailable")
                        elif len(all_tickers) == 1:
                            # Single-ticker dashboard
                            ticker = all_tickers[0]
                            emit("summary_attempt", f"Compiling dashboard for {ticker}")
                            dashboard_payload = build_cfi_dashboard_payload(self.analytics_engine, ticker)
                            if dashboard_payload:
                                display = _display_ticker_symbol(_normalise_ticker_symbol(ticker))
                                self.last_structured_response["dashboard"] = {
                                    "kind": "cfi-classic",
                                    "ticker": display,
                                    "payload": dashboard_payload,
                                }
                                # Return brief message since dashboard shows all the data
                                company_name = dashboard_payload.get("meta", {}).get("company", "")
                                if company_name and company_name.upper() != display.upper():
                                    reply = f"Displaying financial dashboard for {company_name} ({display})."
                                else:
                                    reply = f"Displaying financial dashboard for {display}."
                                emit("summary_complete", f"Dashboard prepared for {ticker}")
                            else:
                                emit("summary_unavailable", f"Dashboard unavailable for {ticker}")
                        # If no tickers detected, fall through to other handlers
                    else:
                        if is_document_followup:
                            emit("intent_skip_summary", "Skipping summary heuristics for document follow-up")
                        else:
                            # Not a dashboard request - check for regular summary
                            # CRITICAL: Skip summary for forecasting queries - they should use LLM with forecast context
                            is_forecasting = False
                            try:
                                from .context_builder import _is_forecasting_query
                                is_forecasting = _is_forecasting_query(user_input)
                            except ImportError:
                                pass
                            
                            if not is_forecasting:
                                summary_target = self._detect_summary_target(user_input, normalized_command)
                                if summary_target:
                                    # Use text summary for regular ticker queries (bare mentions only)
                                    emit("summary_attempt", f"Compiling text summary for {summary_target}")
                                    reply = self._get_ticker_summary(summary_target, user_input)
                                    if reply:
                                        emit("summary_complete", f"Text snapshot prepared for {summary_target}")
                                    else:
                                        emit("summary_unavailable", f"No cached snapshot available for {summary_target}")
                            else:
                                # Forecasting query - skip summary generation, use LLM with forecast context
                                emit("intent_forecasting", "Forecasting query detected - skipping snapshot, using LLM with ML forecast context")
            if reply is None:
                emit("context_build_start", "Gathering enhanced financial context")
                
                # Check if this is a forecasting query FIRST - forecasting queries need special handling
                is_forecasting = False
                try:
                    from .context_builder import _is_forecasting_query
                    is_forecasting = _is_forecasting_query(user_input)
                except ImportError:
                    pass
                
                context_detail = ""
                context = None

                portfolio_context = self._build_portfolio_context(user_input)

                if portfolio_context:
                    context = portfolio_context
                    context_detail = "Portfolio context compiled - using actual portfolio data"
                    LOGGER.info("Using portfolio context for query")
                else:
                    # Try RAG Orchestrator first (with all advanced features)
                    rag_orchestrator = self._get_rag_orchestrator()
                    use_rag_orchestrator = rag_orchestrator is not None
                    
                    if use_rag_orchestrator:
                        try:
                            emit("rag_orchestrator_start", "Using advanced RAG pipeline with reranking, fusion, and multi-hop")
                            conversation_id = getattr(self.conversation, "conversation_id", None)
                            
                            # Process query through complete RAG pipeline
                            rag_prompt, rag_result, rag_metadata = rag_orchestrator.process_query(
                                query=user_input,
                                conversation_id=conversation_id,
                                user_id=None,  # Could extract from session if available
                            )
                            
                            # CRITICAL: Immediately check if RAG returned empty results
                            # If so, fall back to build_financial_context right away
                            num_sec_docs = rag_metadata.get('num_sec_docs', 0)
                            num_uploaded_docs = rag_metadata.get('num_uploaded_docs', 0)
                            
                            # If RAG returned 0 documents, immediately fall back
                            if num_sec_docs == 0 and num_uploaded_docs == 0:
                                LOGGER.warning(
                                    f"RAG returned 0 documents (SEC: {num_sec_docs}, uploaded: {num_uploaded_docs}), "
                                    f"immediately falling back to build_financial_context"
                                )
                                use_rag_orchestrator = False
                                context = None
                                # Skip the rest of RAG processing - raise exception to trigger fallback
                                raise ValueError("RAG returned 0 documents - forcing fallback to build_financial_context")
                            
                            # Check if grounded decision says we shouldn't answer
                            if not rag_metadata.get("should_answer", True):
                                # Return early with suggested response
                                grounded_decision_obj = rag_metadata.get("grounded_decision")
                                suggested_response = None
                                
                                # Handle both dict and object access
                                if isinstance(grounded_decision_obj, dict):
                                    suggested_response = grounded_decision_obj.get("suggested_response")
                                elif hasattr(grounded_decision_obj, "suggested_response"):
                                    suggested_response = grounded_decision_obj.suggested_response
                                
                                if suggested_response:
                                    reply = suggested_response
                                    emit("rag_grounded_decision", "Low confidence - returning suggested response")
                                    LOGGER.info(f"Grounded decision: {rag_metadata.get('grounded_decision', 'N/A')}")
                                    # Continue to reply processing below (don't skip LLM path, just use the suggested response)
                                    context = None  # No context needed since we have the response
                                    context_detail = "Grounded decision: low confidence response"
                                else:
                                    context = rag_prompt
                                    context_detail = f"RAG context (confidence: {rag_metadata.get('confidence', 0.0):.2f})"
                            else:
                                # CRITICAL: Check if RAG context is empty or doesn't contain financial data
                                # If so, fall back to build_financial_context to ensure we have data
                                # Also check metadata for empty retrieval
                                num_sec_docs = rag_metadata.get('num_sec_docs', 0)
                                num_uploaded_docs = rag_metadata.get('num_uploaded_docs', 0)
                                confidence = rag_metadata.get('confidence', 0.0)
                                
                                has_financial_data = (
                                    rag_prompt and 
                                    len(rag_prompt) > 200 and  # Not just a minimal prompt
                                    (num_sec_docs > 0 or num_uploaded_docs > 0) and  # Has retrieved documents
                                    ('FINANCIAL DATA' in rag_prompt or 
                                     'DATABASE RECORDS' in rag_prompt or
                                     'Revenue:' in rag_prompt or
                                     'revenue' in rag_prompt.lower() or
                                     'MANDATORY DATA' in rag_prompt or
                                     'Net Income:' in rag_prompt or
                                     'net income' in rag_prompt.lower())
                                )
                                
                                if not has_financial_data:
                                    LOGGER.warning(
                                        f"RAG context appears empty or lacks financial data "
                                        f"(docs: {num_sec_docs} SEC + {num_uploaded_docs} uploaded, "
                                        f"confidence: {confidence:.2f}, length: {len(rag_prompt) if rag_prompt else 0}), "
                                        f"falling back to build_financial_context"
                                    )
                                    use_rag_orchestrator = False  # Force fallback
                                    context = None  # Clear the RAG context so build_financial_context is used
                                else:
                                    context = rag_prompt
                                    context_detail = (
                                        f"RAG context (confidence: {rag_metadata.get('confidence', 0.0):.2f}, "
                                        f"complexity: {rag_metadata.get('complexity', 'unknown')}, "
                                        f"docs: {rag_metadata.get('num_sec_docs', 0)} SEC + {rag_metadata.get('num_uploaded_docs', 0)} uploaded)"
                                    )
                                    LOGGER.info(
                                        f"RAG Orchestrator: confidence={rag_metadata.get('confidence', 0.0):.3f}, "
                                        f"complexity={rag_metadata.get('complexity', 'unknown')}, "
                                        f"metrics={rag_metadata.get('num_metrics', 0)}, "
                                        f"sec_docs={rag_metadata.get('num_sec_docs', 0)}, "
                                        f"uploaded_docs={rag_metadata.get('num_uploaded_docs', 0)}"
                                    )
                        except Exception as e:
                            LOGGER.warning(f"RAG Orchestrator failed, falling back to legacy context building: {e}", exc_info=True)
                            use_rag_orchestrator = False
                    
                    # Fallback to legacy context building if RAG Orchestrator not available or failed
                    if not use_rag_orchestrator:
                        # Use build_financial_context for all queries (including forecasting)
                        context = build_financial_context(
                            query=user_input,
                            analytics_engine=self.analytics_engine,
                            database_path=str(self.settings.database_path),
                            max_tickers=3,
                            include_macro_context=True
                        )

                        if is_forecasting and not context:
                            LOGGER.warning("Forecasting query detected but context is empty - will still call LLM")
                            context = f"\n{'='*80}\nüìä FORECASTING QUERY DETECTED\n{'='*80}\n"
                            context += f"**Query:** {user_input}\n"
                            context += "**Note:** This is a forecasting query. Please provide a forecast based on available data.\n"
                            context += f"{'='*80}\n"

                        context_detail = "Context compiled" if context else "Context not required"
                        
                        # ENHANCED: Ensure LLM always has context, even if empty
                        # This prevents the LLM from being called with no context at all
                        if not context:
                            # Provide minimal helpful context for the LLM
                            context = f"\n{'='*80}\n"
                            context += f"üìä USER QUERY\n"
                            context += f"{'='*80}\n\n"
                            context += f"**Query:** {user_input}\n\n"
                            context += f"**Note:** No specific financial data was extracted from this query. "
                            context += f"Please provide a helpful response based on your knowledge. "
                            context += f"If the query is unclear, ask for clarification in a friendly way. "
                            context += f"Never say 'I don't understand' - always try to help!\n"
                            context += f"{'='*80}\n"
                            context_detail = "Minimal context provided for LLM"
                        
                        # Add uploaded document context (only if not using RAG Orchestrator, which already includes it)
                        if doc_context:
                            LOGGER.info("\n" + "="*80)
                            LOGGER.info("ADDING DOCUMENT CONTEXT TO LLM PROMPT")
                            LOGGER.info("="*80)
                            LOGGER.info(f"üìé Document context length: {len(doc_context)} characters")
                            LOGGER.info(f"üìù Context preview (first 300 chars):\n{doc_context[:300]}")
                            
                            context = f"{context}\n\n{doc_context}" if context else doc_context
                            context_detail = (
                                f"{context_detail} + uploaded documents"
                                if context_detail and context_detail != "Context not required"
                                else "Uploaded document context attached"
                            )
                            
                            # Verify context includes file content
                            if "UPLOADED FILES" in doc_context or "UPLOADED FINANCIAL DOCUMENTS" in doc_context:
                                LOGGER.info("‚úÖ Document context contains file content markers")
                            else:
                                LOGGER.warning("‚ö†Ô∏è Document context missing file content markers")
                            
                            LOGGER.info(f"üìä Final context length: {len(context)} characters")
                            LOGGER.info("="*80)
                        else:
                            if file_ids:
                                LOGGER.warning("\n" + "="*80)
                                LOGGER.warning("‚ö†Ô∏è CRITICAL: file_ids provided but doc_context is None!")
                                LOGGER.warning("="*80)
                                LOGGER.warning(f"üìé File IDs: {file_ids}")
                                LOGGER.warning(f"üîç This means files were not found in database or had no content")
                                LOGGER.warning(f"üí° Check:")
                                LOGGER.warning(f"   1. Are files stored in uploaded_documents table?")
                                LOGGER.warning(f"   2. Do document_ids match?")
                                LOGGER.warning(f"   3. Do files have content column populated?")
                                LOGGER.warning("="*80)

                emit(
                    "context_build_ready",
                    context_detail or ("Context compiled" if context else "Context not required"),
                )
                
                # NEW: Context-parse fallback ‚Äî enforce formatter whenever context contains ML FORECAST markers
                # even if the forecasting flag wasn't set earlier.
                if not reply and context and ("ML FORECAST" in context or "CRITICAL: THIS IS THE PRIMARY ANSWER" in context):
                    try:
                        from .context_builder import get_last_forecast_metadata
                        from .universal_ml_formatter import format_ml_forecast
                        meta = get_last_forecast_metadata()
                        fr = None
                        if meta and meta.get("forecast_result"):
                            fr = meta["forecast_result"]
                            sector = None
                            try:
                                sector = (self.ticker_sector_map.get(meta.get("ticker",""), {}) or {}).get("sector")
                            except Exception:
                                sector = None
                            reply = format_ml_forecast(
                                ticker=meta.get("ticker", ""),
                                metric=meta.get("metric", ""),
                                model_name=meta.get("method", ""),
                                sector=sector,
                                periods=getattr(fr, "periods", []) or [],
                                predicted=getattr(fr, "predicted_values", []) or [],
                                ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                confidence=getattr(fr, "confidence", None),
                                scenario_style=meta.get("parameters", {}).get("style"),
                                data_sources=meta.get("parameters", {}).get("sources") or [],
                            )
                            if reply:
                                emit("forecast_formatter", "Enforced by context marker (metadata)")
                        # Fallback to active_forecast if metadata missing
                        if not reply:
                            active = self.get_active_forecast()
                            if active and active.get("forecast_result"):
                                fr = active.get("forecast_result")
                                sector = None
                                try:
                                    sector = (self.ticker_sector_map.get(active.get("ticker",""), {}) or {}).get("sector")
                                except Exception:
                                    sector = None
                                reply = format_ml_forecast(
                                    ticker=active.get("ticker", ""),
                                    metric=active.get("metric", ""),
                                    model_name=active.get("method", ""),
                                    sector=sector,
                                    periods=getattr(fr, "periods", []) or [],
                                    predicted=getattr(fr, "predicted_values", []) or [],
                                    ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                    ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                    confidence=getattr(fr, "confidence", None),
                                    scenario_style=(active.get("parameters") or {}).get("style"),
                                    data_sources=(active.get("parameters") or {}).get("sources") or [],
                                )
                                if reply:
                                    emit("forecast_formatter", "Enforced by context marker (active_forecast)")
                    except Exception:
                        # If anything goes wrong, continue with normal flow
                        pass
                
                # Attempt universal ML formatting for forecasting queries before invoking LLM
                reply = None
                if is_forecasting:
                    try:
                        from .context_builder import get_last_forecast_metadata
                        from .universal_ml_formatter import format_ml_forecast
                        meta = get_last_forecast_metadata()
                        if meta and meta.get("forecast_result"):
                            fr = meta["forecast_result"]
                            # Persist for API consumers
                            try:
                                self.set_active_forecast(
                                    ticker=meta.get("ticker", ""),
                                    metric=meta.get("metric", ""),
                                    method=meta.get("method", ""),
                                    periods=meta.get("parameters", {}).get("periods", 0),
                                    forecast_result=fr,
                                    explainability=meta.get("explainability") or {},
                                    parameters=meta.get("parameters") or {},
                                )
                            except Exception:
                                pass
                            # Determine sector for company-specific drivers
                            sector = None
                            try:
                                sector = (self.ticker_sector_map.get(meta.get("ticker",""), {}) or {}).get("sector")
                            except Exception:
                                sector = None
                            reply = format_ml_forecast(
                                ticker=meta.get("ticker", ""),
                                metric=meta.get("metric", ""),
                                model_name=meta.get("method", ""),
                                sector=sector,
                                periods=getattr(fr, "periods", []) or [],
                                predicted=getattr(fr, "predicted_values", []) or [],
                                ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                confidence=getattr(fr, "confidence", None),
                                scenario_style=meta.get("parameters", {}).get("style"),
                                data_sources=meta.get("parameters", {}).get("sources") or [],
                            )
                            if reply:
                                emit("forecast_formatter", "Universal ML formatter applied")
                    except Exception as e:
                        LOGGER.debug(f"Universal ML formatter unavailable or failed: {e}", exc_info=True)

                    # Strict fallback: if no metadata but an active forecast exists, format from it
                    if not reply:
                        try:
                            active = self.get_active_forecast()
                            if active and active.get("forecast_result"):
                                fr = active.get("forecast_result")
                                sector = None
                                try:
                                    sector = (self.ticker_sector_map.get(active.get("ticker",""), {}) or {}).get("sector")
                                except Exception:
                                    sector = None
                                reply = format_ml_forecast(  # type: ignore[name-defined]
                                    ticker=active.get("ticker", ""),
                                    metric=active.get("metric", ""),
                                    model_name=active.get("method", ""),
                                    sector=sector,
                                    periods=getattr(fr, "periods", []) or [],
                                    predicted=getattr(fr, "predicted_values", []) or [],
                                    ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                    ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                    confidence=getattr(fr, "confidence", None),
                                    scenario_style=(active.get("parameters") or {}).get("style"),
                                    data_sources=(active.get("parameters") or {}).get("sources") or [],
                                )
                                if reply:
                                    emit("forecast_formatter", "Universal ML formatter applied via active_forecast fallback")
                        except Exception:
                            pass
                
                # If no preformatted reply, prepare LLM messages
                # Note: reply might be set by grounded decision layer, in which case we skip LLM
                if not reply:
                    # CRITICAL: Ensure context is never None - LLM needs something to work with
                    if not context:
                        context = f"\n{'='*80}\n"
                        context += f"üìä USER QUERY\n"
                        context += f"{'='*80}\n\n"
                        context += f"**Query:** {user_input}\n\n"
                        context += f"**Note:** Please provide a helpful response to this query. "
                        context += f"If it's unclear, ask for clarification. Never say 'I don't understand'.\n"
                        context += f"{'='*80}\n"
                        LOGGER.info("Created fallback context for LLM")
                    
                    # Pass is_forecasting flag and user_query to message preparation
                    LOGGER.info("\n" + "="*80)
                    LOGGER.info("PREPARING LLM MESSAGES")
                    LOGGER.info("="*80)
                    LOGGER.info(f"üìä Context length: {len(context) if context else 0} characters")
                    if context:
                        # Check if document context is in the context
                        if "UPLOADED FILES" in context or "UPLOADED FINANCIAL DOCUMENTS" in context:
                            LOGGER.info("‚úÖ Context contains document/file markers")
                            # Find where document context starts
                            doc_start = context.find("UPLOADED FILES") or context.find("UPLOADED FINANCIAL DOCUMENTS")
                            if doc_start >= 0:
                                doc_preview = context[doc_start:doc_start+500]
                                LOGGER.info(f"üìù Document context preview:\n{doc_preview}")
                        else:
                            LOGGER.warning("‚ö†Ô∏è Context does NOT contain document/file markers")
                            LOGGER.warning("   This means document context was not added to the context variable")
                    LOGGER.info("="*80)
                    
                    messages = self._prepare_llm_messages(context, is_forecasting=is_forecasting, user_query=user_input)
                    LOGGER.info(f"üì® Prepared {len(messages)} messages for LLM")
                    
                    # CRITICAL: Verify document context is in messages
                    # Check if doc_context was built (either from file_ids or auto-fetched from conversation)
                    if doc_context:
                        system_messages = [msg for msg in messages if msg.get("role") == "system"]
                        has_doc_context = False
                        for sys_msg in system_messages:
                            content = sys_msg.get("content", "")
                            if "UPLOADED FILES" in content or "UPLOADED FINANCIAL DOCUMENTS" in content:
                                has_doc_context = True
                                LOGGER.info(f"‚úÖ Document context found in system message ({len(content)} chars)")
                                LOGGER.info(f"   Context preview (first 500 chars): {content[:500]}")
                                break
                        
                        # EMERGENCY FIX: If doc_context was built but not in messages, force it in
                        if not has_doc_context:
                            LOGGER.error(f"‚ùå CRITICAL: doc_context was built ({len(doc_context)} chars) but NOT in LLM messages!")
                            LOGGER.error(f"   Context variable length: {len(context) if context else 0}")
                            LOGGER.error(f"   System messages count: {len(system_messages)}")
                            for i, sys_msg in enumerate(system_messages):
                                content_preview = sys_msg.get('content', '')[:200]
                                LOGGER.error(f"   System message {i} preview: {content_preview}")
                            
                            # EMERGENCY FIX: Force document context into the last system message
                            LOGGER.warning("üîß EMERGENCY FIX: Forcing document context into system message")
                            if system_messages:
                                # Append to last system message
                                last_sys_msg = system_messages[-1]
                                last_sys_msg["content"] = f"{last_sys_msg.get('content', '')}\n\n{doc_context}"
                                LOGGER.info(f"‚úÖ Document context forced into system message (new length: {len(last_sys_msg['content'])} chars)")
                            else:
                                # Add as new system message
                                messages.insert(0, {"role": "system", "content": doc_context})
                                LOGGER.info(f"‚úÖ Document context added as new system message")
                            
                            # Re-verify
                            system_messages = [msg for msg in messages if msg.get("role") == "system"]
                            for sys_msg in system_messages:
                                content = sys_msg.get("content", "")
                                if "UPLOADED FILES" in content or "UPLOADED FINANCIAL DOCUMENTS" in content:
                                    LOGGER.info(f"‚úÖ VERIFIED: Document context now in system message")
                                    break
                    elif file_ids:
                        # file_ids provided but doc_context is None - this is a problem
                        LOGGER.warning(f"‚ö†Ô∏è file_ids provided ({file_ids}) but doc_context is None")
                        LOGGER.warning(f"   Files may not exist in database or have no content")
                    else:
                        LOGGER.info("‚ÑπÔ∏è No document context (no file_ids and no files in conversation)")
                    # Log message contents
                    for i, msg in enumerate(messages):
                        role = msg.get("role", "unknown")
                        content_preview = msg.get("content", "")[:200] if msg.get("content") else ""
                        LOGGER.info(f"   Message {i+1}: role={role}, content_length={len(msg.get('content', ''))}, preview={content_preview}")
                
                # Log context details for debugging
                if context:
                    context_length = len(context)
                    LOGGER.info(f"Context length: {context_length} characters")
                    if is_forecasting:
                        # Check if ML forecast context is present
                        has_ml_forecast = "ML FORECAST" in context or "CRITICAL: THIS IS THE PRIMARY ANSWER" in context
                        LOGGER.info(f"Forecasting query - ML forecast context present: {has_ml_forecast}")
                        if has_ml_forecast:
                            # Log key sections
                            has_model_details = "MODEL TECHNICAL DETAILS" in context or "FINAL CHECKLIST" in context
                            has_explicit_dump = "EXPLICIT DATA DUMP" in context
                            LOGGER.info(f"  - Model details section: {has_model_details}")
                            LOGGER.info(f"  - Explicit data dump: {has_explicit_dump}")
                            # Log a sample of the context to verify it's correct
                            sample_start = context[:500] if len(context) > 500 else context
                            LOGGER.debug(f"Context sample (first 500 chars): {sample_start}")
                
                # If reply already composed by universal formatter, skip LLM
                if reply:
                    emit("llm_query_start", "Universal formatter composed response")
                    emit("llm_query_complete", "Explanation drafted")
                    LOGGER.info(f"Generated reply length: {len(reply)} characters (universal formatter)")
                else:
                    emit("llm_query_start", "Composing explanation")
                
                # is_forecasting is already set above, reuse it
                
                # Use lower temperature and higher max_tokens for forecasting queries
                # Lower temperature = more deterministic, follows instructions better
                # Higher max_tokens = allows for detailed responses
                if not reply:
                    if is_forecasting:
                        reply = self.llm_client.generate_reply(
                            messages,
                            temperature=0.3,  # Lower temperature for more deterministic, instruction-following behavior
                            max_tokens=4000,  # Higher max_tokens to allow detailed responses
                        )
                    else:
                        reply = self.llm_client.generate_reply(messages)
                
                # FINAL SAFETY CHECK: Verify document context was sent to LLM
                if doc_context:
                    # Check all messages one more time before sending
                    all_content = " ".join([msg.get("content", "") for msg in messages if msg.get("content")])
                    has_file_markers = "UPLOADED FILES" in all_content or "UPLOADED FINANCIAL DOCUMENTS" in all_content
                    
                    if not has_file_markers:
                        LOGGER.error("üö® EMERGENCY: Document context built but NOT in final messages sent to LLM!")
                        LOGGER.error(f"   doc_context length: {len(doc_context)} chars")
                        LOGGER.error(f"   Total messages content length: {len(all_content)} chars")
                        LOGGER.error("   This should never happen - the emergency fix should have caught this")
                        
                        # Last resort: Prepend to first user message
                        for msg in messages:
                            if msg.get("role") == "user":
                                original_content = msg.get("content", "")
                                msg["content"] = f"""[UPLOADED FILES CONTENT - USER HAS UPLOADED THESE FILES FOR ANALYSIS]

{doc_context}

[USER QUESTION]
{original_content}"""
                                LOGGER.warning("üîß LAST RESORT: Prepend document context to user message")
                                LOGGER.warning(f"   New user message length: {len(msg['content'])} chars")
                                break
                    else:
                        LOGGER.info("‚úÖ FINAL CHECK PASSED: Document context confirmed in messages sent to LLM")
                        # Log a sample to verify
                        for msg in messages:
                            content = msg.get("content", "")
                            if "UPLOADED FILES" in content or "UPLOADED FINANCIAL DOCUMENTS" in content:
                                LOGGER.info(f"   Found in {msg.get('role')} message: {len(content)} chars")
                                LOGGER.info(f"   Preview: {content[:300]}...")
                                break
                
                emit("llm_query_complete", "Explanation drafted")
                LOGGER.info(f"Generated reply length: {len(reply) if reply else 0} characters")
                
                # Post-generation: Claim-level verification (if enabled)
                if reply and use_rag_orchestrator and rag_orchestrator:
                    try:
                        verification = rag_orchestrator.verify_answer_claims(reply, rag_result)
                        if verification and verification.should_regenerate:
                            LOGGER.warning(
                                f"Claim verification: {verification.num_contradicted} contradicted, "
                                f"{verification.num_not_found} not found. Consider regenerating."
                            )
                            # Note: Removed automatic disclaimer appending
                            # Low confidence is logged but no disclaimer is added
                    except Exception as e:
                        LOGGER.debug(f"Claim verification failed: {e}")
                
                # CRITICAL: Fix astronomical percentages BEFORE verification
                if reply:
                    try:
                        from .response_corrector import fix_astronomical_percentages
                        LOGGER.debug(f"Calling fix_astronomical_percentages on reply (length: {len(reply)})")
                        reply, fixes_applied = fix_astronomical_percentages(reply)
                        if fixes_applied > 0:
                            emit("percentage_fix", f"Fixed {fixes_applied} astronomical percentage(s)")
                            LOGGER.warning(f"üîß Fixed {fixes_applied} astronomical percentage(s) in response")
                        else:
                            LOGGER.debug("fix_astronomical_percentages found no fixes to apply")
                    except ImportError as e:
                        LOGGER.error(f"Response corrector not available: {e}", exc_info=True)
                    except Exception as e:
                        LOGGER.error(f"Percentage fix failed: {e}", exc_info=True)
                        # Don't fail the request, just log the error
                
                # Post-process verification for ML forecast responses
                if is_forecasting and reply:
                    try:
                        from .ml_response_verifier import verify_ml_forecast_response
                        is_complete, missing_details, enhanced_reply = verify_ml_forecast_response(
                            reply, context or "", user_input
                        )
                        if not is_complete:
                            LOGGER.warning(f"ML forecast response missing {len(missing_details)} details, enhancing response")
                            reply = enhanced_reply
                            emit("ml_verification", f"Enhanced response with {len(missing_details)} missing details")
                    except ImportError:
                        LOGGER.debug("ML response verifier not available, skipping verification")
                    except Exception as e:
                        LOGGER.debug(f"ML response verification failed: {e}")
                
                # FINAL ENFORCEMENT: Overwrite with universal formatter if a forecast was generated
                if is_forecasting:
                    try:
                        from .context_builder import get_last_forecast_metadata
                        from .universal_ml_formatter import format_ml_forecast
                        meta = get_last_forecast_metadata()
                        if meta and meta.get("forecast_result"):
                            fr = meta["forecast_result"]
                            # Persist for API/UI consumers
                            try:
                                self.set_active_forecast(
                                    ticker=meta.get("ticker", ""),
                                    metric=meta.get("metric", ""),
                                    method=meta.get("method", ""),
                                    periods=meta.get("parameters", {}).get("periods", 0),
                                    forecast_result=fr,
                                    explainability=meta.get("explainability") or {},
                                    parameters=meta.get("parameters") or {},
                                )
                            except Exception:
                                pass
                            # Determine sector for company-specific drivers
                            sector = None
                            try:
                                sector = (self.ticker_sector_map.get(meta.get("ticker",""), {}) or {}).get("sector")
                            except Exception:
                                sector = None
                            formatted = format_ml_forecast(
                                ticker=meta.get("ticker", ""),
                                metric=meta.get("metric", ""),
                                model_name=meta.get("method", ""),
                                sector=sector,
                                periods=getattr(fr, "periods", []) or [],
                                predicted=getattr(fr, "predicted_values", []) or [],
                                ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                confidence=getattr(fr, "confidence", None),
                                scenario_style=meta.get("parameters", {}).get("style"),
                                data_sources=meta.get("parameters", {}).get("sources") or [],
                            )
                            if formatted:
                                reply = formatted
                                emit("forecast_formatter", "Universal ML formatter enforced after generation")
                        else:
                            # Strict fallback: enforce formatting from active forecast if metadata missing
                            try:
                                active = self.get_active_forecast()
                                if active and active.get("forecast_result"):
                                    fr = active.get("forecast_result")
                                    sector = None
                                    try:
                                        sector = (self.ticker_sector_map.get(active.get("ticker",""), {}) or {}).get("sector")
                                    except Exception:
                                        sector = None
                                    formatted = format_ml_forecast(  # type: ignore[name-defined]
                                        ticker=active.get("ticker", ""),
                                        metric=active.get("metric", ""),
                                        model_name=active.get("method", ""),
                                        sector=sector,
                                        periods=getattr(fr, "periods", []) or [],
                                        predicted=getattr(fr, "predicted_values", []) or [],
                                        ci_low=getattr(fr, "confidence_intervals_low", []) or [],
                                        ci_high=getattr(fr, "confidence_intervals_high", []) or [],
                                        confidence=getattr(fr, "confidence", None),
                                        scenario_style=(active.get("parameters") or {}).get("style"),
                                        data_sources=(active.get("parameters") or {}).get("sources") or [],
                                    )
                                    if formatted:
                                        reply = formatted
                                        emit("forecast_formatter", "Universal ML formatter enforced via active_forecast fallback")
                            except Exception:
                                pass
                    except Exception as e:
                        LOGGER.debug(f"Universal formatter enforcement failed: {e}", exc_info=True)
                
                # NEW: Verify response accuracy (for all responses, not just ML forecasts)
                if reply and self.settings.verification_enabled:
                    try:
                        from .response_verifier import verify_response
                        from .confidence_scorer import calculate_confidence, add_confidence_footer
                        from .source_verifier import verify_all_sources
                        from .response_corrector import correct_response, add_verification_footer
                        from .data_validator import validate_context_data
                        from .hallucination_detector import detect_hallucinations, add_hallucination_warning
                        
                        emit("verification_start", "Verifying response accuracy")
                        
                        # Verify response (pass ticker resolver for better company name resolution)
                        verification_result = verify_response(
                            reply,
                            context or "",
                            user_input,
                            self.analytics_engine,
                            str(self.settings.database_path),
                            ticker_resolver=self._name_to_ticker if hasattr(self, '_name_to_ticker') else None
                        )
                        
                        # NEW: Detect hallucinations
                        hallucination_report = detect_hallucinations(
                            reply,
                            context or "",
                            verification_result.results,
                            verification_result.facts
                        )
                        
                        # Log hallucination detection
                        if hallucination_report.has_hallucination:
                            LOGGER.warning(
                                f"Hallucination detected: {hallucination_report.critical_warnings} critical, "
                                f"{hallucination_report.total_warnings} total warnings, "
                                f"confidence: {hallucination_report.confidence_score*100:.1f}%"
                            )
                            emit("hallucination_detected", f"{hallucination_report.critical_warnings} critical warnings")
                        
                        # Cross-validate data
                        validation_issues = []
                        if self.settings.cross_validation_enabled:
                            try:
                                validation_issues = validate_context_data(
                                    context or "",
                                    self.analytics_engine,
                                    str(self.settings.database_path)
                                )
                            except Exception as e:
                                LOGGER.debug(f"Cross-validation failed: {e}")
                        
                        # Verify sources
                        source_issues = []
                        try:
                            source_issues = verify_all_sources(
                                reply,
                                verification_result.facts,
                                str(self.settings.database_path)
                            )
                        except Exception as e:
                            LOGGER.debug(f"Source verification failed: {e}")
                        
                        # Count sources in response
                        source_count = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', reply))
                        
                        # Calculate confidence
                        confidence = calculate_confidence(
                            reply,
                            verification_result.results,
                            source_count=source_count
                        )
                        
                        # Correct if needed
                        if verification_result.has_errors and self.settings.auto_correct_enabled:
                            reply = correct_response(reply, verification_result.results)
                            emit("verification_correct", f"Applied {len([r for r in verification_result.results if not r.is_correct])} corrections")
                        
                        # Log confidence internally without adding footers
                        if confidence.score < self.settings.min_confidence_threshold:
                            LOGGER.warning(f"Response confidence {confidence.score*100:.1f}% below threshold {self.settings.min_confidence_threshold*100:.1f}%")
                            if self.settings.verification_strict_mode:
                                # In strict mode, reject low-confidence responses
                                reply = "Unable to provide a reliable response based on available data. Please try asking about a different company or topic."
                                emit("verification_reject", "Response rejected due to low confidence")
                            else:
                                # Log warning but don't modify response
                                emit("verification_warning", f"Low confidence: {confidence.score*100:.1f}%")
                        else:
                            # Log completion without adding footer
                            emit("verification_complete", f"Verified: {verification_result.correct_facts}/{verification_result.total_facts} facts verified, {confidence.score*100:.1f}% confidence")
                        
                        # NEW: Add hallucination warning if detected
                        if hallucination_report.has_hallucination:
                            reply = add_hallucination_warning(reply, hallucination_report)
                            # In strict mode, reject responses with critical hallucinations
                            if self.settings.verification_strict_mode and hallucination_report.critical_warnings > 0:
                                reply = "I detected data verification issues with this response. Please verify the information against source documents or try rephrasing your query."
                                emit("hallucination_reject", "Response rejected due to critical hallucinations")
                        
                        # Log verification results
                        LOGGER.info(
                            f"Response verification: {confidence.score*100:.1f}% confidence, "
                            f"{verification_result.correct_facts}/{verification_result.total_facts} facts verified, "
                            f"{len(source_issues)} source issues, {len(validation_issues)} validation issues"
                        )
                    except ImportError as e:
                        LOGGER.debug(f"Response verification modules not available: {e}")
                    except Exception as e:
                        LOGGER.warning(f"Response verification failed: {e}", exc_info=True)

            if reply is None:
                # CRITICAL: Final check before fallback - ensure forecasting queries never get fallback snapshots
                try:
                    from .context_builder import _is_forecasting_query
                    if _is_forecasting_query(user_input):
                        # Forecasting query should have already been handled by LLM with forecast context
                        # If we're here, something went wrong - log it but don't generate a snapshot
                        LOGGER.warning(f"Forecasting query reached fallback handler - this should not happen: {user_input}")
                        reply = "I encountered an issue generating the forecast. Please try rephrasing your query or specifying the company name more clearly."
                        emit("forecasting_fallback", "Forecasting query reached fallback - using error message instead of snapshot")
                    else:
                        emit("fallback", "Using enhanced fallback reply")
                        reply = self._handle_enhanced_error(ErrorCategory.UNKNOWN_ERROR)
                except ImportError:
                    emit("fallback", "Using enhanced fallback reply")
                    reply = self._handle_enhanced_error(ErrorCategory.UNKNOWN_ERROR)

            emit("finalize", "Finalising response")
            database.log_message(
                self.settings.database_path,
                self.conversation.conversation_id,
                role="assistant",
                content=reply,
                created_at=datetime.utcnow(),
            )
            self.conversation.messages.append({"role": "assistant", "content": reply})

            # CRITICAL: Don't cache forecasting queries - they need fresh ML forecast context
            # Check if this was a forecasting query before caching
            is_forecasting_for_cache = False
            try:
                from .context_builder import _is_forecasting_query
                is_forecasting_for_cache = _is_forecasting_query(user_input)
            except ImportError:
                pass
            
            if cacheable and not cached_entry and reply and not is_forecasting_for_cache:
                emit("cache_store", "Caching response for future reuse")
                self._store_cached_reply(canonical_prompt, reply)
            elif is_forecasting_for_cache:
                emit("cache_skip", "Skipping cache for forecasting query - need fresh ML forecast context")

            # FINAL SAFEGUARD: Remove Growth Snapshot and Margin Snapshot sections from forecast responses
            # These sections should NEVER appear in forecast responses
            try:
                from .context_builder import _is_forecasting_query
                if _is_forecasting_query(user_input) and reply:
                    # re is already imported at module level
                    original_reply = reply
                    # Remove Growth Snapshot section (with emoji variations)
                    # Matches: "üìà Growth Snapshot (TICKER ‚Äì PERIOD)" and everything until next section or end
                    # Pattern: emoji + "Growth Snapshot" + optional text + newline + all content until next section
                    reply = re.sub(
                        r'(?i)(?:üìà\s*)?Growth\s+Snapshot[^\n]*(?:\n|$).*?(?=\n\nüìä\s*Margin|\nüìä\s*Margin|\n\n|$)',
                        '',
                        reply,
                        flags=re.DOTALL
                    )
                    # Remove Margin Snapshot section (with emoji variations)
                    # Matches: "üìä Margin Snapshot (TICKER ‚Äì PERIOD)" and everything until end
                    # Pattern: emoji + "Margin Snapshot" + optional text + newline + all content until end
                    reply = re.sub(
                        r'(?i)(?:üìä\s*)?Margin\s+Snapshot[^\n]*(?:\n|$).*?(?=\n\n|$)',
                        '',
                        reply,
                        flags=re.DOTALL
                    )
                    # Clean up any double newlines that might result
                    reply = re.sub(r'\n{3,}', '\n\n', reply)
                    # Log if we removed anything
                    if reply != original_reply:
                        LOGGER.warning(f"Removed Growth/Margin Snapshot sections from forecast response for query: {user_input}")
                        emit("forecasting_snapshot_removed", "Removed Growth/Margin Snapshot sections from forecast response")
            except (ImportError, Exception) as e:
                LOGGER.debug(f"Error in snapshot removal safeguard: {e}")
                pass

            # Universal rewrite step for CLI/non-API path (forecasting or formatted forecast replies)
            try:
                from .context_builder import _is_forecasting_query
                should_rewrite = False
                try:
                    should_rewrite = _is_forecasting_query(user_input)
                except Exception:
                    should_rewrite = False
                if not should_rewrite and reply:
                    # Heuristic: if already in 7-section format, we can still polish wording
                    if ("### 1. Executive Summary" in reply and "### 7. Audit Trail" in reply) or ("ML FORECAST" in (context or "")):
                        should_rewrite = True
                if reply and should_rewrite:
                    from .rewrite_formatter import rewrite_forecast_output
                    rewrite_prompt = rewrite_forecast_output(reply)
                    try:
                        reply = self.llm_client.generate_reply(
                            [{"role": "system", "content": "You are a finance assistant that strictly preserves numbers and follows formatting instructions."},
                             {"role": "user", "content": rewrite_prompt}],
                            temperature=0.2,
                            max_tokens=2000
                        )
                    except Exception:
                        pass  # keep original reply on rewrite failure
            except Exception:
                pass

            # CRITICAL: Fix malformed markdown tables before returning (run after rewrite)
            if reply:
                reply = self._fix_markdown_tables(reply)
            
            emit("complete", "Response ready")
            return reply
        finally:
            self._active_progress_callback = previous_callback


    def history(self) -> Iterable[database.Message]:
        """Return the stored conversation from the database."""
        return database.fetch_conversation(
            self.settings.database_path, self.conversation.conversation_id
        )

    def reset(self) -> None:
        """Start a fresh conversation while keeping the same configuration."""
        self.conversation = Conversation()
        # Optional: Enhanced routing
        enhanced_routing = None
        if self.settings.enable_enhanced_routing:
            try:
                enhanced_routing = enhance_structured_parse(text, structured)
            except Exception as e:
                LOGGER.debug(f"Enhanced routing failed: {e}")
                enhanced_routing = None
            
            if enhanced_routing:
                try:
                    self.last_structured_response["enhanced_routing"] = {
                        "intent": enhanced_routing.intent.value,
                        "confidence": enhanced_routing.confidence,
                        "force_dashboard": enhanced_routing.force_dashboard,
                        "force_text_only": enhanced_routing.force_text_only,
                    }
                except Exception as e:
                    LOGGER.debug(f"Failed to store enhanced routing: {e}")
                
                # Higher threshold now - prefer LLM unless very confident
                if enhanced_routing.confidence < 0.8:
                    return None
                
                # Check for portfolio intents first (before other checks)
                if enhanced_routing.intent.value.startswith("portfolio_"):
                    # Portfolio intents should be handled by LLM with portfolio context
                    # Don't try to process them as structured metrics
                    return None
                
                # Check for ML forecasting intents (must be handled by LLM with forecast context)
                if enhanced_routing.intent.value.startswith("ml_forecast_"):
                    # ML forecasting intents should be handled by LLM with ML forecast context
                    # Don't try to process them as structured metrics
                    return None
        
        # If structured parsing detected compare intent, handle it
        if structured:
            tickers_list = structured.get('tickers')
            if tickers_list is None:
                tickers_list = []
            if (isinstance(tickers_list, list) and
                len(tickers_list) >= 2 and
                structured.get('intent') == 'compare'):
                try:
                    structured_reply = self._handle_structured_metrics(structured)
                    if structured_reply:
                        return structured_reply
                except Exception as e:
                    LOGGER.debug(f"Failed to handle structured metrics: {e}")

        # Check for natural language fallback conditions
        try:
            if self._should_fallback_to_llm(structured):
                return None
        except Exception as e:
            LOGGER.debug(f"Failed to check fallback: {e}")
            # If fallback check fails, default to LLM
            return None
        
        # 8. Default: Let LLM handle with context
        return None

    def _is_legacy_compare_command(self, text: str) -> bool:
        """Determine if this is a legacy compare command vs natural language comparison."""
        # Legacy compare commands are typically: "compare AAPL MSFT" or "compare AAPL MSFT GOOGL"
        # Natural language comparisons are: "Compare Apple vs Microsoft revenue"
        
        tokens = text.split()
        if len(tokens) < 3:  # "compare AAPL" is too short
            return False
        
        # Check if there are natural language words like "vs", "versus", "revenue", etc., it's natural language
        natural_language_words = ["vs", "versus", "revenue", "earnings", "profit", "income", "sales", "growth", "performance", "results", "financial", "metrics", "data", "analysis"]
        has_natural_language = any(word.lower() in text.lower() for word in natural_language_words)
        
        if has_natural_language:
            return False  # It's natural language, not legacy command
        
        # Check if tokens after "compare" look like ticker symbols (uppercase, 1-5 chars)
        ticker_like_tokens = []
        for token in tokens[1:]:  # Skip "compare"
            if token.isupper() and len(token) <= 5 and token.isalpha():
                ticker_like_tokens.append(token)
        
        # If most tokens look like ticker symbols, it's likely a legacy command
        if len(ticker_like_tokens) >= 2:
            return True
        
        # Default to not legacy command if unclear
        return False

    def _is_complex_natural_language_query(self, text: str) -> bool:
        """Detect complex natural language queries that should go directly to LLM."""
        lowered = text.lower()
        
        # Complex query patterns that should bypass structured parsing
        complex_patterns = [
            "tell me about", "how is", "what are the key", "explain the risks",
            "sector analysis", "market outlook", "investment advice", "what's the",
            "how do you", "can you explain", "what does it mean", "how does",
            "what are the", "what is the", "how are the", "can you tell me"
        ]
        
        for pattern in complex_patterns:
            if pattern in lowered:
                return True
        
        # Queries with multiple question words
        question_words = ["what", "how", "why", "when", "where", "which"]
        question_count = sum(1 for word in question_words if word in lowered)
        if question_count >= 2:
            return True
        
        # Sector/industry related queries
        sector_keywords = ["sector", "industry", "market", "economy", "outlook", "trends"]
        for keyword in sector_keywords:
            if keyword in lowered:
                return True
        
        return False

    def _should_fallback_to_llm(self, structured: Optional[Dict[str, Any]]) -> bool:
        """Determine if the query should fall back to LLM instead of structured parsing."""
        if not structured:
            return True
        try:
            intent = structured.get("intent")
            tickers_raw = structured.get("tickers")
            metrics_raw = structured.get("vmetrics")
            norm_text = structured.get("norm_text", "") or ""
            
            # Safely extract tickers - handle None and non-list values
            tickers = []
            if tickers_raw is not None:
                if isinstance(tickers_raw, list):
                    # Filter out None values and extract ticker strings from dicts
                    for item in tickers_raw:
                        if item is None:
                            continue
                        if isinstance(item, dict):
                            ticker = item.get("ticker")
                            if ticker and isinstance(ticker, str):
                                tickers.append(ticker)
                        elif isinstance(item, str):
                            tickers.append(item)
            
            # Safely extract metrics
            metrics = []
            if metrics_raw is not None:
                if isinstance(metrics_raw, list):
                    metrics = [m for m in metrics_raw if m is not None]
            
            # Fall back to LLM if:
            # 1. Intent is unclear or parsing seems forced
            # BUT: Be more permissive - allow new financial intents to go to LLM
            new_financial_intents = [
                "sector_analysis", "market_analysis", "merger_acquisition", "ipo",
                "esg", "credit_analysis", "liquidity_analysis", "capital_structure",
                "economic_indicators", "regulatory", "recommendation", "risk_analysis"
            ]
            
            # If it's a new financial intent, always use LLM (they need comprehensive analysis)
            if intent in new_financial_intents:
                return True
            
            # For other intents, check if parsing seems forced
            if not intent or (intent == "lookup" and not tickers and not metrics):
                return True
            
            # 2. Too many ambiguous tickers parsed (likely over-parsing)
            # BUT: Be more lenient - allow up to 5 tickers for sector/market analysis
            if len(tickers) > 5:
                return True
                
            # 3. Complex natural language patterns that don't fit structured intents
            # Expanded list to include more financial query types
            complex_patterns = [
                "tell me about", "how is", "what are the key", "explain the risks",
                "sector analysis", "market outlook", "investment advice",
                "what's happening", "what's going on", "analyze", "evaluate",
                "assess", "review", "examine", "investigate", "study",
                "what can you tell me", "what do you know", "what information",
                "help me understand", "break down", "walk me through"
            ]
            
            for pattern in complex_patterns:
                if norm_text and pattern in norm_text.lower():
                    return True
                    
            # 4. Ambiguous ticker parsing for ranking/explain queries
            if intent in ["rank", "explain_metric"]:
                # If tickers were parsed but shouldn't have been
                if tickers and norm_text:
                    # Filter out None values from tickers list
                    valid_tickers = [t for t in tickers if t is not None and isinstance(t, str)]
                    if valid_tickers:
                        # Check if any ticker is in the norm_text
                        norm_upper = norm_text.upper()
                        if not any(ticker in norm_upper for ticker in ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA"]):
                            return True
            
            return False
        except Exception as e:
            LOGGER.debug(f"Error in _should_fallback_to_llm: {e}")
            # If any error occurs, default to LLM
            return True

    def _get_company_name(self, ticker: str) -> str:
        """Get company name for ticker symbol."""
        try:
            # Try to get company name from database
            from .web import _lookup_company_name
            company_name = _lookup_company_name(ticker)
            if company_name:
                return company_name
        except Exception:
            pass
        
        # Fallback to ticker symbol
        return ticker

    def _handle_metrics_comparison(self, tokens: Sequence[str]) -> str:
        """Render a comparison table for the resolved tickers/metrics."""
        cleaned_tokens: List[str] = []
        for token in tokens:
            stripped = token.strip()
            if not stripped:
                continue
            upper = stripped.upper().rstrip(',')
            if upper in {"VS", "AND", "FOR"}:
                continue
            cleaned_tokens.append(stripped)

        tickers, period_filters = self._split_tickers_and_periods(cleaned_tokens)
        if len(tickers) < 2:
            return "Usage: compare <TICKER_A> <TICKER_B> [MORE] [YEAR]"
        resolution = self._resolve_tickers(tickers)
        if resolution.missing:
            return self._format_missing_message(tickers, resolution.available or [])
        if not resolution.available:
            return self._format_missing_message(tickers, [])
        return self._format_metrics_table(
            resolution.available, period_filters=period_filters
        )

    def _dispatch_metrics_request(
        self,
        tickers: Sequence[str],
        period_filters: Optional[Sequence[tuple[int, int]]] = None,
    ) -> str:
        """Fetch metrics and build a response message for the chat."""
        if not tickers:
            return "Provide at least one ticker for metrics."
        
        # Always use text table format for chat responses
        # Dashboards are only built via explicit "dashboard" keyword
        return self._format_metrics_table(
            tickers,
            period_filters=period_filters,
        )

    def _handle_structured_metrics(self, structured: Dict[str, Any]) -> Optional[str]:
        """Handle parsed structured intents without relying on legacy commands."""
        if not structured:
            return None
        
        # Early check for ML forecasting intents - bypass structured metrics processing
        try:
            if hasattr(self, 'last_structured_response') and self.last_structured_response.get("enhanced_routing"):
                enhanced_routing = self.last_structured_response.get("enhanced_routing", {})
                intent = enhanced_routing.get("intent", "")
                if intent and intent.startswith("ml_forecast_"):
                    # ML forecasting intent - don't process as structured metrics, use LLM instead
                    return None
        except Exception as e:
            LOGGER.debug(f"Failed to check for ML forecasting intent: {e}")
        tickers = structured.get("tickers")
        if tickers is None:
            tickers = []
        if not isinstance(tickers, list):
            return None
        unique_tickers: List[str] = []
        for entry in tickers:
            if entry is None:
                continue
            if not isinstance(entry, dict):
                continue
            ticker = entry.get("ticker")
            if ticker and isinstance(ticker, str) and ticker not in unique_tickers:
                unique_tickers.append(ticker)

        if not unique_tickers:
            return None

        resolution = self._resolve_tickers(unique_tickers)
        if resolution.missing and (not resolution.available or len(resolution.available) == 0):
            return self._format_missing_message(unique_tickers, resolution.available or [])

        resolved_tickers = resolution.available if resolution.available else unique_tickers
        period_filters = self._periods_to_filters(structured.get("periods", {}))

        intent = structured.get("intent")
        if intent == "compare" and len(resolved_tickers) >= 2:
            # Check if specific metrics are requested
            requested_metrics = structured.get("vmetrics", []) or []
            if requested_metrics and isinstance(requested_metrics, list):
                # Filter to only requested metrics
                metric_keys = []
                for metric in requested_metrics:
                    if metric is None:
                        continue
                    if not isinstance(metric, dict):
                        continue
                    key = metric.get("key")
                    if key and isinstance(key, str):
                        metric_keys.append(key)
                if metric_keys:
                    return self._format_metrics_table(resolved_tickers, period_filters=period_filters, metric_filter=metric_keys)
            return self._format_metrics_table(resolved_tickers, period_filters=period_filters)

        if intent in {"lookup", "trend", "rank"}:
            return self._dispatch_metrics_request(resolved_tickers, period_filters)

        return None

    @staticmethod
    def _periods_to_filters(periods: Dict[str, Any]) -> Optional[List[tuple[int, int]]]:
        """Convert parsed period metadata into fiscal year filters."""
        if not periods:
            return None
        items = periods.get("items") or []
        if periods.get("type") == "range" and len(items) >= 2:
            start = items[0].get("fy")
            end = items[-1].get("fy")
            if start is not None and end is not None:
                if end < start:
                    start, end = end, start
                return [(start, end)]

        years: List[int] = []
        for item in items:
            year = item.get("fy")
            if year is None or year in years:
                continue
            years.append(year)

        if not years:
            return None

        years_sorted = sorted(years)
        if periods.get("type") == "multi":
            return [(year, year) for year in years_sorted]
        return [(years_sorted[0], years_sorted[-1])]

    def _handle_ingest_command(self, text: str) -> str:
        """Execute ingestion commands issued by the user."""
        parts = text.split()
        if len(parts) < 2:
            return "Usage: ingest <TICKER> [years]"
        if parts[1].lower() == "status":
            if len(parts) < 3:
                return "Usage: ingest status <TICKER>"
            ticker = parts[2].upper()
            manager = tasks.get_task_manager(self.settings)
            status = manager.get_status(ticker)
            if not status:
                return f"No ingestion task found for {ticker}."
            return (
                f"Ingestion status for {ticker}: {status.summary()} "
                f"(submitted {status.submitted_at:%Y-%m-%d %H:%M:%S} UTC)"
            )

        ticker = parts[1].upper()
        years = 5
        if len(parts) >= 3:
            try:
                years = int(parts[2])
            except ValueError:
                return "Years must be an integer (e.g. ingest TSLA 5)."
        manager = tasks.get_task_manager(self.settings)
        manager.submit_ingest(ticker, years=years)
        current_status = manager.get_status(ticker)
        summary = current_status.summary() if current_status else "queued"
        return (
            f"Ingestion for {ticker} queued (status: {summary}). "
            f"Use 'ingest status {ticker}' to check progress."
        )

    def _handle_scenario_command(self, text: str) -> str:
        """Run scenario modelling commands and persist the results."""
        parts = text.split()
        if len(parts) < 3:
            return "Usage: scenario <TICKER> <NAME> [rev=+5% margin=+1% mult=+0.5%]"

        ticker = parts[1].upper()
        name = parts[2]
        deltas = {
            "revenue_growth_delta": 0.0,
            "ebitda_margin_delta": 0.0,
            "multiple_delta": 0.0,
        }
        for token in parts[3:]:
            token_lower = token.lower()
            if token_lower.startswith("rev="):
                deltas["revenue_growth_delta"] = self._parse_percent(token_lower[4:])
            elif token_lower.startswith("margin="):
                deltas["ebitda_margin_delta"] = self._parse_percent(token_lower[7:])
            elif token_lower.startswith("mult="):
                deltas["multiple_delta"] = self._parse_percent(token_lower[5:])

        runner = getattr(self.analytics_engine, 'run_scenario', None)
        if not callable(runner):
            return 'Scenario analysis is not available in this configuration.'

        summary = runner(
            ticker,
            scenario_name=name,
            **deltas,
        )
        return getattr(summary, 'narrative', str(summary))
    def _resolve_tickers(self, subjects: Sequence[str]) -> "FinanlyzeOSChatbot._TickerResolution":
        """Resolve tickers against the dataset, recording missing entries."""
        subjects_list = list(subjects)
        if subjects_list:
            joined = ", ".join(subjects_list)
            self._progress("ticker_resolution_start", f"Resolving tickers: {joined}")

        available: List[str] = []
        missing: List[str] = []
        lookup = getattr(self.analytics_engine, "lookup_ticker", None)
        for subject in subjects_list:
            resolved: Optional[str] = None
            if callable(lookup):
                try:
                    resolved = lookup(subject, allow_partial=True)  # type: ignore[misc]
                except TypeError:
                    try:
                        resolved = lookup(subject)  # type: ignore[misc]
                    except Exception:
                        resolved = None
                except Exception:
                    resolved = None
            if not resolved:
                resolved = self._name_to_ticker(subject)
            if resolved:
                resolved = resolved.upper()
                if resolved not in available:
                    available.append(resolved)
                continue

            candidate = subject.upper()
            try:
                records = self._fetch_metrics_cached(candidate)
            except Exception:
                records = []
            if records:
                if candidate not in available:
                    available.append(candidate)
                continue
            suggestions = self._suggest_tickers([subject], limit=1)
            if suggestions:
                suggestion = suggestions[0].upper()
                if suggestion not in available:
                    available.append(suggestion)
                continue
            missing.append(subject)

        if subjects_list:
            if available:
                detail = f"Resolved: {', '.join(available)}"
                if missing:
                    detail += f" | Missing: {', '.join(missing)}"
            elif missing:
                detail = f"Unable to resolve: {', '.join(missing)}"
            else:
                detail = "No tickers provided"
            self._progress("ticker_resolution_complete", detail)

        return FinanlyzeOSChatbot._TickerResolution(available=available, missing=missing)

    def _format_missing_message(
        self, requested: Sequence[str], available: Optional[Sequence[str]]
    ) -> str:
        """Build a friendly message for unresolved tickers."""
        if available is None:
            available = []
        missing = [ticker for ticker in requested if ticker.upper() not in available]
        suggestions = self._suggest_tickers(missing)
        hint_parts: List[str] = []
        if available:
            hint_parts.extend(sorted({ticker.upper() for ticker in available}))
        if suggestions:
            hint_parts.extend(suggestions)
        if hint_parts:
            hint = ", ".join(dict.fromkeys(hint_parts))
            return (
                "Unable to resolve one or more tickers. Try specifying: "
                f"{hint}."
            )
        missing_list = ", ".join(missing)
        return (
            f"Unable to resolve: {missing_list}. Ingest the data first using "
            "'ingest <ticker>'."
        )

    def _suggest_tickers(
        self,
        subjects: Sequence[str],
        limit: int = 5,
    ) -> List[str]:
        """Offer fuzzy ticker suggestions for unresolved names."""
        suggestions: List[str] = []
        seen = set()
        for subject in subjects:
            for ticker, score in self.name_index.resolve_fuzzy(subject, n=limit, cutoff=0.6):
                ticker = ticker.upper()
                if ticker in seen:
                    continue
                suggestions.append(ticker)
                seen.add(ticker)
                if len(suggestions) >= limit:
                    return suggestions
        return suggestions

    def _record_span(self, record: database.MetricRecord) -> Optional[tuple[int, int]]:
        """Best-effort extraction of a (start_year, end_year) tuple for a metric snapshot."""
        if record.period:
            try:
                return self.analytics_engine._period_span(record.period)
            except Exception:
                pass
        start = record.start_year or record.end_year or 0
        end = record.end_year or record.start_year or 0
        if not start and not end:
            return None
        if start and not end:
            end = start
        if end and not start:
            start = end
        return (start, end)

    def _format_period_label(
        self,
        record: database.MetricRecord,
        span: Optional[tuple[int, int]] = None,
    ) -> Optional[str]:
        """Return a human readable label for a metric snapshot period."""
        if record.period:
            return record.period
        span = span or self._record_span(record)
        if not span:
            return None
        start, end = span
        if start and end and start != end:
            return f"FY{start}-FY{end}"
        year = end or start
        return f"FY{year}" if year else None

    def _build_trend_series(
        self,
        histories: Mapping[str, Sequence[database.MetricRecord]],
        tickers: Sequence[str],
    ) -> List[Dict[str, Any]]:
        """Compile time series data suitable for lightweight trend visualisations."""
        trend_series: List[Dict[str, Any]] = []
        allowed_metrics = {metric.lower() for metric in TREND_METRICS}
        for ticker in tickers:
            records = histories.get(ticker)
            if not records:
                continue
            metric_points: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
            for record in records:
                metric_name = record.metric.lower()
                if metric_name not in allowed_metrics:
                    continue
                if record.value is None:
                    continue
                span = self._record_span(record)
                sort_key = span or (
                    record.start_year or 0,
                    record.end_year or 0,
                )
                label = self._format_period_label(record, span)
                formatted = self._format_metric_value(
                    metric_name,
                    {metric_name: record},
                )
                metric_points[metric_name].append(
                    {
                        "sort": sort_key,
                        "period": label or record.period or "",
                        "value": record.value,
                        "formatted": formatted,
                    }
                )
            for metric_name, points in metric_points.items():
                if not points:
                    continue
                points.sort(key=lambda item: (item["sort"][0], item["sort"][1]))
                serialised_points = [
                    {
                        "period": point["period"],
                        "value": point["value"],
                        "formatted_value": point["formatted"],
                    }
                    for point in points
                    if point["period"]
                ]
                if len(serialised_points) < 2:
                    continue
                trend_series.append(
                    {
                        "ticker": ticker,
                        "metric": metric_name,
                        "label": _METRIC_LABEL_MAP.get(
                            metric_name, metric_name.replace("_", " ").title()
                        ),
                        "points": serialised_points,
                    }
                )
        return trend_series

    def _build_filing_urls(
        self, accession: Optional[str], cik: Optional[str]
    ) -> Dict[str, Optional[str]]:
        """Mirror the filing URL helpers used by the FastAPI layer for reuse in chat."""
        if not accession:
            return {"interactive": None, "detail": None, "search": None}
        if cik:
            clean_cik = cik.lstrip("0") or cik
            accession_no_dash = accession.replace("-", "")
            interactive = (
                "https://www.sec.gov/cgi-bin/viewer"
                f"?action=view&cik={clean_cik}&accession_number={accession}&xbrl_type=v"
            )
            detail = (
                f"https://www.sec.gov/Archives/edgar/data/{clean_cik}/"
                f"{accession_no_dash}/{accession}-index.html"
            )
            search = (
                f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={clean_cik}"
            )
            return {"interactive": interactive, "detail": detail, "search": search}
        return {"interactive": None, "detail": None, "search": None}

    def _build_metric_citations(
        self,
        metrics_per_ticker: Mapping[str, Dict[str, database.MetricRecord]],
        tickers: Sequence[str],
        limit: int = 24,
    ) -> List[Dict[str, Any]]:
        """Generate a concise list of fact sources underpinning the response."""
        citations: List[Dict[str, Any]] = []
        for ticker in tickers:
            metric_map = metrics_per_ticker.get(ticker)
            if not metric_map:
                continue
            for metric_name, record in metric_map.items():
                if len(citations) >= limit:
                    break
                label = _METRIC_LABEL_MAP.get(
                    metric_name, metric_name.replace("_", " ").title()
                )
                span = self._record_span(record)
                period_label = self._format_period_label(record, span)
                formatted_value = self._format_metric_value(
                    metric_name, {metric_name: record}
                )
                citation: Dict[str, Any] = {
                    "ticker": ticker,
                    "metric": metric_name,
                    "label": label,
                    "period": period_label,
                    "value": record.value,
                    "formatted_value": formatted_value,
                    "source": getattr(record, "source", None),
                }
                fiscal_year = None
                if span and span[1]:
                    fiscal_year = span[1]
                elif record.end_year:
                    fiscal_year = record.end_year
                fact_records: Sequence[database.FinancialFactRecord] = ()
                try:
                    fact_records = self.analytics_engine.financial_facts(
                        ticker=ticker,
                        fiscal_year=fiscal_year,
                        metric=metric_name,
                        limit=1,
                    )
                except Exception:  # pragma: no cover - safety guard
                    fact_records = ()
                if fact_records:
                    fact = fact_records[0]
                    citation["filing"] = fact.source_filing
                    citation["unit"] = fact.unit
                    raw_payload: Any = fact.raw or {}
                    if isinstance(raw_payload, str):
                        try:
                            raw_payload = json.loads(raw_payload)
                        except Exception:
                            raw_payload = {}
                    accession = None
                    if isinstance(raw_payload, dict):
                        accession = raw_payload.get("accn") or raw_payload.get("accession")
                        if not citation.get("filing") and raw_payload.get("filed"):
                            citation["filed_at"] = raw_payload.get("filed")
                        if raw_payload.get("form"):
                            citation["form"] = raw_payload.get("form")
                    accession = accession or fact.source_filing
                    urls = self._build_filing_urls(accession, getattr(fact, "cik", None))
                    citation["urls"] = urls
                citations.append(citation)
            if len(citations) >= limit:
                break
        return citations

    def _build_export_payloads(
        self,
        table_data: Optional[Dict[str, Any]],
        highlights: Sequence[str],
        citations: Optional[Sequence[Dict[str, Any]]] = None,
    ) -> List[Dict[str, Any]]:
        """Prepare lightweight export descriptors consumed by the web UI."""
        if not table_data:
            return []
        headers = table_data.get("headers") or []
        rows = table_data.get("rows") or []
        if not headers or not rows:
            return []
        descriptor = table_data.get("descriptor")
        tickers = table_data.get("tickers") or []
        slug = "-vs-".join(ticker.lower().replace(" ", "-") for ticker in tickers if ticker)
        if not slug:
            slug = "metrics"
        timestamp = datetime.utcnow().strftime("%Y%m%d")
        base_name = f"finanlyzeos-{slug}-{timestamp}"
        csv_payload = {
            "type": "csv",
            "label": "Download CSV",
            "filename": f"{base_name}.csv",
            "headers": headers,
            "rows": rows,
            "descriptor": descriptor,
        }
        pdf_payload = {
            "type": "pdf",
            "label": "Download PDF",
            "filename": f"{base_name}.pdf",
            "headers": headers,
            "rows": rows,
            "descriptor": descriptor,
            "highlights": list(highlights),
            "title": table_data.get("title"),
        }
        sources: List[Dict[str, str]] = []
        for citation in citations or []:
            if not isinstance(citation, dict):
                continue
            parts = [
                str(citation.get("ticker") or "").strip() or None,
                str(citation.get("label") or "").strip() or None,
                str(citation.get("period") or "").strip() or None,
                str(citation.get("formatted_value") or "").strip()
                or (
                    str(citation.get("value"))
                    if citation.get("value") is not None
                    else None
                ),
                str(citation.get("form") or "").strip() or None,
            ]
            descriptor = " ‚Ä¢ ".join(part for part in parts if part)
            urls = citation.get("urls") if isinstance(citation.get("urls"), dict) else {}
            url = urls.get("detail") or urls.get("interactive")
            if descriptor or url:
                entry: Dict[str, str] = {"text": descriptor}
                if url:
                    entry["url"] = url
                sources.append(entry)
        if sources:
            pdf_payload["sources"] = sources
        return [csv_payload, pdf_payload]

    def _format_metrics_table(
        self,
        tickers: Sequence[str],
        *,
        period_filters: Optional[Sequence[tuple[int, int]]] = None,
        metric_filter: Optional[List[str]] = None,
    ) -> str:
        """Render metrics output as a table suitable for chat."""
        metrics_per_ticker: Dict[str, Dict[str, database.MetricRecord]] = {}
        histories: Dict[str, List[database.MetricRecord]] = {}
        kpi_overrides: Dict[str, Dict[str, Tuple[database.KpiValueRecord, bool]]] = {}
        missing: List[str] = []
        latest_spans: Dict[str, tuple[int, int]] = {}
        for ticker in tickers:
            self._progress("metrics_fetch_start", f"Fetching metrics for {ticker}")
            records = self._fetch_metrics_cached(
                ticker,
                period_filters=period_filters,
            )
            if not records:
                missing.append(ticker)
                self._progress("metrics_fetch_missing", f"No metrics available for {ticker}")
                continue
            histories[ticker] = list(records)
            selected = self._select_latest_records(
                records, span_fn=self.analytics_engine._period_span
            )
            metrics_per_ticker[ticker] = selected
            if selected:
                span = max(
                    (
                        self.analytics_engine._period_span(record.period)
                        for record in selected.values()
                    ),
                    key=lambda value: value[1],
                )
                latest_spans[ticker] = span
                fiscal_end = span[1]
                if fiscal_end:
                    overrides: Dict[str, Tuple[database.KpiValueRecord, bool]] = {}
                    try:
                        kpi_rows = database.fetch_kpi_values(
                            self.settings.database_path,
                            ticker,
                            fiscal_year=fiscal_end,
                        )
                    except Exception:
                        kpi_rows = []
                    for kpi_record in kpi_rows:
                        used_override = False
                        existing = selected.get(kpi_record.metric_id)
                        if existing is None or existing.value is None:
                            period_label = (
                                f"FY{kpi_record.fiscal_year}"
                                if kpi_record.fiscal_year
                                else f"FY{fiscal_end}"
                            )
                            selected[kpi_record.metric_id] = database.MetricRecord(
                                ticker=ticker,
                                metric=kpi_record.metric_id,
                                period=period_label,
                                value=kpi_record.value,
                                source=f"kpi:{kpi_record.method}",
                                updated_at=kpi_record.updated_at,
                                start_year=kpi_record.fiscal_year,
                                end_year=kpi_record.fiscal_year,
                            )
                            used_override = True
                        overrides[kpi_record.metric_id] = (kpi_record, used_override)
                    if overrides:
                        kpi_overrides[ticker] = overrides

            self._progress("metrics_fetch_progress", f"Loaded {len(records)} rows for {ticker}")

        if not metrics_per_ticker:
            if missing:
                missing_detail = ", ".join(missing)
                self._progress("metrics_fetch_notice", f"Metrics unavailable for {missing_detail}")
            if period_filters:
                desc = self._describe_period_filters(period_filters)
                return f"No metrics available for {', '.join(tickers)} in {desc}."
            return "No metrics available for the requested tickers."

        if metrics_per_ticker:
            prepared = ", ".join(metrics_per_ticker.keys())
            self._progress("metrics_fetch_complete", f"Prepared metrics for {prepared}")
        if missing and metrics_per_ticker:
            missing_detail = ", ".join(missing)
            self._progress("metrics_fetch_notice", f"Metrics unavailable for {missing_detail}")

        benchmark_label: Optional[str] = None
        compute_benchmark = getattr(self.analytics_engine, "compute_benchmark_metrics", None)
        if callable(compute_benchmark):
            metrics_needed = {definition.name for definition in METRIC_DEFINITIONS}
            metrics_needed.update(BENCHMARK_KEY_METRICS.keys())
            try:
                benchmark_metrics = compute_benchmark(
                    sorted(metrics_needed),
                    period_filters=period_filters,
                )
            except Exception:
                benchmark_metrics = {}
            if benchmark_metrics:
                label_getter = getattr(self.analytics_engine, "benchmark_label", None)
                benchmark_label = (
                    label_getter()
                    if callable(label_getter)
                    else getattr(self.analytics_engine, "BENCHMARK_LABEL", "Benchmark")
                )
                metrics_per_ticker[benchmark_label] = benchmark_metrics

        ordered_tickers = [ticker for ticker in tickers if ticker in metrics_per_ticker]
        display_tickers = list(ordered_tickers)
        if benchmark_label:
            display_tickers.append(benchmark_label)
        headers = ["Metric"] + display_tickers
        rows: List[List[str]] = []
        cell_metadata: Dict[str, Dict[str, Dict[str, Any]]] = {}
        metric_keys: List[str] = []
        # Filter metrics if specific metrics are requested
        definitions_to_use = METRIC_DEFINITIONS
        if metric_filter:
            definitions_to_use = [defn for defn in METRIC_DEFINITIONS if defn.name in metric_filter]
        
        for definition in definitions_to_use:
            label = definition.description
            metric_keys.append(definition.name)
            row = [label]
            for ticker in display_tickers:
                formatted_value = self._format_metric_value(
                    definition.name, metrics_per_ticker[ticker]
                )
                row.append(formatted_value)
                override_entry = kpi_overrides.get(ticker, {}).get(definition.name)
                if override_entry:
                    record, used_override = override_entry
                    if record.value is not None and (used_override or record.method == "external"):
                        meta_bucket = cell_metadata.setdefault(definition.name, {})
                        meta_bucket[ticker] = {
                            "method": record.method,
                            "source": record.source,
                            "source_ref": record.source_ref,
                            "warning": record.warning,
                            "unit": record.unit,
                            "updated_at": record.updated_at.isoformat(),
                            "used_override": used_override,
                        }
            rows.append(row)

        descriptor_filters: List[tuple[int, int]]
        if period_filters:
            descriptor_filters = list(period_filters)
        else:
            descriptor_filters = list(latest_spans.values())
        descriptor = (
            self._describe_period_filters(descriptor_filters)
            if descriptor_filters
            else "latest available"
        )

        highlights = self._compose_benchmark_summary(
            metrics_per_ticker,
            benchmark_label=benchmark_label,
        )

        table_payload = {
            "headers": headers,
            "rows": rows,
            "descriptor": descriptor,
            "tickers": display_tickers,
            "title": f"Benchmark summary for {', '.join(ordered_tickers)}",
            "cell_metadata": cell_metadata,
            "metric_keys": metric_keys,
        }

        trends_payload = self._build_trend_series(histories, ordered_tickers)
        citations_payload = self._build_metric_citations(
            metrics_per_ticker, ordered_tickers
        )
        exports_payload = self._build_export_payloads(
            table_payload, highlights, citations_payload
        )
        conclusion = ""

        dashboard_descriptor = None
        # Build multiple single-company dashboards for multi-ticker requests
        if len(display_tickers) >= 2 and not metric_filter:
            dashboards_list = []
            for ticker in ordered_tickers[:3]:  # Limit to 3 tickers for performance
                try:
                    single_payload = build_cfi_dashboard_payload(
                        self.analytics_engine,
                        ticker,
                    )
                    if single_payload:
                        dashboards_list.append({
                            "ticker": _display_ticker_symbol(ticker),
                            "payload": single_payload,
                        })
                except Exception as e:
                    LOGGER.warning(f"Failed to build dashboard for {ticker}: {e}")
                    continue
            
            if dashboards_list:
                dashboard_descriptor = {
                    "kind": "multi-classic",
                    "dashboards": dashboards_list,
                }

        self.last_structured_response = {
            "highlights": highlights,
            "trends": trends_payload,
            "comparison_table": table_payload,
            "citations": citations_payload,
            "exports": exports_payload,
            "conclusion": conclusion,
            "dashboard": dashboard_descriptor,
        }

        output_lines: List[str] = []
        output_lines.append(self._render_table(headers, rows))
        if conclusion:
            output_lines.append("")
            output_lines.append(conclusion)
        if missing:
            context = (
                self._describe_period_filters(period_filters)
                if period_filters
                else "the requested periods"
            )
            output_lines.append(
                f"No metrics for {', '.join(missing)} in {context}."
            )
        return "\n".join(output_lines)

    def _describe_period_filters(
        self, period_filters: Sequence[tuple[int, int]]
    ) -> str:
        """Render a human-readable summary of applied period filters."""
        labels = []
        for start, end in period_filters:
            if start == 0 and end == 0:
                continue
            if start == end:
                labels.append(f"FY{start}")
            else:
                labels.append(f"FY{start}-FY{end}")
        return ", ".join(labels) if labels else "the requested periods"

    def _build_benchmark_conclusion(
        self,
        highlights: Sequence[str],
        tickers: Sequence[str],
        descriptor: str,
        benchmark_label: Optional[str],
    ) -> str:
        """Derive a short narrative summary and action prompt from highlight bullets."""
        if not highlights:
            return ""

        leadership_counter: Counter[str] = Counter()
        summary_fragments: List[str] = []
        sector_labels: Dict[str, Optional[str]] = {}

        def _format_ticker(ticker: str) -> str:
            label = sector_labels.get(ticker)
            if label is None:
                label = self._sector_label(ticker)
                sector_labels[ticker] = label
            return f"{ticker} ({label})" if label else ticker

        for entry in highlights:
            if ":" not in entry:
                continue
            metric_label, remainder = entry.split(":", 1)
            remainder = remainder.strip()
            best_segment = remainder.split(" vs ")[0]
            tokens = best_segment.split()
            if not tokens:
                continue
            best_ticker = tokens[0]
            leadership_counter[best_ticker] += 1
            value_tokens = " ".join(tokens[1:]).strip().rstrip(",")
            metric_phrase = metric_label.strip().lower()
            sentence = f"{_format_ticker(best_ticker)} leads {metric_phrase}"
            if value_tokens:
                sentence = f"{sentence} at {value_tokens}"
            comparatives = remainder.split(" vs ")[1:]
            if comparatives:
                formatted_comparatives = "; ".join(comparatives)
                sentence = f"{sentence}, ahead of {formatted_comparatives}"
            summary_fragments.append(sentence + ".")

        if not summary_fragments:
            return ""

        total = len(summary_fragments)
        summary_sentence = " ".join(summary_fragments)

        action_sentence = ""
        if leadership_counter:
            max_count = max(leadership_counter.values())
            if max_count > 0:
                leaders = [ticker for ticker, count in leadership_counter.items() if count == max_count]
                if len(leaders) == 1:
                    leader = leaders[0]
                    others = [ticker for ticker in tickers if ticker != leader]
                    action_sentence = (
                        f"{_format_ticker(leader)} leads {max_count} of {total} highlighted KPIs"
                    )
                    if benchmark_label:
                        action_sentence += f" relative to {benchmark_label}"
                    if others:
                        action_sentence += (
                            ". Consider tilting exposure toward the leader while monitoring "
                            + ", ".join(_format_ticker(t) for t in others)
                            + " for catalysts."
                        )
                    else:
                        action_sentence += (
                            ". Consider tilting exposure toward the leader while monitoring macro risks."
                        )
                else:
                    leader_text = ", ".join(_format_ticker(t) for t in leaders)
                    sectors = {sector_labels.get(t) or self._sector_label(t) for t in leaders}
                    sectors.discard(None)
                    sector_clause = ""
                    if len(sectors) == 1:
                        only_sector = sectors.pop()
                        sector_clause = f" (shared exposure to {only_sector})"
                    action_sentence = (
                        f"Leadership is split across {leader_text}{sector_clause}; keep positions balanced and watch the next catalysts closely."
                    )

        return " ".join(filter(None, [summary_sentence, action_sentence]))

    def _select_latest_records(
        self,
        records: Iterable[database.MetricRecord],
        *,
        span_fn,
    ) -> Dict[str, database.MetricRecord]:
        """Choose latest metric snapshots for each metric name."""
        selected: Dict[str, database.MetricRecord] = {}
        for record in records:
            existing = selected.get(record.metric)
            if existing is None:
                selected[record.metric] = record
                continue
            if record.value is not None and existing.value is None:
                selected[record.metric] = record
                continue
            if record.value is None and existing.value is not None:
                continue
            new_span = span_fn(record.period)
            old_span = span_fn(existing.period)
            if new_span[1] > old_span[1] or (
                new_span[1] == old_span[1] and new_span[0] > old_span[0]
            ):
                selected[record.metric] = record
        return selected
    
    def _handle_simple_factual_query(self, user_input: str, query_metadata: Dict[str, any]) -> Optional[str]:
        """Handle simple factual queries with direct database lookup (fast path)."""
        try:
            tickers = query_metadata.get("tickers", [])
            if not tickers:
                return None
            
            # Extract the main ticker and metric from the query
            ticker = tickers[0]  # Use first ticker found
            query_lower = user_input.lower()
            
            # Simple metric extraction
            if any(word in query_lower for word in ['revenue', 'sales']):
                metric = 'revenue'
            elif any(word in query_lower for word in ['price', 'stock price']):
                metric = 'current_price'
            elif any(word in query_lower for word in ['market cap', 'market capitalization']):
                metric = 'market_cap'
            elif any(word in query_lower for word in ['pe ratio', 'p/e']):
                metric = 'pe_ratio'
            elif any(word in query_lower for word in ['earnings', 'net income']):
                metric = 'net_income'
            else:
                return None  # Can't determine metric, fall back to full pipeline
            
            # Get the metric directly from analytics engine
            try:
                records = self.analytics_engine.get_metrics(ticker, [metric])
                if records and records[0].value is not None:
                    value = records[0].value
                    period = records[0].period or "latest"
                    
                    # Format the response
                    if metric == 'revenue':
                        formatted_value = f"${value/1e9:.1f}B" if value >= 1e9 else f"${value/1e6:.1f}M"
                        return f"{ticker}'s revenue for {period} is {formatted_value}."
                    elif metric == 'current_price':
                        return f"{ticker} is currently trading at ${value:.2f}."
                    elif metric == 'market_cap':
                        formatted_value = f"${value/1e9:.1f}B" if value >= 1e9 else f"${value/1e6:.1f}M"
                        return f"{ticker}'s market cap is {formatted_value}."
                    elif metric == 'pe_ratio':
                        return f"{ticker}'s P/E ratio is {value:.1f}."
                    elif metric == 'net_income':
                        formatted_value = f"${value/1e9:.1f}B" if value >= 1e9 else f"${value/1e6:.1f}M"
                        return f"{ticker}'s net income for {period} is {formatted_value}."
            except Exception as e:
                LOGGER.debug(f"Fast path query failed for {ticker} {metric}: {e}")
                return None
                
        except Exception as e:
            LOGGER.debug(f"Simple factual query handler failed: {e}")
            return None
        
        return None  # Fall back to full pipeline

    def _build_rag_context(self, user_input: str) -> Optional[str]:
        """Assemble finance facts to ground the LLM response."""
        tickers = self._detect_tickers(user_input)
        if not tickers:
            return None
        normalized_tickers: List[str] = []
        seen_tickers: set[str] = set()
        for ticker in tickers:
            if not ticker:
                continue
            upper = ticker.upper()
            if upper in seen_tickers:
                continue
            seen_tickers.add(upper)
            normalized_tickers.append(upper)
        if normalized_tickers:
            self._progress("context_sources_scan", f"Scanning context for {', '.join(normalized_tickers)}")
        tickers = normalized_tickers or tickers
        cache_key = "|".join(sorted(normalized_tickers)) if normalized_tickers else None
        if cache_key:
            cached = self._get_cached_context(cache_key)
            if cached:
                self._progress("context_cache_hit", "Reusing cached context bundle")
                return cached

        context_sections: List[str] = []
        for ticker in tickers:
            try:
                records = self._fetch_metrics_cached(ticker)
            except Exception:  # pragma: no cover - database path
                continue
            if not records:
                continue

            latest = self._select_latest_records(
                records, span_fn=self.analytics_engine._period_span
            )
            if not latest:
                continue

            spans = [
                self.analytics_engine._period_span(record.period)
                for record in latest.values()
                if record.period
            ]
            descriptor = (
                self._describe_period_filters(spans) if spans else "latest available"
            )

            lines = [f"{ticker} ({descriptor})"]
            for metric_name in CONTEXT_SUMMARY_METRICS:
                formatted = self._format_metric_value(metric_name, latest)
                if formatted == "n/a":
                    continue
                label = _METRIC_LABEL_MAP.get(
                    metric_name, metric_name.replace("_", " ").title()
                )
                lines.append(f"- {label}: {formatted}")

            if len(lines) > 1:
                lines_text = "\n".join(lines)
                context_sections.append(lines_text)

        if not context_sections:
            self._progress("context_sources_empty", "No supplemental context located")
            return None
        combined = "\n".join(context_sections)
        final_context = "Financial context:\n" + combined
        self._progress("context_sources_ready", f"Added {len(context_sections)} context sections")
        if cache_key:
            self._store_cached_context(cache_key, final_context)
        return final_context
    def _build_portfolio_context(self, user_input: str) -> Optional[str]:
        """Build comprehensive portfolio context for portfolio-related queries."""
        import re
        lowered = user_input.lower()
        
        # Check if this is a portfolio query (even without explicit ID)
        portfolio_keywords = [
            "portfolio", "my portfolio", "the portfolio", "this portfolio",
            "portfolio exposure", "portfolio analysis", "portfolio optimize",
            "portfolio holdings", "portfolio risk", "portfolio allocation",
            "portfolio diversification", "portfolio performance"
        ]
        is_portfolio_query = any(kw in lowered for kw in portfolio_keywords)
        
        # Extract portfolio ID from query - try multiple patterns
        portfolio_id_patterns = [
            r"\bport_[\w]{4,12}\b",  # Standard format: port_xxxxx
            r"\bportfolio\s+([a-z0-9_]{4,12})\b",  # "portfolio xxxxx"
            r"\bportfolio\s+id[:\s]+([a-z0-9_]{4,12})\b",  # "portfolio id: xxxxx"
        ]
        
        portfolio_id = None
        for pattern in portfolio_id_patterns:
            match = re.search(pattern, user_input, re.IGNORECASE)
            if match:
                portfolio_id = match.group(0) if len(match.groups()) == 0 else match.group(1)
                LOGGER.info(f"Found explicit portfolio ID: {portfolio_id}")
                break
        
        if not portfolio_id and is_portfolio_query:
            # If it's a portfolio query but no explicit ID, try to get the most recent portfolio
            try:
                # Try to get list of portfolios from database
                import sqlite3
                with sqlite3.connect(self.settings.database_path) as conn:
                    cursor = conn.execute(
                        "SELECT portfolio_id FROM portfolio_metadata ORDER BY created_at DESC LIMIT 1"
                    )
                    row = cursor.fetchone()
                    if row:
                        portfolio_id = row[0]
                        LOGGER.info(f"Using most recent portfolio: {portfolio_id}")
            except Exception as e:
                LOGGER.warning(f"Could not fetch most recent portfolio: {e}")
        
        if not portfolio_id:
            if is_portfolio_query:
                LOGGER.warning(f"Portfolio query detected but no portfolio ID found: {user_input}")
                # Return helpful error message for user
                return (
                    "PORTFOLIO QUERY DETECTED BUT NO PORTFOLIO FOUND\n\n"
                    "I detected that you're asking about a portfolio, but I couldn't find a portfolio in the database.\n\n"
                    "To analyze your portfolio:\n"
                    "1. Go to the Portfolio Management section\n"
                    "2. Upload your portfolio (CSV, Excel, or JSON format)\n"
                    "3. Once uploaded, you can ask questions like:\n"
                    "   - 'What's my portfolio risk?'\n"
                    "   - 'Analyze my portfolio exposure'\n"
                    "   - 'What are my holdings?'\n\n"
                    "If you have a portfolio ID (e.g., port_xxxxx), include it in your question."
                )
            return None
        
        try:
            from .portfolio import (
                get_portfolio_holdings,
                enrich_holdings_with_fundamentals,
                calculate_portfolio_statistics,
                analyze_exposure,
                calculate_portfolio_volatility,
                calculate_portfolio_sharpe,
                calculate_portfolio_sortino,
                calculate_portfolio_alpha,
                calculate_tracking_error,
                calculate_portfolio_beta,
            )
            from .portfolio_risk_metrics import calculate_cvar
            
            # Fetch holdings
            holdings = get_portfolio_holdings(self.settings.database_path, portfolio_id)
            if not holdings:
                LOGGER.warning(f"No holdings found for portfolio {portfolio_id}")
                return None
            
            LOGGER.info(f"Fetched {len(holdings)} holdings for portfolio {portfolio_id}")
            
            # Enrich with fundamentals
            enriched_holdings = enrich_holdings_with_fundamentals(
                self.settings.database_path, holdings
            )
            
            if not enriched_holdings:
                return None
            
            # Calculate portfolio statistics
            stats = calculate_portfolio_statistics(enriched_holdings)
            
            # Analyze exposure
            exposure = analyze_exposure(enriched_holdings, self.settings.database_path)
            
            # Calculate risk metrics
            risk_metrics = {}
            try:
                cvar_result = calculate_cvar(self.settings.database_path, portfolio_id, confidence_level=0.95)
                if cvar_result:
                    risk_metrics['cvar'] = cvar_result.cvar
                    risk_metrics['var'] = cvar_result.var
                    risk_metrics['expected_loss'] = cvar_result.expected_loss
            except Exception as e:
                LOGGER.warning(f"Could not calculate CVaR: {e}")
            
            try:
                volatility = calculate_portfolio_volatility(self.settings.database_path, portfolio_id)
                if volatility is not None:
                    risk_metrics['volatility'] = volatility
            except Exception as e:
                LOGGER.warning(f"Could not calculate volatility: {e}")
            
            try:
                sharpe = calculate_portfolio_sharpe(self.settings.database_path, portfolio_id)
                if sharpe is not None:
                    risk_metrics['sharpe'] = sharpe
            except Exception as e:
                LOGGER.warning(f"Could not calculate Sharpe ratio: {e}")
            
            try:
                sortino = calculate_portfolio_sortino(self.settings.database_path, portfolio_id)
                if sortino is not None:
                    risk_metrics['sortino'] = sortino
            except Exception as e:
                LOGGER.warning(f"Could not calculate Sortino ratio: {e}")
            
            try:
                beta = calculate_portfolio_beta(self.settings.database_path, portfolio_id)
                if beta is not None:
                    risk_metrics['beta'] = beta
            except Exception as e:
                LOGGER.warning(f"Could not calculate beta: {e}")
            
            try:
                alpha = calculate_portfolio_alpha(self.settings.database_path, portfolio_id)
                if alpha is not None:
                    risk_metrics['alpha'] = alpha
            except Exception as e:
                LOGGER.warning(f"Could not calculate alpha: {e}")
            
            try:
                tracking_error = calculate_tracking_error(self.settings.database_path, portfolio_id)
                if tracking_error is not None:
                    risk_metrics['tracking_error'] = tracking_error
            except Exception as e:
                LOGGER.warning(f"Could not calculate tracking error: {e}")
            
            # Build comprehensive context
            context_parts = []
            context_parts.append("=" * 80)
            context_parts.append("PORTFOLIO ANALYSIS DATA - USE THIS DATA ONLY")
            context_parts.append(f"Portfolio ID: {portfolio_id}")
            context_parts.append("=" * 80)
            context_parts.append("")
            context_parts.append("CRITICAL INSTRUCTIONS:")
            context_parts.append("- You MUST use ONLY the data provided below")
            context_parts.append("- DO NOT make up or hallucinate portfolio data")
            context_parts.append("- USE CALCULATED RISK METRICS: When asked about CVaR, VaR, volatility, Sharpe ratio, Sortino ratio, alpha, beta, or tracking error:")
            context_parts.append("  * Use the ACTUAL calculated values shown in the 'Calculated Risk & Performance Metrics' section below")
            context_parts.append("  * These values have been calculated from historical portfolio returns and are ACCURATE")
            context_parts.append("  * DO NOT say you cannot calculate these metrics - they are already calculated and shown below")
            context_parts.append("  * Reference the specific calculated values (e.g., 'The portfolio CVaR is X%' using the value shown)")
            context_parts.append("  * If a metric is not shown (e.g., 'Risk metrics: Unable to calculate'), explain that insufficient historical data is available")
            context_parts.append("- Use the calculated values when available")
            context_parts.append("- Reference specific tickers, weights, and metrics from below")
            context_parts.append("- Quote the actual numbers from the data provided")
            context_parts.append("")
            context_parts.append("HOW TO INTERPRET THESE METRICS:")
            context_parts.append("- CVaR (Conditional Value at Risk): Expected loss in worst 5% of scenarios. Higher = more risk.")
            context_parts.append("- VaR (Value at Risk): Maximum expected loss at 95% confidence. Higher = more risk.")
            context_parts.append("- Volatility: Standard deviation of returns. Higher = more price fluctuation.")
            context_parts.append("- Sharpe Ratio: Risk-adjusted return (excess return / volatility). Higher is better (typically >1 is good).")
            context_parts.append("- Sortino Ratio: Downside risk-adjusted return. Higher is better (only penalizes negative volatility).")
            context_parts.append("- Beta: Sensitivity to market movements. 1.0 = market, >1.0 = more volatile, <1.0 = less volatile.")
            context_parts.append("- Alpha: Excess return vs benchmark. Positive = outperformance, negative = underperformance.")
            context_parts.append("- Tracking Error: Volatility of excess returns vs benchmark. Lower = more consistent relative performance.")
            context_parts.append("")
            
            # Portfolio Statistics
            context_parts.append("### Portfolio Statistics")
            if stats:
                if stats.total_market_value:
                    context_parts.append(f"Total Market Value: ${stats.total_market_value:,.2f}")
                if stats.num_holdings:
                    context_parts.append(f"Number of Positions: {stats.num_holdings}")
                if stats.weighted_avg_pe:
                    context_parts.append(f"Weighted Average P/E Ratio: {stats.weighted_avg_pe:.2f}")
                if stats.weighted_avg_dividend_yield:
                    context_parts.append(f"Weighted Average Dividend Yield: {stats.weighted_avg_dividend_yield:.2%}")
                if stats.top_10_weight:
                    context_parts.append(f"Top 10 Holdings Concentration: {stats.top_10_weight:.1%}")
                if stats.num_sectors:
                    context_parts.append(f"Number of Sectors: {stats.num_sectors}")
            context_parts.append("")
            
            # Sector Exposure
            context_parts.append("### Sector Exposure (by weight)")
            if exposure and exposure.sector_exposure:
                for sector, weight in sorted(exposure.sector_exposure.items(), key=lambda x: x[1], reverse=True)[:10]:
                    context_parts.append(f"{sector}: {weight:.1%}")
            elif stats and stats.sector_concentration:
                for sector, weight in sorted(stats.sector_concentration.items(), key=lambda x: x[1], reverse=True)[:10]:
                    context_parts.append(f"{sector}: {weight:.1%}")
            context_parts.append("")
            
            # Factor Exposure
            context_parts.append("### Factor Exposure")
            if exposure and exposure.factor_exposure:
                for factor, value in sorted(exposure.factor_exposure.items(), key=lambda x: abs(x[1]), reverse=True)[:10]:
                    context_parts.append(f"{factor}: {value:.3f}")
            context_parts.append("")
            
            # Top Holdings - DETAILED
            context_parts.append("### Top Holdings (by weight)")
            sorted_holdings = sorted(enriched_holdings, key=lambda h: h.weight or 0, reverse=True)[:20]
            for i, holding in enumerate(sorted_holdings, 1):
                parts = [f"{i}. {holding.ticker}"]
                if holding.weight:
                    parts.append(f"({holding.weight:.1%} weight)")
                if holding.shares:
                    parts.append(f"{holding.shares:.0f} shares")
                if holding.market_value:
                    parts.append(f"${holding.market_value:,.0f}")
                if holding.sector:
                    parts.append(f"[{holding.sector}]")
                if holding.pe_ratio:
                    parts.append(f"P/E: {holding.pe_ratio:.1f}")
                if holding.dividend_yield:
                    parts.append(f"Div: {holding.dividend_yield:.2%}")
                if holding.roe:
                    parts.append(f"ROE: {holding.roe:.1%}")
                if holding.roic:
                    parts.append(f"ROIC: {holding.roic:.1%}")
                context_parts.append(" ".join(parts))
            context_parts.append("")
            
            # Risk/Concentration Metrics
            context_parts.append("### Concentration & Risk Metrics")
            if exposure and exposure.concentration_metrics:
                hhi = exposure.concentration_metrics.get("hhi")
                if hhi:
                    context_parts.append(f"Herfindahl-Hirschman Index (HHI): {hhi:.0f}")
            if stats and stats.top_10_weight:
                context_parts.append(f"Top 10 Holdings Weight: {stats.top_10_weight:.1%}")
            context_parts.append("")
            
            # Calculated Risk & Performance Metrics
            context_parts.append("### Calculated Risk & Performance Metrics")
            context_parts.append("These metrics have been calculated from historical portfolio returns and are ready to use:")
            context_parts.append("")
            if risk_metrics:
                if 'cvar' in risk_metrics:
                    context_parts.append(f"‚Ä¢ CVaR (Conditional Value at Risk, 95%): {risk_metrics['cvar']:.4f} ({risk_metrics['cvar']*100:.2f}%)")
                    context_parts.append("  ‚Üí Expected loss in worst 5% of scenarios")
                if 'var' in risk_metrics:
                    context_parts.append(f"‚Ä¢ VaR (Value at Risk, 95%): {risk_metrics['var']:.4f} ({risk_metrics['var']*100:.2f}%)")
                    context_parts.append("  ‚Üí Maximum expected loss at 95% confidence level")
                if 'expected_loss' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Expected Loss (95% confidence): {risk_metrics['expected_loss']:.4f} ({risk_metrics['expected_loss']*100:.2f}%)")
                if 'volatility' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Annualized Volatility: {risk_metrics['volatility']:.4f} ({risk_metrics['volatility']*100:.2f}%)")
                    context_parts.append("  ‚Üí Standard deviation of returns (higher = more price fluctuation)")
                if 'sharpe' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Sharpe Ratio (annualized): {risk_metrics['sharpe']:.3f}")
                    context_parts.append("  ‚Üí Risk-adjusted return (higher is better, typically >1 is good)")
                if 'sortino' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Sortino Ratio (annualized): {risk_metrics['sortino']:.3f}")
                    context_parts.append("  ‚Üí Downside risk-adjusted return (higher is better)")
                if 'beta' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Beta (vs benchmark): {risk_metrics['beta']:.3f}")
                    context_parts.append("  ‚Üí Sensitivity to market (1.0 = market, >1.0 = more volatile, <1.0 = less volatile)")
                if 'alpha' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Alpha (vs benchmark, annualized): {risk_metrics['alpha']:.4f} ({risk_metrics['alpha']*100:.2f}%)")
                    context_parts.append("  ‚Üí Excess return vs benchmark (positive = outperformance)")
                if 'tracking_error' in risk_metrics:
                    context_parts.append(f"‚Ä¢ Tracking Error (vs benchmark, annualized): {risk_metrics['tracking_error']:.4f} ({risk_metrics['tracking_error']*100:.2f}%)")
                    context_parts.append("  ‚Üí Volatility of excess returns (lower = more consistent relative performance)")
            else:
                context_parts.append("Risk metrics: Unable to calculate (insufficient historical data)")
                context_parts.append("  ‚Üí This means there is not enough historical price data to calculate these metrics")
            context_parts.append("")
            
            context_parts.append("=" * 80)
            context_parts.append("PORTFOLIO DATA SOURCES - MANDATORY TO INCLUDE IN RESPONSE")
            context_parts.append("=" * 80)
            context_parts.append("")
            context_parts.append("üìä **CRITICAL: You MUST include a 'üìä Sources:' section at the end of your response with:**")
            context_parts.append("")
            
            # Portfolio metadata source
            context_parts.append("### Portfolio Data Sources")
            context_parts.append(f"- Portfolio ID: {portfolio_id}")
            context_parts.append(f"- Portfolio Holdings: {len(enriched_holdings)} positions")
            context_parts.append(f"- Total Market Value: ${stats.total_market_value:,.2f}" if stats and stats.total_market_value else "- Total Market Value: Calculated from holdings")
            context_parts.append("")
            
            # Top holdings SEC filing sources
            context_parts.append("### SEC Filing Sources for Top Holdings")
            top_holdings = sorted(enriched_holdings, key=lambda h: h.weight or 0, reverse=True)[:10]
            from .context_builder import _get_filing_sources
            
            for holding in top_holdings:
                if not holding.ticker:
                    continue
                filing_sources = _get_filing_sources(holding.ticker, self.settings.database_path)
                if filing_sources:
                    # Get most recent filing
                    recent_filing = filing_sources[0]
                    form_type = recent_filing.get("form_type", "SEC filing")
                    fiscal_year = recent_filing.get("fiscal_year")
                    sec_url = recent_filing.get("sec_url")
                    
                    if sec_url and fiscal_year:
                        context_parts.append(f"- **{holding.ticker}** ({holding.weight:.1%}): [{form_type} FY{fiscal_year}]({sec_url})")
                    elif sec_url:
                        context_parts.append(f"- **{holding.ticker}** ({holding.weight:.1%}): [{form_type}]({sec_url})")
                    else:
                        # Fallback to Yahoo Finance if no SEC URL available
                        context_parts.append(f"- **{holding.ticker}** ({holding.weight:.1%}): [Yahoo Finance - {holding.ticker}](https://finance.yahoo.com/quote/{holding.ticker})")
                else:
                    # Fallback to Yahoo Finance when no filing sources found
                    context_parts.append(f"- **{holding.ticker}** ({holding.weight:.1%}): [Yahoo Finance - {holding.ticker}](https://finance.yahoo.com/quote/{holding.ticker})")
            
            context_parts.append("")
            
            # Yahoo Finance sources for all holdings
            context_parts.append("### Yahoo Finance Sources for Holdings")
            unique_tickers = list(set([h.ticker for h in enriched_holdings if h.ticker]))[:15]
            for ticker in unique_tickers:
                context_parts.append(f"- [{ticker}](https://finance.yahoo.com/quote/{ticker})")
            context_parts.append("")
            
            # Benchmark data sources
            context_parts.append("### Benchmark Data Sources")
            context_parts.append("- **SPY (S&P 500 ETF)**: [Yahoo Finance - SPY](https://finance.yahoo.com/quote/SPY) | [SPDR S&P 500 ETF Trust](https://www.ssga.com/us/en/institutional/etfs/funds/spdr-sp-500-etf-trust-spy)")
            context_parts.append("- **QQQ (Nasdaq 100 ETF)**: [Yahoo Finance - QQQ](https://finance.yahoo.com/quote/QQQ) | [Invesco QQQ Trust](https://www.invesco.com/us/financial-products/etfs/product-detail?productId=QQQ)")
            context_parts.append("- **DIA (Dow Jones ETF)**: [Yahoo Finance - DIA](https://finance.yahoo.com/quote/DIA) | [SPDR Dow Jones Industrial Average ETF](https://www.ssga.com/us/en/institutional/etfs/funds/spdr-dow-jones-industrial-average-etf-trust-dia)")
            context_parts.append("- **IWM (Russell 2000 ETF)**: [Yahoo Finance - IWM](https://finance.yahoo.com/quote/IWM) | [iShares Russell 2000 ETF](https://www.ishares.com/us/products/239710/ishares-russell-2000-etf)")
            context_parts.append("")
            
            # Risk calculation methodology
            context_parts.append("### Risk & Performance Calculation Methodology")
            context_parts.append("- **CVaR & VaR**: Calculated using historical portfolio returns (252 trading days) with 95% confidence level")
            context_parts.append("- **Volatility**: Annualized standard deviation of portfolio returns (‚àö252 √ó daily std)")
            context_parts.append("- **Sharpe Ratio**: (Portfolio Return - Risk-Free Rate) / Portfolio Volatility (annualized)")
            context_parts.append("- **Sortino Ratio**: (Portfolio Return - Risk-Free Rate) / Downside Deviation (annualized)")
            context_parts.append("- **Beta**: Covariance(Portfolio, Benchmark) / Variance(Benchmark) - calculated vs SPY or alternative benchmark")
            context_parts.append("- **Alpha**: Portfolio Return - (Risk-Free Rate + Beta √ó (Benchmark Return - Risk-Free Rate)) (annualized)")
            context_parts.append("- **Tracking Error**: Annualized standard deviation of excess returns (Portfolio - Benchmark)")
            context_parts.append("- **Historical Price Data**: Sourced from database and Yahoo Finance API")
            context_parts.append("")
            
            # Portfolio statistics source
            context_parts.append("### Portfolio Statistics Sources")
            context_parts.append("- **Holdings Data**: Portfolio holdings database")
            context_parts.append("- **Fundamental Data**: SEC filings (10-K, 10-Q) and Yahoo Finance")
            context_parts.append("- **Sector Classifications**: GICS sector classifications from holdings data")
            context_parts.append("- **Market Values**: Calculated from current share prices and holdings quantities")
            context_parts.append("")
            
            context_parts.append("=" * 80)
            context_parts.append("END OF PORTFOLIO DATA")
            context_parts.append("=" * 80)
            context_parts.append("")
            context_parts.append("FINAL REMINDER - CRITICAL RULES:")
            context_parts.append("1. You MUST use ONLY the data shown above")
            context_parts.append("2. DO NOT invent or hallucinate any portfolio data")
            context_parts.append("3. Reference the ACTUAL tickers and weights listed above")
            context_parts.append("4. Use the ACTUAL sector exposure percentages provided")
            context_parts.append("5. Calculate recommendations based on THESE SPECIFIC holdings")
            context_parts.append("6. Quote the ACTUAL numbers (weights, values, metrics) from above")
            context_parts.append("7. If data is missing, explicitly state it - do not make up numbers")
            context_parts.append("8. DO NOT provide generic portfolio advice - analyze THIS SPECIFIC portfolio")
            context_parts.append("9. **MANDATORY: Include a 'üìä Sources:' section at the end with clickable links from the sources above**")
            context_parts.append("10. **Include at least 5-10 source links**: SEC filings for top holdings, Yahoo Finance links, benchmark sources")
            context_parts.append("")
            context_parts.append("USER QUESTION: " + user_input)
            context_parts.append("")
            context_parts.append("Now answer the user's question using ONLY the portfolio data shown above.")
            context_parts.append("")
            context_parts.append("**REMEMBER: End your response with a 'üìä Sources:' section listing all relevant sources from the sections above.**")
            
            context_str = "\n".join(context_parts)
            LOGGER.info(f"Built portfolio context for {portfolio_id}: {len(context_str)} characters, {len(enriched_holdings)} holdings")
            return context_str
        except Exception as e:
            LOGGER.exception(f"Could not build portfolio context: {e}")
            return None
    def _parse_scenario_parameters(self, user_input: str) -> Dict[str, Any]:
        """
        Parse specific parameters from scenario questions.
        
        Detects patterns like:
        - "What if revenue grows 10%?"
        - "What if COGS rises 5%?"
        - "What if margins improve 2 percentage points?"
        - "What if volume increases 15%?"
        - "What if marketing spend increases 20%?"
        - "What if revenue grows 10% AND COGS rises 5%?" (multi-factor)
        - "What if volume doubles and prices fall 5%?" (complex)
        
        Returns:
            Dictionary with scenario parameters, validation warnings, and metadata
        """
        lowered = user_input.lower()
        scenario_params = {}
        warnings = []
        
        # Detect multi-factor scenarios (AND, plus, combined)
        is_multi_factor = bool(re.search(r"\b(?:and|plus|with|combined\s+with|along\s+with)\b", lowered))
        
        # Revenue/sales growth
        revenue_match = re.search(
            r"(?:revenue|sales?)\s+(?:grows?|increases?|rises?|goes?\s+up|falls?|decreases?|declines?|drops?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if revenue_match:
            value = float(revenue_match.group(1)) / 100
            # Check direction
            if re.search(r"falls?|decreases?|declines?|drops?", lowered):
                value = -value
            # Validate bounds (reasonable business assumptions)
            if abs(value) > 1.0:  # More than 100% change
                warnings.append(f"revenue_growth: {value:.1%} seems extreme (>100%)")
            scenario_params["revenue_growth"] = value
        
        # COGS/cost changes
        cogs_match = re.search(
            r"(?:cogs|cost\s+of\s+goods|costs?)\s+(?:rises?|increases?|grows?|goes?\s+up|falls?|decreases?|drops?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if cogs_match:
            change = float(cogs_match.group(1)) / 100
            # Check if it's a decrease
            if re.search(r"falls?|decreases?|drops?|declines?", lowered):
                change = -change
            # Validate bounds
            if abs(change) > 0.5:  # More than 50% COGS change is unusual
                warnings.append(f"cogs_change: {change:.1%} is very large (>50%)")
            scenario_params["cogs_change"] = change
        
        # Margin changes
        margin_match = re.search(
            r"(?:margin|gross\s+margin|profit\s+margin|operating\s+margin)\s+(?:improves?|increases?|expands?|rises?|deteriorates?|decreases?|shrinks?|falls?)\s+(?:by\s+)?(\d+(?:\.\d+)?)\s*(?:percentage\s+points?|pp|points?|%)?",
            lowered
        )
        if margin_match:
            change = float(margin_match.group(1)) / 100  # Convert to decimal
            if re.search(r"deteriorates?|decreases?|shrinks?|falls?", lowered):
                change = -change
            # Validate margin bounds (margins typically -20% to +20% range)
            if abs(change) > 0.2:  # More than 20pp change
                warnings.append(f"margin_change: {change:.1%} is very large (>20pp)")
            scenario_params["margin_change"] = change
        
        # Volume changes
        volume_match = re.search(
            r"(?:volume|sales\s+volume|units)\s+(?:increases?|grows?|rises?|doubles?|decreases?|falls?|drops?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if volume_match:
            change = float(volume_match.group(1)) / 100
            if re.search(r"decreases?|falls?|drops?|declines?", lowered):
                change = -change
            elif re.search(r"doubles?", lowered):
                change = 1.0  # 100% increase
            scenario_params["volume_change"] = change
        
        # Marketing/OpEx changes
        marketing_match = re.search(
            r"(?:marketing|opex|operating\s+expenses?)\s+(?:spend|spending|expenses?)\s+(?:increases?|rises?|grows?|decreases?|falls?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if marketing_match:
            change = float(marketing_match.group(1)) / 100
            if re.search(r"decreases?|falls?|drops?", lowered):
                change = -change
            scenario_params["marketing_change"] = change
        
        # GDP/economic changes
        gdp_match = re.search(
            r"gdp\s+(?:drops?|falls?|decreases?|rises?|grows?|increases?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if gdp_match:
            change = float(gdp_match.group(1)) / 100
            if re.search(r"drops?|falls?|decreases?", lowered):
                change = -change
            scenario_params["gdp_change"] = change
        
        # Price changes
        price_match = re.search(
            r"(?:price|prices?|pricing)\s+(?:increases?|rises?|decreases?|falls?)\s+(?:by\s+)?(\d+(?:\.\d+)?)%?",
            lowered
        )
        if price_match:
            change = float(price_match.group(1)) / 100
            if re.search(r"decreases?|falls?|drops?", lowered):
                change = -change
            # Validate price bounds
            if abs(change) > 0.5:  # More than 50% price change
                warnings.append(f"price_change: {change:.1%} is very large (>50%)")
            scenario_params["price_change"] = change
        
        # Interest rate changes
        interest_match = re.search(
            r"(?:interest\s+rate|rates?|fed\s+rate)\s+(?:increases?|rises?|goes?\s+up|decreases?|falls?|drops?)\s+(?:by\s+)?(\d+(?:\.\d+)?)\s*(?:percentage\s+points?|pp|points?|%)?",
            lowered
        )
        if interest_match:
            change = float(interest_match.group(1)) / 100
            if re.search(r"decreases?|falls?|drops?", lowered):
                change = -change
            # Interest rate changes typically in smaller increments
            if abs(change) > 0.05:  # More than 5pp
                warnings.append(f"interest_rate_change: {change:.1%} is large (>5pp)")
            scenario_params["interest_rate_change"] = change
        
        # Tax rate changes
        tax_match = re.search(
            r"(?:tax\s+rate|taxes?)\s+(?:increases?|rises?|decreases?|falls?)\s+(?:by\s+)?(\d+(?:\.\d+)?)\s*(?:percentage\s+points?|pp|points?|%)?",
            lowered
        )
        if tax_match:
            change = float(tax_match.group(1)) / 100
            if re.search(r"decreases?|falls?|drops?", lowered):
                change = -change
            if abs(change) > 0.15:  # More than 15pp tax change
                warnings.append(f"tax_rate_change: {change:.1%} is very large (>15pp)")
            scenario_params["tax_rate_change"] = change
        
        # Market share changes
        market_share_match = re.search(
            r"(?:market\s+share)\s+(?:increases?|grows?|gains?|decreases?|loses?|falls?)\s+(?:by\s+)?(\d+(?:\.\d+)?)\s*(?:percentage\s+points?|pp|points?|%)?",
            lowered
        )
        if market_share_match:
            change = float(market_share_match.group(1)) / 100
            if re.search(r"decreases?|loses?|falls?", lowered):
                change = -change
            if abs(change) > 0.1:  # More than 10pp market share change
                warnings.append(f"market_share_change: {change:.1%} is aggressive (>10pp)")
            scenario_params["market_share_change"] = change
        
        # Return with metadata
        result = {
            "parameters": scenario_params,
            "warnings": warnings,
            "is_multi_factor": is_multi_factor,
            "parameter_count": len(scenario_params)
        }
        
        # Log parsing results
        if scenario_params:
            LOGGER.info(f"Parsed scenario parameters: {scenario_params}")
            if warnings:
                LOGGER.warning(f"Scenario validation warnings: {warnings}")
        
        return result
