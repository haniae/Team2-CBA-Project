"""Utility functions for persisting chatbot conversations and market data in SQLite.

Key guarantees:
- ALL tickers normalized to uppercase on write.
- ALL timestamps written as UTC ISO-8601 strings (+00:00).
- No SQLite datetime(...) usage; string ordering/compare is used consistently.
- Safe datetime parsing on read (graceful with NULL/empty legacy values).
- UNIQUE/NULL behavior consistent (e.g., source_filing is NOT NULL DEFAULT '').
"""

from __future__ import annotations

import json
import sqlite3
# Database access layer: manages schema creation, typed records, and CRUD helpers for messages,
# metrics, financial facts, audit events, and scenario results.
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import date, datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Iterator, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union

if TYPE_CHECKING:  # pragma: no cover
    from .data_sources import AuditEvent, FilingRecord, FinancialFact, MarketQuote


# -----------------------------
# Dataclasses
# -----------------------------

@dataclass(frozen=True)
class Message:
    """Single conversational message stored for replaying chatbot history."""
    role: str
    content: str
    created_at: datetime


@dataclass(frozen=True)
class MetricRecord:
    """Snapshot of a computed metric value for a ticker and fiscal period."""
    ticker: str
    metric: str
    period: str
    value: Optional[float]
    source: str
    updated_at: datetime
    start_year: Optional[int]
    end_year: Optional[int]


@dataclass(frozen=True)
class KpiValueRecord:
    """Value persisted by the KPI backfill pipeline with provenance metadata."""

    ticker: str
    fiscal_year: Optional[int]
    fiscal_quarter: Optional[int]
    metric_id: str
    value: Optional[float]
    unit: Optional[str]
    method: str
    source: str
    source_ref: Optional[str]
    warning: Optional[str]
    updated_at: datetime


@dataclass(frozen=True)
class FinancialFactRecord:
    """Raw fact as ingested from SEC filings, optionally with adjustments."""
    ticker: str
    metric: str
    fiscal_year: Optional[int]
    fiscal_period: Optional[str]
    period: str
    value: Optional[float]
    unit: Optional[str]
    source: str
    source_filing: Optional[str]
    period_start: Optional[datetime]
    period_end: Optional[datetime]
    adjusted: bool
    adjustment_note: Optional[str]
    ingested_at: Optional[datetime]
    cik: Optional[str] = None
    raw: Optional[Dict[str, Any]] = None


@dataclass(frozen=True)
class ScenarioResultRecord:
    """Persisted output of a scenario run generated by the analytics engine."""
    ticker: str
    scenario_name: str
    metrics: Dict[str, Optional[float]]
    narrative: str
    created_at: datetime


@dataclass(frozen=True)
class AuditEventRecord:
    """Audit entry describing ingestion or scenario events for traceability."""
    ticker: str
    event_type: str
    entity_id: Optional[str]
    details: Dict[str, Any]
    created_at: datetime
    created_by: str


@dataclass(frozen=True)
class TickerAliasRecord:
    """Canonical mapping of ticker symbols to CIK/company names."""

    ticker: str
    cik: str
    company_name: str
    updated_at: datetime


@dataclass(frozen=True)
class PortfolioMetadataRecord:
    """Portfolio metadata including name, benchmark, and strategy."""

    portfolio_id: str
    name: str
    base_currency: str
    benchmark_index: Optional[str]
    inception_date: datetime
    strategy_type: Optional[str]
    created_at: datetime


@dataclass(frozen=True)
class PortfolioHoldingRecord:
    """Individual holding in a portfolio."""

    ticker: str
    portfolio_id: str
    position_date: datetime
    shares: Optional[float]
    weight: Optional[float]
    cost_basis: Optional[float]
    market_value: Optional[float]
    currency: str
    account_id: Optional[str]


@dataclass(frozen=True)
class PolicyConstraintRecord:
    """Policy constraint definition for a portfolio."""

    constraint_id: str
    portfolio_id: str
    constraint_type: str
    target_value: Optional[float]
    min_value: Optional[float]
    max_value: Optional[float]
    unit: Optional[str]
    active: bool
    dimension: Optional[str]  # e.g., "Technology" for sector_limit, ticker for allocation_cap


@dataclass(frozen=True)
class PolicyDocumentRecord:
    """Policy document metadata and parsed content."""

    document_id: str
    portfolio_id: str
    document_type: str  # IPS, guidelines, mandate
    file_path: Optional[str]
    parsed_constraints: Dict[str, Any]
    uploaded_at: datetime


@dataclass(frozen=True)
class UploadedDocumentRecord:
    """Document uploaded by a user for conversational analysis."""

    document_id: str
    conversation_id: Optional[str]
    filename: str
    file_type: Optional[str]
    file_size: int
    content: str
    metadata: Dict[str, Any]
    uploaded_at: datetime
    updated_at: datetime


@dataclass(frozen=True)
class PortfolioTransactionRecord:
    """Portfolio transaction (buy, sell, rebalance)."""

    transaction_id: str
    portfolio_id: str
    ticker: str
    trade_date: datetime
    action: str  # buy, sell, rebalance
    shares: Optional[float]
    price: Optional[float]
    commission: Optional[float]
    notes: Optional[str]


@dataclass(frozen=True)
class ExposureSnapshotRecord:
    """Exposure snapshot by dimension (sector, factor, issuer, geography)."""

    snapshot_id: str
    portfolio_id: str
    snapshot_date: datetime
    exposure_type: str
    dimension: str
    value: Optional[float]
    weight: Optional[float]


@dataclass(frozen=True)
class AttributionResultRecord:
    """Performance attribution result."""

    attribution_id: str
    portfolio_id: str
    start_date: datetime
    end_date: datetime
    attribution_type: str  # allocation, selection, interaction
    dimension: str
    contribution: Optional[float]
    created_at: datetime


# -----------------------------
# Helpers
# -----------------------------

def _ensure_utc(value: datetime) -> datetime:
    """Normalise naive datetimes to UTC-aware values."""
    if value.tzinfo is None:
        return value.replace(tzinfo=timezone.utc)
    return value.astimezone(timezone.utc)


def _iso_utc(value: datetime) -> str:
    """UTC ISO-8601 with explicit offset (+00:00)."""
    return _ensure_utc(value).isoformat()


def _json_default(value: Any) -> str:
    """JSON serializer for datetime/date objects."""
    if isinstance(value, (datetime, date)):
        return value.isoformat()
    raise TypeError(f"Object of type {type(value)!r} is not JSON serialisable")


def _json_dumps(payload: Any) -> str:
    """Serialise payloads with a deterministic JSON representation."""
    return json.dumps(payload, default=_json_default, separators=(",", ":"))


def _parse_dt(s: Optional[str]) -> Optional[datetime]:
    """Parse ISO strings persisted in SQLite back into datetimes."""
    if not s:
        return None
    return datetime.fromisoformat(s)


def _normalize_ticker(t: Optional[str]) -> str:
    """Upper-case ticker symbols while handling None/empty inputs."""
    return (t or "").upper()


def _connect(database_path: Path) -> sqlite3.Connection:
    """Open a SQLite connection with recommended pragmas enabled."""
    conn = sqlite3.connect(database_path)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA synchronous=NORMAL;")
    conn.execute("PRAGMA temp_store=MEMORY;")
    conn.execute("PRAGMA cache_size=-16000;")
    conn.execute("PRAGMA foreign_keys=ON;")
    return conn


def _table_has_column(connection: sqlite3.Connection, table: str, column: str) -> bool:
    """Return True if the specified column already exists on the table."""
    rows = connection.execute(f"PRAGMA table_info({table})")
    return any(row[1] == column for row in rows)


def _ensure_column(connection: sqlite3.Connection, table: str, column: str, definition: str) -> None:
    """Add a column to a table if it is missing (idempotent)."""
    if not _table_has_column(connection, table, column):
        connection.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")


def _apply_migrations(connection: sqlite3.Connection) -> None:
    """Upgrade older schemas so they match the current expectations."""
    tables = {
        row[0] for row in connection.execute("SELECT name FROM sqlite_master WHERE type='table'")
    }

    # financial_facts
    if "financial_facts" in tables:
        _ensure_column(connection, "financial_facts", "cik", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "financial_facts", "fiscal_period", "TEXT")
        _ensure_column(connection, "financial_facts", "period", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "financial_facts", "unit", "TEXT")
        _ensure_column(connection, "financial_facts", "company_name", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "financial_facts", "source", "TEXT NOT NULL DEFAULT 'edgar'")
        _ensure_column(connection, "financial_facts", "source_filing", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "financial_facts", "raw", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "financial_facts", "period_start", "TEXT")
        _ensure_column(connection, "financial_facts", "period_end", "TEXT")
        _ensure_column(connection, "financial_facts", "adjusted", "INTEGER NOT NULL DEFAULT 0")
        _ensure_column(connection, "financial_facts", "adjustment_note", "TEXT")
        _ensure_column(connection, "financial_facts", "ingested_at", "TEXT")

        # Normalize legacy values
        connection.execute("""
            UPDATE financial_facts
            SET period_start = NULLIF(period_start, ''),
                period_end   = NULLIF(period_end, ''),
                ingested_at  = NULLIF(ingested_at, '')
        """)
        connection.execute("""
            UPDATE financial_facts
            SET source_filing = ''
            WHERE source_filing IS NULL
        """)
        connection.execute(
            """
            DELETE FROM financial_facts
            WHERE rowid NOT IN (
                SELECT MAX(rowid)
                FROM financial_facts
                GROUP BY ticker, metric, period, source, source_filing
            )
            """
        )
        connection.execute(
            """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_financial_facts_unique
            ON financial_facts (ticker, metric, period, source, source_filing)
            """
        )

    # metric_snapshots
    if "metric_snapshots" in tables:
        _ensure_column(connection, "metric_snapshots", "start_year", "INTEGER")
        _ensure_column(connection, "metric_snapshots", "end_year", "INTEGER")
        _ensure_column(connection, "metric_snapshots", "source", "TEXT NOT NULL DEFAULT 'edgar'")
        _ensure_column(connection, "metric_snapshots", "updated_at", "TEXT")

    if "custom_kpis" in tables:
        _ensure_column(connection, "custom_kpis", "frequency", "TEXT")
        _ensure_column(connection, "custom_kpis", "unit", "TEXT")
        _ensure_column(connection, "custom_kpis", "inputs", "TEXT NOT NULL DEFAULT '[]'")
        _ensure_column(connection, "custom_kpis", "source_tags", "TEXT NOT NULL DEFAULT '[]'")
        _ensure_column(connection, "custom_kpis", "metadata", "TEXT NOT NULL DEFAULT '{}'")
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS custom_kpi_versions (
                version_id TEXT PRIMARY KEY,
                kpi_id TEXT NOT NULL,
                version_number INTEGER NOT NULL,
                formula TEXT NOT NULL,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                created_by TEXT NOT NULL,
                FOREIGN KEY (kpi_id) REFERENCES custom_kpis(kpi_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS metric_dictionary (
                alias TEXT PRIMARY KEY,
                raw_alias TEXT,
                canonical_metric TEXT NOT NULL,
                source_system TEXT NOT NULL DEFAULT 'sec_xbrl',
                primary_tag TEXT,
                fallback_tags TEXT NOT NULL DEFAULT '[]',
                description TEXT,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                created_by TEXT NOT NULL DEFAULT 'system',
                updated_by TEXT NOT NULL DEFAULT 'system'
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS data_source_preferences (
                preference_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                source_order TEXT NOT NULL DEFAULT '[]',
                fallback_rules TEXT NOT NULL DEFAULT '{}',
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS analysis_templates (
                template_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                kpi_ids TEXT NOT NULL DEFAULT '[]',
                layout_config TEXT NOT NULL DEFAULT '{}',
                parameter_schema TEXT NOT NULL DEFAULT '{}',
                data_preferences_id TEXT,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS analysis_template_versions (
                version_id TEXT PRIMARY KEY,
                template_id TEXT NOT NULL,
                version_number INTEGER NOT NULL,
                snapshot TEXT NOT NULL,
                created_at TEXT NOT NULL,
                created_by TEXT NOT NULL,
                change_summary TEXT,
                FOREIGN KEY (template_id) REFERENCES analysis_templates(template_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS analytics_profiles (
                profile_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                kpi_library TEXT NOT NULL DEFAULT '[]',
                template_ids TEXT NOT NULL DEFAULT '[]',
                data_preferences_id TEXT,
                output_preferences TEXT NOT NULL DEFAULT '{}',
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS saved_sessions (
                session_id TEXT PRIMARY KEY,
                profile_id TEXT NOT NULL,
                user_id TEXT NOT NULL,
                name TEXT,
                workspace_state TEXT NOT NULL,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                expires_at TEXT,
                FOREIGN KEY (profile_id) REFERENCES analytics_profiles(profile_id)
            )
            """
        )

    if "custom_models" in tables:
        _ensure_column(connection, "custom_models", "description", "TEXT")
        _ensure_column(connection, "custom_models", "target_metric", "TEXT")
        _ensure_column(connection, "custom_models", "frequency", "TEXT")
        _ensure_column(connection, "custom_models", "forecast_horizon", "INTEGER")
        _ensure_column(connection, "custom_models", "status", "TEXT NOT NULL DEFAULT 'configured'")
        _ensure_column(connection, "custom_models", "regressors", "TEXT NOT NULL DEFAULT '[]'")
        _ensure_column(connection, "custom_models", "metadata", "TEXT NOT NULL DEFAULT '{}'")

    if "user_forecasting_plugins" in tables:
        _ensure_column(connection, "user_forecasting_plugins", "description", "TEXT")
        _ensure_column(connection, "user_forecasting_plugins", "metadata", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "user_forecasting_plugins", "state", "TEXT")
        _ensure_column(connection, "user_forecasting_plugins", "last_trained_at", "TEXT")

    if "model_runs" in tables:
        _ensure_column(connection, "model_runs", "assumptions", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "model_runs", "driver_explanations", "TEXT NOT NULL DEFAULT '[]'")
        _ensure_column(connection, "model_runs", "artifacts", "TEXT NOT NULL DEFAULT '[]'")

    if "model_artifacts" not in tables:
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS model_artifacts (
                artifact_id TEXT PRIMARY KEY,
                model_id TEXT NOT NULL,
                run_id TEXT,
                artifact_type TEXT NOT NULL,
                location TEXT,
                data BLOB,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                FOREIGN KEY (model_id) REFERENCES custom_models(model_id),
                FOREIGN KEY (run_id) REFERENCES model_runs(run_id)
            )
            """
        )

    if "template_render_jobs" not in tables:
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS template_render_jobs (
                job_id TEXT PRIMARY KEY,
                template_id TEXT NOT NULL,
                user_id TEXT NOT NULL,
                ticker TEXT,
                context TEXT NOT NULL DEFAULT '{}',
                audit_log TEXT NOT NULL DEFAULT '[]',
                output_path TEXT,
                created_at TEXT NOT NULL,
                FOREIGN KEY (template_id) REFERENCES report_templates(template_id)
            )
            """
        )

    # kpi_values provenance upgrades
    if "kpi_values" in tables:
        _ensure_column(connection, "kpi_values", "unit", "TEXT")
        _ensure_column(connection, "kpi_values", "method", "TEXT NOT NULL DEFAULT 'derived'")
        _ensure_column(connection, "kpi_values", "source", "TEXT NOT NULL DEFAULT 'SEC'")
        _ensure_column(connection, "kpi_values", "source_ref", "TEXT")
        _ensure_column(connection, "kpi_values", "warning", "TEXT")
        _ensure_column(connection, "kpi_values", "updated_at", "TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP")
        connection.execute("""
            UPDATE metric_snapshots
            SET updated_at = NULLIF(updated_at, '')
        """)

    # market_quotes
    if "market_quotes" in tables:
        _ensure_column(connection, "market_quotes", "data", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "market_quotes", "source", "TEXT NOT NULL DEFAULT 'yahoo'")

    # audit_events
    if "audit_events" in tables:
        _ensure_column(connection, "audit_events", "ticker", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "audit_events", "details", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "audit_events", "created_by", "TEXT NOT NULL DEFAULT 'system'")

    # company_filings
    if "company_filings" in tables:
        _ensure_column(connection, "company_filings", "data", "TEXT NOT NULL DEFAULT '{}'")
        _ensure_column(connection, "company_filings", "source", "TEXT NOT NULL DEFAULT 'edgar'")

    # ticker_aliases
    if "ticker_aliases" in tables:
        _ensure_column(connection, "ticker_aliases", "cik", "TEXT NOT NULL")
        _ensure_column(connection, "ticker_aliases", "company_name", "TEXT NOT NULL DEFAULT ''")
        _ensure_column(connection, "ticker_aliases", "updated_at", "TEXT NOT NULL")


# -----------------------------
# Schema init
# -----------------------------

def initialise(database_path: Path) -> None:
    """Create the database file and ensure core tables exist."""
    # Convert to Path if string is provided
    if isinstance(database_path, str):
        database_path = Path(database_path)
    
    database_path.parent.mkdir(parents=True, exist_ok=True)
    with _connect(database_path) as connection:
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS conversation_metadata (
                conversation_id TEXT PRIMARY KEY,
                title TEXT,
                updated_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS scenario_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                scenario_name TEXT NOT NULL,
                metrics TEXT NOT NULL,
                narrative TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS company_filings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cik TEXT NOT NULL,
                ticker TEXT NOT NULL,
                accession_number TEXT NOT NULL,
                form_type TEXT NOT NULL,
                filed_at TEXT NOT NULL,
                period_of_report TEXT,
                acceptance_datetime TEXT,
                data TEXT NOT NULL DEFAULT '{}',
                source TEXT NOT NULL DEFAULT 'edgar',
                UNIQUE(cik, accession_number)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS financial_facts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cik TEXT NOT NULL,
                ticker TEXT NOT NULL,
                company_name TEXT NOT NULL DEFAULT '',
                metric TEXT NOT NULL,
                fiscal_year INTEGER,
                fiscal_period TEXT,
                period TEXT NOT NULL DEFAULT '',
                value REAL,
                unit TEXT,
                source_filing TEXT NOT NULL DEFAULT '',
                raw TEXT NOT NULL DEFAULT '{}',
                period_start TEXT,
                period_end TEXT,
                adjusted INTEGER NOT NULL DEFAULT 0,
                adjustment_note TEXT,
                source TEXT NOT NULL DEFAULT 'edgar',
                ingested_at TEXT,
                UNIQUE(ticker, metric, period, source, source_filing)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS market_quotes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                price REAL NOT NULL,
                currency TEXT,
                volume REAL,
                quote_time TEXT NOT NULL,
                data TEXT NOT NULL DEFAULT '{}',
                source TEXT NOT NULL DEFAULT 'yahoo',
                UNIQUE(ticker, quote_time, source)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS audit_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL DEFAULT '',
                event_type TEXT NOT NULL,
                entity_id TEXT,
                details TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                created_by TEXT NOT NULL DEFAULT 'system'
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS metric_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                metric TEXT NOT NULL,
                period TEXT NOT NULL,
                start_year INTEGER,
                end_year INTEGER,
                value REAL,
                source TEXT NOT NULL DEFAULT 'edgar',
                updated_at TEXT,
                UNIQUE(ticker, metric, period, source)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS kpi_values (
                ticker TEXT NOT NULL,
                fiscal_year INTEGER,
                fiscal_quarter INTEGER,
                metric_id TEXT NOT NULL,
                value REAL,
                unit TEXT,
                method TEXT NOT NULL DEFAULT 'derived',
                source TEXT NOT NULL DEFAULT 'SEC',
                source_ref TEXT,
                warning TEXT,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (ticker, fiscal_year, fiscal_quarter, metric_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS ticker_aliases (
                ticker TEXT PRIMARY KEY,
                cik TEXT NOT NULL,
                company_name TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS ml_forecasts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id TEXT NOT NULL,
                forecast_name TEXT,
                ticker TEXT NOT NULL,
                metric TEXT NOT NULL,
                method TEXT NOT NULL,
                periods INTEGER NOT NULL,
                predicted_values TEXT NOT NULL,
                confidence_intervals_low TEXT NOT NULL,
                confidence_intervals_high TEXT NOT NULL,
                model_confidence REAL,
                parameters TEXT NOT NULL DEFAULT '{}',
                explainability TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                created_by TEXT NOT NULL DEFAULT 'user',
                UNIQUE(conversation_id, forecast_name)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS portfolio_metadata (
                portfolio_id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                base_currency TEXT NOT NULL,
                benchmark_index TEXT,
                inception_date TEXT NOT NULL,
                strategy_type TEXT,
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS portfolio_holdings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                portfolio_id TEXT NOT NULL,
                position_date TEXT NOT NULL,
                shares REAL,
                weight REAL,
                cost_basis REAL,
                market_value REAL,
                currency TEXT NOT NULL,
                account_id TEXT
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS policy_constraints (
                constraint_id TEXT PRIMARY KEY,
                portfolio_id TEXT NOT NULL,
                constraint_type TEXT NOT NULL,
                target_value REAL,
                min_value REAL,
                max_value REAL,
                unit TEXT,
                active INTEGER NOT NULL DEFAULT 1,
                dimension TEXT
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS policy_documents (
                document_id TEXT PRIMARY KEY,
                portfolio_id TEXT NOT NULL,
                document_type TEXT NOT NULL,
                file_path TEXT,
                parsed_constraints TEXT NOT NULL DEFAULT '{}',
                uploaded_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS portfolio_transactions (
                transaction_id TEXT PRIMARY KEY,
                portfolio_id TEXT NOT NULL,
                ticker TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                action TEXT NOT NULL,
                shares REAL,
                price REAL,
                commission REAL,
                notes TEXT
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS exposure_snapshots (
                snapshot_id TEXT PRIMARY KEY,
                portfolio_id TEXT NOT NULL,
                snapshot_date TEXT NOT NULL,
                exposure_type TEXT NOT NULL,
                dimension TEXT NOT NULL,
                value REAL,
                weight REAL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS attribution_results (
                attribution_id TEXT PRIMARY KEY,
                portfolio_id TEXT NOT NULL,
                start_date TEXT NOT NULL,
                end_date TEXT NOT NULL,
                attribution_type TEXT NOT NULL,
                dimension TEXT NOT NULL,
                contribution REAL,
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS custom_kpis (
                kpi_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL DEFAULT 'default',
                name TEXT NOT NULL,
                formula TEXT NOT NULL,
                description TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                frequency TEXT,
                unit TEXT,
                inputs TEXT NOT NULL DEFAULT '[]',
                source_tags TEXT NOT NULL DEFAULT '[]',
                metadata TEXT NOT NULL DEFAULT '{}'
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS custom_kpi_usage (
                usage_id TEXT PRIMARY KEY,
                kpi_id TEXT NOT NULL,
                ticker TEXT NOT NULL,
                period TEXT NOT NULL,
                value REAL,
                calculated_at TEXT NOT NULL,
                FOREIGN KEY (kpi_id) REFERENCES custom_kpis(kpi_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS kpi_dependencies (
                kpi_id TEXT NOT NULL,
                metric_name TEXT NOT NULL,
                dependency_type TEXT NOT NULL DEFAULT 'metric',
                PRIMARY KEY (kpi_id, metric_name),
                FOREIGN KEY (kpi_id) REFERENCES custom_kpis(kpi_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS custom_kpi_versions (
                version_id TEXT PRIMARY KEY,
                kpi_id TEXT NOT NULL,
                version_number INTEGER NOT NULL,
                formula TEXT NOT NULL,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                created_by TEXT NOT NULL,
                FOREIGN KEY (kpi_id) REFERENCES custom_kpis(kpi_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS custom_models (
                model_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL DEFAULT 'default',
                name TEXT NOT NULL,
                model_type TEXT NOT NULL,
                parameters TEXT NOT NULL DEFAULT '{}',
                metrics TEXT NOT NULL DEFAULT '[]',
                created_at TEXT NOT NULL,
                description TEXT,
                target_metric TEXT,
                frequency TEXT,
                forecast_horizon INTEGER,
                status TEXT NOT NULL DEFAULT 'configured',
                regressors TEXT NOT NULL DEFAULT '[]',
                metadata TEXT NOT NULL DEFAULT '{}'
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS user_forecasting_plugins (
                plugin_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL DEFAULT 'default',
                name TEXT NOT NULL,
                class_name TEXT NOT NULL,
                module_path TEXT NOT NULL,
                description TEXT,
                metadata TEXT NOT NULL DEFAULT '{}',
                state TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                last_trained_at TEXT
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS model_runs (
                run_id TEXT PRIMARY KEY,
                model_id TEXT NOT NULL,
                ticker TEXT NOT NULL,
                forecast_periods TEXT NOT NULL DEFAULT '[]',
                results TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                assumptions TEXT NOT NULL DEFAULT '{}',
                driver_explanations TEXT NOT NULL DEFAULT '[]',
                artifacts TEXT NOT NULL DEFAULT '[]',
                FOREIGN KEY (model_id) REFERENCES custom_models(model_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS model_assumptions (
                assumption_id TEXT PRIMARY KEY,
                model_id TEXT NOT NULL,
                variable TEXT NOT NULL,
                value REAL,
                scenario TEXT NOT NULL DEFAULT 'base',
                FOREIGN KEY (model_id) REFERENCES custom_models(model_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS model_artifacts (
                artifact_id TEXT PRIMARY KEY,
                model_id TEXT NOT NULL,
                run_id TEXT,
                artifact_type TEXT NOT NULL,
                location TEXT,
                data BLOB,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                FOREIGN KEY (model_id) REFERENCES custom_models(model_id),
                FOREIGN KEY (run_id) REFERENCES model_runs(run_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS report_templates (
                template_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL DEFAULT 'default',
                name TEXT NOT NULL,
                file_type TEXT NOT NULL,
                file_path TEXT NOT NULL,
                structure TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS template_placeholders (
                placeholder_id TEXT PRIMARY KEY,
                template_id TEXT NOT NULL,
                placeholder_name TEXT NOT NULL,
                data_type TEXT NOT NULL,
                source_metric TEXT,
                FOREIGN KEY (template_id) REFERENCES report_templates(template_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS frameworks (
                framework_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL DEFAULT 'default',
                name TEXT NOT NULL,
                file_type TEXT NOT NULL,
                file_path TEXT NOT NULL,
                extracted_content TEXT NOT NULL DEFAULT '{}',
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS template_render_jobs (
                job_id TEXT PRIMARY KEY,
                template_id TEXT NOT NULL,
                user_id TEXT NOT NULL,
                ticker TEXT,
                context TEXT NOT NULL DEFAULT '{}',
                audit_log TEXT NOT NULL DEFAULT '[]',
                output_path TEXT,
                created_at TEXT NOT NULL,
                FOREIGN KEY (template_id) REFERENCES report_templates(template_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS framework_kpis (
                framework_id TEXT NOT NULL,
                kpi_name TEXT NOT NULL,
                kpi_definition TEXT,
                extracted_from TEXT,
                PRIMARY KEY (framework_id, kpi_name),
                FOREIGN KEY (framework_id) REFERENCES frameworks(framework_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS framework_methodology (
                framework_id TEXT NOT NULL,
                section TEXT NOT NULL,
                content TEXT NOT NULL,
                applies_to TEXT,
                PRIMARY KEY (framework_id, section),
                FOREIGN KEY (framework_id) REFERENCES frameworks(framework_id)
            )
            """
        )
        connection.execute(
            """
            CREATE TABLE IF NOT EXISTS uploaded_documents (
                document_id TEXT PRIMARY KEY,
                conversation_id TEXT,
                filename TEXT NOT NULL,
                file_type TEXT NOT NULL,
                file_size INTEGER NOT NULL,
                content TEXT NOT NULL,
                metadata TEXT NOT NULL DEFAULT '{}',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )

        _apply_migrations(connection)

        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_financial_facts_key
            ON financial_facts (ticker, metric, period, source)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_market_quotes_ticker_time
            ON market_quotes (ticker, quote_time DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_metric_snapshots_ticker
            ON metric_snapshots (ticker, metric)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_kpi_values_lookup
            ON kpi_values (ticker, metric_id, fiscal_year, fiscal_quarter)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_scenario_results_ticker
            ON scenario_results (ticker, scenario_name, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_ticker_aliases_name
            ON ticker_aliases (company_name)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_portfolio_holdings_portfolio_date
            ON portfolio_holdings (portfolio_id, position_date DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_policy_constraints_portfolio
            ON policy_constraints (portfolio_id, active)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_portfolio_transactions_portfolio_date
            ON portfolio_transactions (portfolio_id, trade_date DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_exposure_snapshots_portfolio_date
            ON exposure_snapshots (portfolio_id, snapshot_date DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_attribution_results_portfolio
            ON attribution_results (portfolio_id, start_date DESC, end_date DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_custom_kpis_user
            ON custom_kpis (user_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_metric_dictionary_canonical
            ON metric_dictionary (canonical_metric)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_data_source_preferences_user
            ON data_source_preferences (user_id, updated_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_analysis_templates_user
            ON analysis_templates (user_id, updated_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_analytics_profiles_user
            ON analytics_profiles (user_id, updated_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_saved_sessions_profile
            ON saved_sessions (profile_id, updated_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_custom_kpi_usage_lookup
            ON custom_kpi_usage (kpi_id, ticker, period)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_kpi_dependencies_lookup
            ON kpi_dependencies (kpi_id, metric_name)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_custom_models_user
            ON custom_models (user_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_forecasting_plugins_user
            ON user_forecasting_plugins (user_id, name)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_model_runs_lookup
            ON model_runs (model_id, ticker, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_model_artifacts_model
            ON model_artifacts (model_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_report_templates_user
            ON report_templates (user_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_frameworks_user
            ON frameworks (user_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_uploaded_documents_conversation
            ON uploaded_documents (conversation_id, created_at DESC)
            """
        )
        connection.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_template_render_jobs_template
            ON template_render_jobs (template_id, created_at DESC)
            """
        )
        connection.commit()


# -----------------------------
# Conversations
# -----------------------------

def log_message(
    database_path: Path,
    conversation_id: str,
    role: str,
    content: str,
    created_at: Optional[datetime] = None,
) -> None:
    """Append a chat message to the persisted conversation log."""
    created_at = _ensure_utc(created_at or datetime.now(timezone.utc))
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT INTO conversations (conversation_id, role, content, created_at)
            VALUES (?, ?, ?, ?)
            """,
            (conversation_id, role, content, _iso_utc(created_at)),
        )
        connection.commit()


def fetch_conversation(
    database_path: Path, conversation_id: str
) -> Iterable[Message]:
    """Yield all messages stored for a conversation id."""
    with _connect(database_path) as connection:
        rows = connection.execute(
            """
            SELECT role, content, created_at
            FROM conversations
            WHERE conversation_id = ?
            ORDER BY id ASC
            """,
            (conversation_id,),
        )
        for role, content, created_at in rows:
            yield Message(
                role=role,
                content=content,
                created_at=_parse_dt(created_at) or datetime(1970, 1, 1, tzinfo=timezone.utc),
            )


def fetch_uploaded_documents(
    database_path: Path,
    conversation_id: Optional[str],
    *,
    limit: int = 5,
    include_unscoped: bool = True,
) -> List[UploadedDocumentRecord]:
    """
    Fetch recently uploaded documents for a conversation.

    Args:
        database_path: Path to the SQLite database.
        conversation_id: Conversation identifier to scope results.
        limit: Maximum number of documents to return.
        include_unscoped: When True, include recent documents that were uploaded
            without an associated conversation_id (e.g. legacy uploads).
    """
    if limit <= 0:
        return []

    def _rows_to_records(rows: Iterable[sqlite3.Row]) -> List[UploadedDocumentRecord]:
        records: List[UploadedDocumentRecord] = []
        for row in rows:
            raw_metadata = row[6] if len(row) > 6 else "{}"
            try:
                metadata: Dict[str, Any] = json.loads(raw_metadata) if raw_metadata else {}
            except json.JSONDecodeError:
                metadata = {}
            records.append(
                UploadedDocumentRecord(
                    document_id=row[0],
                    conversation_id=row[1],
                    filename=row[2],
                    file_type=row[3],
                    file_size=row[4],
                    content=row[5] or "",
                    metadata=metadata,
                    uploaded_at=_parse_dt(row[7]) or datetime(1970, 1, 1, tzinfo=timezone.utc),
                    updated_at=_parse_dt(row[8]) or datetime(1970, 1, 1, tzinfo=timezone.utc),
                )
            )
        return records

    records: List[UploadedDocumentRecord] = []
    seen_ids: set[str] = set()

    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row

        if conversation_id:
            convo_rows = connection.execute(
                """
                SELECT document_id, conversation_id, filename, file_type, file_size,
                       content, metadata, created_at, updated_at
                FROM uploaded_documents
                WHERE conversation_id = ?
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (conversation_id, limit),
            ).fetchall()
            convo_records = _rows_to_records(convo_rows)
            records.extend(convo_records)
            seen_ids.update(record.document_id for record in convo_records)

        remaining = limit - len(records)

        if include_unscoped and remaining > 0:
            unscoped_rows = connection.execute(
                """
                SELECT document_id, conversation_id, filename, file_type, file_size,
                       content, metadata, created_at, updated_at
                FROM uploaded_documents
                WHERE conversation_id IS NULL
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (remaining,),
            ).fetchall()
            for record in _rows_to_records(unscoped_rows):
                if record.document_id in seen_ids:
                    continue
                records.append(record)
                seen_ids.add(record.document_id)
                if len(records) >= limit:
                    break

    return records


@contextmanager
def temporary_connection(database_path: Path) -> Iterator[sqlite3.Connection]:
    """Provide a context-managed SQLite connection for bulk work."""
    connection = _connect(database_path)
    try:
        yield connection
        connection.commit()
    finally:
        connection.close()


def most_recent_conversation_id(database_path: Path) -> Optional[str]:
    """Return the latest conversation id seen in storage."""
    with _connect(database_path) as connection:
        row = connection.execute(
            """
            SELECT conversation_id
            FROM conversations
            ORDER BY id DESC
            LIMIT 1
            """
        ).fetchone()
        return row[0] if row else None


def iter_conversation_summaries(database_path: Path) -> Iterator[Tuple[str, int]]:
    """Yield lightweight conversation summaries for the UI."""
    with _connect(database_path) as connection:
        rows = connection.execute(
            """
            SELECT conversation_id, COUNT(*) AS message_count
            FROM conversations
            GROUP BY conversation_id
            ORDER BY MAX(created_at) DESC
            """
        )
        yield from rows


def get_conversation_title(database_path: Path, conversation_id: str) -> Optional[str]:
    """Retrieve the custom title for a conversation, if set."""
    with _connect(database_path) as connection:
        row = connection.execute(
            """
            SELECT title
            FROM conversation_metadata
            WHERE conversation_id = ?
            """,
            (conversation_id,),
        ).fetchone()
        return row[0] if row else None


def set_conversation_title(database_path: Path, conversation_id: str, title: str) -> None:
    """Set or update the custom title for a conversation."""
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT OR REPLACE INTO conversation_metadata (conversation_id, title, updated_at)
            VALUES (?, ?, ?)
            """,
            (conversation_id, title, _iso_utc(datetime.now(timezone.utc))),
        )
        connection.commit()


# -----------------------------
# Company Filings
# -----------------------------

def bulk_upsert_company_filings(
    database_path: Path,
    filings: Sequence["FilingRecord"],
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Upsert SEC filing metadata in bulk."""
    if not filings:
        return 0

    payload = [
        (
            filing.cik,
            _normalize_ticker(filing.ticker),
            filing.accession_number,
            filing.form_type,
            _iso_utc(filing.filed_at),
            filing.period_of_report.isoformat() if filing.period_of_report else None,
            filing.acceptance_datetime.isoformat() if filing.acceptance_datetime else None,
            _json_dumps(filing.data),
            filing.source,
        )
        for filing in filings
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT OR IGNORE INTO company_filings (
                cik, ticker, accession_number, form_type, filed_at,
                period_of_report, acceptance_datetime, data, source
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


# -----------------------------
# Financial Facts
# -----------------------------

def bulk_upsert_financial_facts(
    database_path: Path,
    facts: Sequence["FinancialFact"],
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Upsert SEC financial facts in batches."""
    if not facts:
        return 0

    def _fact_priority(fact: "FinancialFact") -> Tuple[int, datetime]:
        """Rank facts so FY/TTM rows win over interim duplicates."""
        period = (fact.period or fact.fiscal_period or "").upper()
        score = 0
        if "FY" in period:
            score += 3
        elif "TTM" in period or "LTM" in period:
            score += 2
        elif "HY" in period:
            score += 1
        if "Q" in period and "FY" not in period:
            score -= 1
        timestamp = fact.ingested_at or datetime.min.replace(tzinfo=timezone.utc)
        return (score, timestamp)

    dedup: Dict[Tuple[str, str, Optional[int]], "FinancialFact"] = {}
    for fact in facts:
        key = (
            _normalize_ticker(fact.ticker),
            fact.metric.lower(),
            fact.fiscal_year,
        )
        existing = dedup.get(key)
        if existing is None or _fact_priority(fact) >= _fact_priority(existing):
            dedup[key] = fact

    payload = [
        (
            fact.cik,
            _normalize_ticker(fact.ticker),
            fact.company_name or "",
            fact.metric.lower(),
            fact.fiscal_year,
            fact.fiscal_period,
            fact.period,
            fact.value,
            fact.unit,
            (fact.source_filing or ""),
            _json_dumps(fact.raw),
            fact.period_start.isoformat() if fact.period_start else None,
            fact.period_end.isoformat() if fact.period_end else None,
            1 if fact.adjusted else 0,
            fact.adjustment_note,
            fact.source,
            _iso_utc(fact.ingested_at) if fact.ingested_at else None,
        )
        for fact in dedup.values()
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO financial_facts (
                cik, ticker, company_name, metric, fiscal_year, fiscal_period, period, value,
                unit, source_filing, raw, period_start, period_end, adjusted,
                adjustment_note, source, ingested_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT DO UPDATE SET
                cik = excluded.cik,
                company_name = excluded.company_name,
                fiscal_year = excluded.fiscal_year,
                fiscal_period = excluded.fiscal_period,
                period = excluded.period,
                value = excluded.value,
                unit = excluded.unit,
                raw = excluded.raw,
                period_start = excluded.period_start,
                period_end = excluded.period_end,
                adjusted = excluded.adjusted,
                adjustment_note = excluded.adjustment_note,
                source = excluded.source,
                source_filing = excluded.source_filing,
                ingested_at = excluded.ingested_at
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def fetch_financial_facts(
    database_path: Path,
    ticker: Optional[str] = None,
    *,
    fiscal_year: Optional[int] = None,
    metric: Optional[str] = None,
    limit: Optional[int] = None,
) -> List[FinancialFactRecord]:
    """Stream financial facts filtered by optional ticker/year/metric."""
    sql = [
        "SELECT ticker, cik, metric, fiscal_year, fiscal_period, period, value, unit, source,",
        "       source_filing, period_start, period_end, adjusted, adjustment_note, ingested_at, raw",
        "FROM financial_facts",
        "WHERE 1=1",
    ]
    params: List[Any] = []

    if ticker is not None:
        sql.append("AND ticker = ?")
        params.append(_normalize_ticker(ticker))

    if fiscal_year is not None:
        sql.append("AND fiscal_year = ?")
        params.append(fiscal_year)
    if metric is not None:
        sql.append("AND metric = ?")
        params.append(metric.lower())

    sql.append("ORDER BY (fiscal_year IS NULL) ASC, fiscal_year DESC, period DESC, metric ASC")
    if limit is not None:
        sql.append("LIMIT ?")
        params.append(limit)

    query = "\n".join(sql)
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(query, params).fetchall()

    records: List[FinancialFactRecord] = []
    for row in rows:
        raw_payload = row["raw"]
        raw_data = json.loads(raw_payload) if raw_payload else None
        records.append(
            FinancialFactRecord(
                ticker=_normalize_ticker(row["ticker"]),
                metric=row["metric"],
                fiscal_year=row["fiscal_year"],
                fiscal_period=row["fiscal_period"],
                period=row["period"],
                value=row["value"],
                unit=row["unit"],
                source=row["source"],
                source_filing=row["source_filing"] or None,
                period_start=_parse_dt(row["period_start"]),
                period_end=_parse_dt(row["period_end"]),
                adjusted=bool(row["adjusted"]),
                adjustment_note=row["adjustment_note"],
                ingested_at=_parse_dt(row["ingested_at"]),
                cik=row["cik"],
                raw=raw_data,
            )
        )
    return records


# -----------------------------
# Market Quotes
# -----------------------------

def bulk_insert_market_quotes(
    database_path: Path,
    quotes: Sequence["MarketQuote"],
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Insert or replace market quotes used for ratios."""
    if not quotes:
        return 0

    payload = [
        (
            _normalize_ticker(quote.ticker),
            quote.price,
            quote.currency,
            quote.volume,
            _iso_utc(quote.timestamp),
            _json_dumps(quote.raw),
            quote.source,
        )
        for quote in quotes
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT OR IGNORE INTO market_quotes (
                ticker, price, currency, volume, quote_time, data, source
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def fetch_latest_quote(database_path: Path, ticker: str) -> Optional[Dict[str, Any]]:
    """Return the most recent cached quote for a ticker."""
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        row = connection.execute(
            """
            SELECT price, currency, volume, quote_time, data, source
            FROM market_quotes
            WHERE ticker = ?
            ORDER BY quote_time DESC
            LIMIT 1
            """,
            (_normalize_ticker(ticker),),
        ).fetchone()

    if row is None:
        return None

    return {
        "price": row["price"],
        "currency": row["currency"],
        "volume": row["volume"],
        "timestamp": _parse_dt(row["quote_time"]),
        "source": row["source"],
        "raw": json.loads(row["data"]) if row["data"] else {},
    }


def fetch_quote_on_or_before(
    database_path: Path, ticker: str, *, before: datetime
) -> Optional[Dict[str, Any]]:
    """Return the closest quote snapshot at or before a timestamp."""
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        row = connection.execute(
            """
            SELECT price, currency, volume, quote_time, data, source
            FROM market_quotes
            WHERE ticker = ? AND quote_time <= ?
            ORDER BY quote_time DESC
            LIMIT 1
            """,
            (_normalize_ticker(ticker), _iso_utc(before)),
        ).fetchone()

    if row is None:
        return None

    return {
        "price": row["price"],
        "currency": row["currency"],
        "volume": row["volume"],
        "timestamp": _parse_dt(row["quote_time"]),
        "source": row["source"],
        "raw": json.loads(row["data"]) if row["data"] else {},
    }


# -----------------------------
# Audit Events
# -----------------------------

def bulk_insert_audit_events(
    database_path: Path,
    events: Sequence["AuditEvent"],
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Persist audit trail events for later inspection."""
    if not events:
        return 0

    payload = [
        (
            _normalize_ticker(event.ticker),
            event.event_type,
            event.entity_id,
            _json_dumps(event.details),
            _iso_utc(event.created_at),
            event.created_by,
        )
        for event in events
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO audit_events (
                ticker, event_type, entity_id, details, created_at, created_by
            ) VALUES (?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def record_audit_event(
    database_path: Path,
    event: "AuditEvent",
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Persist a single audit trail event.

    This is a thin wrapper around ``bulk_insert_audit_events`` that callers can
    use when only one event needs to be recorded (common in error-handling
    paths).  It gracefully ignores ``None`` payloads so defensive callers do not
    need to duplicate guards.
    """
    if event is None:
        return 0
    return bulk_insert_audit_events(database_path, [event], connection=connection)


def fetch_audit_events(
    database_path: Path,
    ticker: str,
    *,
    fiscal_year: Optional[int] = None,
    limit: Optional[int] = None,
) -> List[AuditEventRecord]:
    """Return audit log entries for a ticker."""
    sql = [
        "SELECT event_type, entity_id, details, created_at, created_by",
        "FROM audit_events",
        "WHERE ticker = ?",
    ]
    params: List[Any] = [_normalize_ticker(ticker)]

    if fiscal_year is not None:
        sql.append("AND entity_id LIKE ?")
        params.append(f"%FY{fiscal_year}%")

    sql.append("ORDER BY created_at DESC")
    if limit is not None:
        sql.append("LIMIT ?")
        params.append(limit)

    query = "\n".join(sql)
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(query, params).fetchall()

    events: List[AuditEventRecord] = []
    for row in rows:
        events.append(
            AuditEventRecord(
                ticker=_normalize_ticker(ticker),
                event_type=row["event_type"],
                entity_id=row["entity_id"],
                details=json.loads(row["details"]) if row["details"] else {},
                created_at=_parse_dt(row["created_at"]) or datetime(1970, 1, 1, tzinfo=timezone.utc),
                created_by=row["created_by"],
            )
        )
    return events


def fetch_recent_audit_events(
    database_path: Path,
    limit: int = 20,
) -> List[AuditEventRecord]:
    """Return the most recent audit events across all tickers."""
    sql = [
        "SELECT ticker, event_type, entity_id, details, created_at, created_by",
        "FROM audit_events",
        "ORDER BY created_at DESC",
    ]
    params: List[Any] = []
    if limit:
        sql.append("LIMIT ?")
        params.append(limit)

    query = "\n".join(sql)
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(query, params).fetchall()

    events: List[AuditEventRecord] = []
    for row in rows:
        details_raw = row["details"] or "{}"
        try:
            details = json.loads(details_raw)
        except Exception:
            details = {}
        events.append(
            AuditEventRecord(
                ticker=row["ticker"],
                event_type=row["event_type"],
                entity_id=row["entity_id"],
                details=details,
                created_at=_parse_dt(row["created_at"]) or datetime(1970, 1, 1, tzinfo=timezone.utc),
                created_by=row["created_by"],
            )
        )
    return events


# -----------------------------
# Metric Snapshots
# -----------------------------


def latest_fiscal_year(
    database_path: Path,
    ticker: str,
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> Optional[int]:
    """Return the most recent fiscal year stored for ``ticker``."""
    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        row = connection.execute(
            """
            SELECT MAX(fiscal_year)
            FROM financial_facts
            WHERE ticker = ?
            """,
            (_normalize_ticker(ticker),),
        ).fetchone()
        max_year = row[0] if row and row[0] is not None else None
        return int(max_year) if max_year is not None else None
    finally:
        if own_connection:
            connection.close()


def upsert_ticker_aliases(
    database_path: Path,
    aliases: Sequence[TickerAliasRecord],
    *,
    connection: Optional[sqlite3.Connection] = None,
) -> int:
    """Persist canonical ticker aliases with company names."""
    if not aliases:
        return 0

    payload = [
        (
            _normalize_ticker(alias.ticker),
            alias.cik,
            alias.company_name,
            _iso_utc(alias.updated_at),
        )
        for alias in aliases
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO ticker_aliases (ticker, cik, company_name, updated_at)
            VALUES (?, ?, ?, ?)
            ON CONFLICT(ticker) DO UPDATE SET
                cik = excluded.cik,
                company_name = excluded.company_name,
                updated_at = excluded.updated_at
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def lookup_ticker(
    database_path: Path,
    query: str,
    *,
    allow_partial: bool = False,
) -> Optional[str]:
    """Resolve a natural-language company reference into a ticker symbol."""
    if not query:
        return None

    term = query.strip()
    if not term:
        return None

    ticker_guess = _normalize_ticker(term)
    name_upper = term.upper()

    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row

        # 1. Direct ticker match
        row = connection.execute(
            """
            SELECT ticker
            FROM financial_facts
            WHERE ticker = ?
            LIMIT 1
            """,
            (ticker_guess,),
        ).fetchone()
        if row:
            return row["ticker"]

        row = connection.execute(
            """
            SELECT ticker
            FROM ticker_aliases
            WHERE ticker = ?
            LIMIT 1
            """,
            (ticker_guess,),
        ).fetchone()
        if row:
            return row["ticker"]

        # 2. Exact company name match (case-insensitive)
        row = connection.execute(
            """
            SELECT ticker
            FROM (
                SELECT DISTINCT ticker, company_name
                FROM financial_facts
            )
            WHERE UPPER(company_name) = ?
            ORDER BY LENGTH(company_name) ASC
            LIMIT 1
            """,
            (name_upper,),
        ).fetchone()
        if row:
            return row["ticker"]

        row = connection.execute(
            """
            SELECT ticker
            FROM ticker_aliases
            WHERE UPPER(company_name) = ?
            LIMIT 1
            """,
            (name_upper,),
        ).fetchone()
        if row:
            return row["ticker"]

        if not allow_partial:
            return None

        pattern = f"%{name_upper}%"
        row = connection.execute(
            """
            SELECT ticker
            FROM (
                SELECT DISTINCT ticker, company_name
                FROM financial_facts
                WHERE company_name IS NOT NULL AND company_name <> ''
            )
            WHERE UPPER(company_name) LIKE ?
            ORDER BY LENGTH(company_name) ASC, ticker ASC
            LIMIT 1
            """,
            (pattern,),
        ).fetchone()
        if row:
            return row["ticker"]

        row = connection.execute(
            """
            SELECT ticker
            FROM ticker_aliases
            WHERE UPPER(company_name) LIKE ?
            ORDER BY LENGTH(company_name) ASC, ticker ASC
            LIMIT 1
            """,
            (pattern,),
        ).fetchone()
        if row:
            return row["ticker"]

def replace_metric_snapshots(
    database_path: Path,
    records: Sequence[MetricRecord],
) -> int:
    """Replace metric snapshots with the supplied values."""
    if not records:
        return 0

    payload = [
        (
            _normalize_ticker(record.ticker),
            record.metric,
            record.period,
            record.start_year,
            record.end_year,
            record.value,
            record.source,
            _iso_utc(record.updated_at),
        )
        for record in records
    ]

    with _connect(database_path) as connection:
        cursor = connection.executemany(
            """
            INSERT INTO metric_snapshots (
                ticker, metric, period, start_year, end_year, value, source, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(ticker, metric, period, source) DO UPDATE SET
                start_year = excluded.start_year,
                end_year   = excluded.end_year,
                value      = excluded.value,
                updated_at = excluded.updated_at
            """,
            payload,
        )
        connection.commit()
        return cursor.rowcount


def fetch_metric_snapshots(
    database_path: Path,
    ticker: str,
    *,
    period_filters: Optional[Sequence[Tuple[int, int]]] = None,
) -> List[MetricRecord]:
    """Return metric snapshots for `ticker` optionally filtered by years."""
    sql = [
        "SELECT ticker, metric, period, value, source, updated_at, start_year, end_year",
        "FROM metric_snapshots",
        "WHERE ticker = ?",
    ]
    params: List[Any] = [_normalize_ticker(ticker)]

    if period_filters:
        clauses = []
        for start_year, end_year in period_filters:
            # overlap: NOT (end < start OR start > end)
            clauses.append(
                "(start_year IS NOT NULL AND end_year IS NOT NULL "
                "AND NOT (end_year < ? OR start_year > ?))"
            )
            params.extend([start_year, end_year])
        sql.append("AND (" + " OR ".join(clauses) + ")")

    sql.append("ORDER BY metric ASC, (end_year IS NULL) ASC, end_year DESC, updated_at DESC")
    query = "\n".join(sql)

    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(query, params).fetchall()

    results: List[MetricRecord] = []
    for row in rows:
        results.append(
            MetricRecord(
                ticker=row["ticker"],
                metric=row["metric"],
                period=row["period"],
                value=row["value"],
                source=row["source"],
                updated_at=_parse_dt(row["updated_at"]) or datetime(1970, 1, 1, tzinfo=timezone.utc),
                start_year=row["start_year"],
                end_year=row["end_year"],
            )
        )
    return results


# -----------------------------
# KPI values
# -----------------------------


def upsert_kpi_value(
    database_path: Path,
    ticker: str,
    fiscal_year: Optional[int],
    fiscal_quarter: Optional[int],
    metric_id: str,
    value: Optional[float],
    unit: Optional[str],
    method: str,
    source: str,
    source_ref: Optional[str],
    warning: Optional[str] = None,
) -> None:
    """Insert or update a KPI backfill value with provenance metadata."""
    timestamp = _iso_utc(datetime.now(timezone.utc))
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT INTO kpi_values (
                ticker, fiscal_year, fiscal_quarter, metric_id,
                value, unit, method, source, source_ref, warning, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(ticker, fiscal_year, fiscal_quarter, metric_id) DO UPDATE SET
                value      = excluded.value,
                unit       = excluded.unit,
                method     = excluded.method,
                source     = excluded.source,
                source_ref = excluded.source_ref,
                warning    = excluded.warning,
                updated_at = excluded.updated_at
            """,
            (
                _normalize_ticker(ticker),
                fiscal_year,
                fiscal_quarter,
                metric_id,
                value,
                unit,
                method,
                source,
                source_ref,
                warning,
                timestamp,
            ),
        )
        connection.commit()


def fetch_kpi_values(
    database_path: Path,
    ticker: str,
    *,
    fiscal_year: Optional[int] = None,
    fiscal_quarter: Optional[int] = None,
) -> List[KpiValueRecord]:
    """Return KPI backfill entries for ``ticker`` optionally filtered by period."""
    sql = [
        "SELECT ticker, fiscal_year, fiscal_quarter, metric_id, value, unit, method, source, source_ref, warning, updated_at",
        "FROM kpi_values",
        "WHERE ticker = ?",
    ]
    params: List[Any] = [_normalize_ticker(ticker)]
    if fiscal_year is not None:
        sql.append("AND fiscal_year = ?")
        params.append(fiscal_year)
    if fiscal_quarter is not None:
        sql.append("AND fiscal_quarter = ?")
        params.append(fiscal_quarter)
    sql.append("ORDER BY metric_id ASC")

    query = "\n".join(sql)
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        try:
            rows = connection.execute(query, params).fetchall()
        except sqlite3.OperationalError as exc:
            if "no such table" in str(exc).lower():
                return []
            raise

    results: List[KpiValueRecord] = []
    for row in rows:
        updated_at = _parse_dt(row["updated_at"]) or datetime(1970, 1, 1, tzinfo=timezone.utc)
        results.append(
            KpiValueRecord(
                ticker=row["ticker"],
                fiscal_year=row["fiscal_year"],
                fiscal_quarter=row["fiscal_quarter"],
                metric_id=row["metric_id"],
                value=row["value"],
                unit=row["unit"],
                method=row["method"],
                source=row["source"],
                source_ref=row["source_ref"],
                warning=row["warning"],
                updated_at=updated_at,
            )
        )
    return results


# -----------------------------
# Convenience wrapper
# -----------------------------


class Database:
    """Lightweight helper providing object-oriented access to database helpers."""

    def __init__(self, database_path: Union[str, Path]) -> None:
        self.path = Path(database_path)

    def fetch_metric_snapshots(
        self,
        ticker: str,
        *,
        period_filters: Optional[Sequence[Tuple[int, int]]] = None,
    ) -> List[MetricRecord]:
        return fetch_metric_snapshots(self.path, ticker, period_filters=period_filters)

    def fetch_financial_facts(
        self,
        ticker: Optional[str] = None,
        *,
        fiscal_year: Optional[int] = None,
        metric: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> List[FinancialFactRecord]:
        return fetch_financial_facts(
            self.path,
            ticker,
            fiscal_year=fiscal_year,
            metric=metric,
            limit=limit,
        )

    def fetch_kpi_values(
        self,
        ticker: str,
        *,
        fiscal_year: Optional[int] = None,
        fiscal_quarter: Optional[int] = None,
    ) -> List[KpiValueRecord]:
        return fetch_kpi_values(
            self.path,
            ticker,
            fiscal_year=fiscal_year,
            fiscal_quarter=fiscal_quarter,
        )

    def upsert_kpi(
        self,
        ticker: str,
        fiscal_year: Optional[int],
        fiscal_quarter: Optional[int],
        metric_id: str,
        value: Optional[float],
        unit: Optional[str],
        method: str,
        source: str,
        source_ref: Optional[str],
        warning: Optional[str] = None,
    ) -> None:
        upsert_kpi_value(
            self.path,
            ticker,
            fiscal_year,
            fiscal_quarter,
            metric_id,
            value,
            unit,
            method,
            source,
            source_ref,
            warning,
        )

    def fetch_latest_quote(self, ticker: str) -> Optional[Dict[str, Any]]:
        return fetch_latest_quote(self.path, ticker)

    def fetch_quote_on_or_before(
        self,
        ticker: str,
        *,
        before: datetime,
    ) -> Optional[Dict[str, Any]]:
        return fetch_quote_on_or_before(self.path, ticker, before=before)


# -----------------------------
# Scenario results
# -----------------------------


def store_scenario_result(database_path: Path, record: ScenarioResultRecord) -> None:
    """Persist the output of a scenario run."""
    payload = _json_dumps(record.metrics)
    created_at = _ensure_utc(record.created_at).isoformat()
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT INTO scenario_results (ticker, scenario_name, metrics, narrative, created_at)
            VALUES (?, ?, ?, ?, ?)
            """,
            (
                _normalize_ticker(record.ticker),
                record.scenario_name,
                payload,
                record.narrative,
                created_at,
            ),
        )
        connection.commit()


def fetch_scenario_results(
    database_path: Path,
    *,
    ticker: str,
    scenario_name: Optional[str] = None,
    limit: int = 10,
) -> List[ScenarioResultRecord]:
    """Yield stored scenario results for a ticker."""
    query = [
        "SELECT ticker, scenario_name, metrics, narrative, created_at",
        "FROM scenario_results",
        "WHERE ticker = ?",
    ]
    params: List[Any] = [_normalize_ticker(ticker)]
    if scenario_name:
        query.append("AND scenario_name = ?")
        params.append(scenario_name)
    query.append("ORDER BY created_at DESC")
    if limit:
        query.append("LIMIT ?")
        params.append(limit)

    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        query_str = "\n".join(query)
        rows = connection.execute(query_str, params).fetchall()

    results: List[ScenarioResultRecord] = []
    for row in rows:
        metrics_raw = json.loads(row["metrics"] or "{}")
        metrics: Dict[str, Optional[float]] = {}
        for key, value in metrics_raw.items():
            if isinstance(value, (int, float)):
                metrics[key] = float(value)
            else:
                metrics[key] = None
        created_at = _parse_dt(row["created_at"]) or datetime.now(timezone.utc)
        results.append(
            ScenarioResultRecord(
                ticker=_normalize_ticker(row["ticker"]),
                scenario_name=row["scenario_name"],
                metrics=metrics,
                narrative=row["narrative"],
                created_at=created_at,
            )
        )
    return results


# -----------------------------
# ML Forecast Persistence
# -----------------------------


def save_ml_forecast(
    database_path: Path,
    conversation_id: str,
    forecast_name: str,
    ticker: str,
    metric: str,
    method: str,
    periods: int,
    predicted_values: List[float],
    confidence_intervals_low: List[float],
    confidence_intervals_high: List[float],
    model_confidence: float,
    parameters: Optional[Dict[str, Any]] = None,
    explainability: Optional[Dict[str, Any]] = None,
) -> int:
    """
    Save an ML forecast to the database.
    
    Args:
        database_path: Path to database
        conversation_id: Conversation ID
        forecast_name: User-defined name for the forecast
        ticker: Company ticker
        metric: Metric forecasted
        method: ML model used
        periods: Number of periods forecasted
        predicted_values: List of predicted values
        confidence_intervals_low: Lower confidence bounds
        confidence_intervals_high: Upper confidence bounds
        model_confidence: Overall model confidence (0-1)
        parameters: Model parameters used
        explainability: Explainability data (drivers, features, etc.)
        
    Returns:
        Row ID of the saved forecast
    """
    created_at = datetime.now(timezone.utc).isoformat()
    
    # Serialize lists and dicts to JSON
    predicted_json = json.dumps(predicted_values)
    ci_low_json = json.dumps(confidence_intervals_low)
    ci_high_json = json.dumps(confidence_intervals_high)
    parameters_json = json.dumps(parameters or {})
    explainability_json = json.dumps(explainability or {})
    
    with _connect(database_path) as connection:
        cursor = connection.execute(
            """
            INSERT OR REPLACE INTO ml_forecasts (
                conversation_id, forecast_name, ticker, metric, method, periods,
                predicted_values, confidence_intervals_low, confidence_intervals_high,
                model_confidence, parameters, explainability, created_at
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                conversation_id,
                forecast_name,
                _normalize_ticker(ticker),
                metric,
                method,
                periods,
                predicted_json,
                ci_low_json,
                ci_high_json,
                model_confidence,
                parameters_json,
                explainability_json,
                created_at,
            ),
        )
        connection.commit()
        return cursor.lastrowid


def load_ml_forecast(
    database_path: Path,
    conversation_id: str,
    forecast_name: str
) -> Optional[Dict[str, Any]]:
    """
    Load a saved ML forecast from the database.
    
    Args:
        database_path: Path to database
        conversation_id: Conversation ID
        forecast_name: Name of the forecast to load
        
    Returns:
        Dictionary with forecast data, or None if not found
    """
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        row = connection.execute(
            """
            SELECT * FROM ml_forecasts
            WHERE conversation_id = ? AND forecast_name = ?
            ORDER BY created_at DESC
            LIMIT 1
            """,
            (conversation_id, forecast_name)
        ).fetchone()
    
    if not row:
        return None
    
    # Deserialize JSON fields
    predicted_values = json.loads(row["predicted_values"])
    ci_low = json.loads(row["confidence_intervals_low"])
    ci_high = json.loads(row["confidence_intervals_high"])
    parameters = json.loads(row["parameters"] or "{}")
    explainability = json.loads(row["explainability"] or "{}")
    
    return {
        "id": row["id"],
        "ticker": row["ticker"],
        "metric": row["metric"],
        "method": row["method"],
        "periods": row["periods"],
        "predicted_values": predicted_values,
        "confidence_intervals_low": ci_low,
        "confidence_intervals_high": ci_high,
        "model_confidence": row["model_confidence"],
        "parameters": parameters,
        "explainability": explainability,
        "created_at": row["created_at"],
    }


def list_ml_forecasts(
    database_path: Path,
    conversation_id: str,
    ticker: Optional[str] = None
) -> List[Tuple[str, str, str, str]]:
    """
    List saved ML forecasts for a conversation.
    
    Args:
        database_path: Path to database
        conversation_id: Conversation ID
        ticker: Optional ticker filter
        
    Returns:
        List of tuples: (forecast_name, ticker, metric, created_at)
    """
    query = """
        SELECT forecast_name, ticker, metric, created_at
        FROM ml_forecasts
        WHERE conversation_id = ?
    """
    params = [conversation_id]
    
    if ticker:
        query += " AND ticker = ?"
        params.append(_normalize_ticker(ticker))
    
    query += " ORDER BY created_at DESC"
    
    with _connect(database_path) as connection:
        rows = connection.execute(query, params).fetchall()
    
    return rows


# -----------------------------
# Portfolio Management
# -----------------------------


def upsert_portfolio_metadata(database_path: Path, record: PortfolioMetadataRecord) -> None:
    """Insert or update portfolio metadata."""
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT OR REPLACE INTO portfolio_metadata
            (portfolio_id, name, base_currency, benchmark_index, inception_date, strategy_type, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                record.portfolio_id,
                record.name,
                record.base_currency,
                record.benchmark_index,
                _iso_utc(record.inception_date),
                record.strategy_type,
                _iso_utc(record.created_at),
            ),
        )
        connection.commit()


def fetch_portfolio_metadata(database_path: Path, portfolio_id: str) -> Optional[PortfolioMetadataRecord]:
    """Fetch portfolio metadata by ID."""
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        row = connection.execute(
            """
            SELECT portfolio_id, name, base_currency, benchmark_index, inception_date, strategy_type, created_at
            FROM portfolio_metadata
            WHERE portfolio_id = ?
            """,
            (portfolio_id,),
        ).fetchone()
        
        if not row:
            return None
        
        return PortfolioMetadataRecord(
            portfolio_id=row["portfolio_id"],
            name=row["name"],
            base_currency=row["base_currency"],
            benchmark_index=row["benchmark_index"],
            inception_date=_parse_dt(row["inception_date"]) or datetime.now(timezone.utc),
            strategy_type=row["strategy_type"],
            created_at=_parse_dt(row["created_at"]) or datetime.now(timezone.utc),
        )


def list_portfolios(database_path: Path) -> List[PortfolioMetadataRecord]:
    """List all portfolios in the database."""
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(
            """
            SELECT portfolio_id, name, base_currency, benchmark_index, inception_date, strategy_type, created_at
            FROM portfolio_metadata
            ORDER BY created_at DESC
            """
        ).fetchall()
        
        return [
            PortfolioMetadataRecord(
                portfolio_id=row["portfolio_id"],
                name=row["name"],
                base_currency=row["base_currency"],
                benchmark_index=row["benchmark_index"],
                inception_date=_parse_dt(row["inception_date"]) or datetime.now(timezone.utc),
                strategy_type=row["strategy_type"],
                created_at=_parse_dt(row["created_at"]) or datetime.now(timezone.utc),
            )
            for row in rows
        ]


def bulk_insert_portfolio_holdings(
    database_path: Path, holdings: Sequence[PortfolioHoldingRecord], *, connection: Optional[sqlite3.Connection] = None
) -> int:
    """Insert portfolio holdings in bulk."""
    if not holdings:
        return 0

    payload = [
        (
            _normalize_ticker(holding.ticker),
            holding.portfolio_id,
            _iso_utc(holding.position_date),
            holding.shares,
            holding.weight,
            holding.cost_basis,
            holding.market_value,
            holding.currency,
            holding.account_id,
        )
        for holding in holdings
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO portfolio_holdings (ticker, portfolio_id, position_date, shares, weight, cost_basis, market_value, currency, account_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def fetch_portfolio_holdings(
    database_path: Path, portfolio_id: str, *, as_of_date: Optional[datetime] = None
) -> List[PortfolioHoldingRecord]:
    """Fetch holdings for a portfolio, optionally filtered by date."""
    query = [
        "SELECT ticker, portfolio_id, position_date, shares, weight, cost_basis, market_value, currency, account_id",
        "FROM portfolio_holdings",
        "WHERE portfolio_id = ?",
    ]
    params: List[Any] = [portfolio_id]
    
    if as_of_date:
        query.append("AND position_date <= ?")
        params.append(_iso_utc(as_of_date))
    
    query.append("ORDER BY position_date DESC, weight DESC")
    
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute("\n".join(query), params).fetchall()
    
    return [
        PortfolioHoldingRecord(
            ticker=_normalize_ticker(row["ticker"]),
            portfolio_id=row["portfolio_id"],
            position_date=_parse_dt(row["position_date"]) or datetime.now(timezone.utc),
            shares=row["shares"],
            weight=row["weight"],
            cost_basis=row["cost_basis"],
            market_value=row["market_value"],
            currency=row["currency"],
            account_id=row["account_id"],
        )
        for row in rows
    ]


def upsert_policy_constraint(database_path: Path, record: PolicyConstraintRecord) -> None:
    """Insert or update a policy constraint."""
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT OR REPLACE INTO policy_constraints
            (constraint_id, portfolio_id, constraint_type, target_value, min_value, max_value, unit, active, dimension)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                record.constraint_id,
                record.portfolio_id,
                record.constraint_type,
                record.target_value,
                record.min_value,
                record.max_value,
                record.unit,
                1 if record.active else 0,
                record.dimension,
            ),
        )
        connection.commit()


def fetch_policy_constraints(database_path: Path, portfolio_id: str, *, active_only: bool = True) -> List[PolicyConstraintRecord]:
    """Fetch policy constraints for a portfolio."""
    query = [
        "SELECT constraint_id, portfolio_id, constraint_type, target_value, min_value, max_value, unit, active, dimension",
        "FROM policy_constraints",
        "WHERE portfolio_id = ?",
    ]
    params: List[Any] = [portfolio_id]
    
    if active_only:
        query.append("AND active = 1")
    
    query.append("ORDER BY constraint_type, dimension")
    
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute("\n".join(query), params).fetchall()
    
    return [
        PolicyConstraintRecord(
            constraint_id=row["constraint_id"],
            portfolio_id=row["portfolio_id"],
            constraint_type=row["constraint_type"],
            target_value=row["target_value"],
            min_value=row["min_value"],
            max_value=row["max_value"],
            unit=row["unit"],
            active=bool(row["active"]),
            dimension=row["dimension"],
        )
        for row in rows
    ]


def upsert_policy_document(database_path: Path, record: PolicyDocumentRecord) -> None:
    """Insert or update a policy document."""
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT OR REPLACE INTO policy_documents
            (document_id, portfolio_id, document_type, file_path, parsed_constraints, uploaded_at)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (
                record.document_id,
                record.portfolio_id,
                record.document_type,
                record.file_path,
                _json_dumps(record.parsed_constraints),
                _iso_utc(record.uploaded_at),
            ),
        )
        connection.commit()


def fetch_policy_documents(database_path: Path, portfolio_id: str) -> List[PolicyDocumentRecord]:
    """Fetch policy documents for a portfolio."""
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute(
            """
            SELECT document_id, portfolio_id, document_type, file_path, parsed_constraints, uploaded_at
            FROM policy_documents
            WHERE portfolio_id = ?
            ORDER BY uploaded_at DESC
            """,
            (portfolio_id,),
        ).fetchall()
    
    return [
        PolicyDocumentRecord(
            document_id=row["document_id"],
            portfolio_id=row["portfolio_id"],
            document_type=row["document_type"],
            file_path=row["file_path"],
            parsed_constraints=json.loads(row["parsed_constraints"] or "{}"),
            uploaded_at=_parse_dt(row["uploaded_at"]) or datetime.now(timezone.utc),
        )
        for row in rows
    ]


def insert_portfolio_transaction(database_path: Path, record: PortfolioTransactionRecord) -> None:
    """Insert a portfolio transaction."""
    with _connect(database_path) as connection:
        connection.execute(
            """
            INSERT INTO portfolio_transactions
            (transaction_id, portfolio_id, ticker, trade_date, action, shares, price, commission, notes)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                record.transaction_id,
                record.portfolio_id,
                _normalize_ticker(record.ticker),
                _iso_utc(record.trade_date),
                record.action,
                record.shares,
                record.price,
                record.commission,
                record.notes,
            ),
        )
        connection.commit()


def fetch_portfolio_transactions(
    database_path: Path, portfolio_id: str, *, ticker: Optional[str] = None, limit: Optional[int] = None
) -> List[PortfolioTransactionRecord]:
    """Fetch portfolio transactions, optionally filtered by ticker."""
    query = [
        "SELECT transaction_id, portfolio_id, ticker, trade_date, action, shares, price, commission, notes",
        "FROM portfolio_transactions",
        "WHERE portfolio_id = ?",
    ]
    params: List[Any] = [portfolio_id]
    
    if ticker:
        query.append("AND ticker = ?")
        params.append(_normalize_ticker(ticker))
    
    query.append("ORDER BY trade_date DESC")
    
    if limit:
        query.append("LIMIT ?")
        params.append(limit)
    
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute("\n".join(query), params).fetchall()
    
    return [
        PortfolioTransactionRecord(
            transaction_id=row["transaction_id"],
            portfolio_id=row["portfolio_id"],
            ticker=_normalize_ticker(row["ticker"]),
            trade_date=_parse_dt(row["trade_date"]) or datetime.now(timezone.utc),
            action=row["action"],
            shares=row["shares"],
            price=row["price"],
            commission=row["commission"],
            notes=row["notes"],
        )
        for row in rows
    ]


def bulk_insert_exposure_snapshots(
    database_path: Path, snapshots: Sequence[ExposureSnapshotRecord], *, connection: Optional[sqlite3.Connection] = None
) -> int:
    """Insert exposure snapshots in bulk."""
    if not snapshots:
        return 0

    payload = [
        (
            snapshot.snapshot_id,
            snapshot.portfolio_id,
            _iso_utc(snapshot.snapshot_date),
            snapshot.exposure_type,
            snapshot.dimension,
            snapshot.value,
            snapshot.weight,
        )
        for snapshot in snapshots
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO exposure_snapshots (snapshot_id, portfolio_id, snapshot_date, exposure_type, dimension, value, weight)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def fetch_exposure_snapshots(
    database_path: Path, portfolio_id: str, *, exposure_type: Optional[str] = None, as_of_date: Optional[datetime] = None
) -> List[ExposureSnapshotRecord]:
    """Fetch exposure snapshots for a portfolio."""
    query = [
        "SELECT snapshot_id, portfolio_id, snapshot_date, exposure_type, dimension, value, weight",
        "FROM exposure_snapshots",
        "WHERE portfolio_id = ?",
    ]
    params: List[Any] = [portfolio_id]
    
    if exposure_type:
        query.append("AND exposure_type = ?")
        params.append(exposure_type)
    
    if as_of_date:
        query.append("AND snapshot_date <= ?")
        params.append(_iso_utc(as_of_date))
    
    query.append("ORDER BY snapshot_date DESC, exposure_type, dimension")
    
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute("\n".join(query), params).fetchall()
    
    return [
        ExposureSnapshotRecord(
            snapshot_id=row["snapshot_id"],
            portfolio_id=row["portfolio_id"],
            snapshot_date=_parse_dt(row["snapshot_date"]) or datetime.now(timezone.utc),
            exposure_type=row["exposure_type"],
            dimension=row["dimension"],
            value=row["value"],
            weight=row["weight"],
        )
        for row in rows
    ]


def bulk_insert_attribution_results(
    database_path: Path, results: Sequence[AttributionResultRecord], *, connection: Optional[sqlite3.Connection] = None
) -> int:
    """Insert attribution results in bulk."""
    if not results:
        return 0

    payload = [
        (
            result.attribution_id,
            result.portfolio_id,
            _iso_utc(result.start_date),
            _iso_utc(result.end_date),
            result.attribution_type,
            result.dimension,
            result.contribution,
            _iso_utc(result.created_at),
        )
        for result in results
    ]

    own_connection = False
    if connection is None:
        connection = _connect(database_path)
        own_connection = True
    try:
        cursor = connection.executemany(
            """
            INSERT INTO attribution_results (attribution_id, portfolio_id, start_date, end_date, attribution_type, dimension, contribution, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
            payload,
        )
        if own_connection:
            connection.commit()
        return cursor.rowcount
    finally:
        if own_connection:
            connection.close()


def fetch_attribution_results(
    database_path: Path,
    portfolio_id: str,
    *,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    attribution_type: Optional[str] = None,
) -> List[AttributionResultRecord]:
    """Fetch attribution results for a portfolio."""
    query = [
        "SELECT attribution_id, portfolio_id, start_date, end_date, attribution_type, dimension, contribution, created_at",
        "FROM attribution_results",
        "WHERE portfolio_id = ?",
    ]
    params: List[Any] = [portfolio_id]
    
    if start_date:
        query.append("AND start_date >= ?")
        params.append(_iso_utc(start_date))
    
    if end_date:
        query.append("AND end_date <= ?")
        params.append(_iso_utc(end_date))
    
    if attribution_type:
        query.append("AND attribution_type = ?")
        params.append(attribution_type)
    
    query.append("ORDER BY created_at DESC, attribution_type, dimension")
    
    with _connect(database_path) as connection:
        connection.row_factory = sqlite3.Row
        rows = connection.execute("\n".join(query), params).fetchall()
    
    return [
        AttributionResultRecord(
            attribution_id=row["attribution_id"],
            portfolio_id=row["portfolio_id"],
            start_date=_parse_dt(row["start_date"]) or datetime.now(timezone.utc),
            end_date=_parse_dt(row["end_date"]) or datetime.now(timezone.utc),
            attribution_type=row["attribution_type"],
            dimension=row["dimension"],
            contribution=row["contribution"],
            created_at=_parse_dt(row["created_at"]) or datetime.now(timezone.utc),
        )
        for row in rows
    ]
